{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Analysis of Data\n",
    "\n",
    "## Environment Settings\n",
    "\n",
    "An statistical Analysis of the data captured will be performed.\n",
    "\n",
    "The environment configuration is the following:\n",
    "\n",
    "- A rectangle area is used whose dimension is 2 x 1.5 meters. \n",
    "- A custom robot similar to an epuck was used.\n",
    "- The robot starts in the middle of the arena.\n",
    "- The robot moves in a random fashion way around the environment avoiding obstacles.\n",
    "- The robot has 8 sensors that measure the distance between the robot and the walls.\n",
    "- Some noise was introduced in the sensors measurements of the robot using the concept of [lookup tables](https://cyberbotics.com/doc/reference/distancesensor) in the Webots simulator which according to Webots documentation \"The first column of the table specifies the input distances, the second column specifies the corresponding desired response values, and the third column indicates the desired standard deviation of the noise. The noise on the return value is computed according to a gaussian random number distribution whose range is calculated as a percent of the response value (two times the standard deviation is often referred to as the signal quality)\". The following values were taken:\n",
    "\n",
    "    -First experiment:\n",
    "        - (0, 0, 0.01)\n",
    "        - (10, 10, 0.01)\n",
    "    -Second experiment:\n",
    "    \n",
    "        - (0, 0, 0.2)\n",
    "        - (10, 10, 0.2)\n",
    "- The simulator runs during 10 minutes in fast mode which is translated into 12 hours of collected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/site-packages (0.22)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/site-packages (from scikit-learn) (1.16.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/site-packages (from scikit-learn) (0.14.1)\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.7/site-packages (2.3.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/site-packages (from keras) (1.12.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/site-packages (from keras) (1.0.9)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/site-packages (from keras) (1.0.7)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/site-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/site-packages (from keras) (5.2)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/site-packages (from keras) (1.16.1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.7/site-packages (from keras) (2.9.0)\n"
     ]
    }
   ],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install scikit-learn\n",
    "!{sys.executable} -m pip install keras\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>theta</th>\n",
       "      <th>dx</th>\n",
       "      <th>dy</th>\n",
       "      <th>dtheta</th>\n",
       "      <th>sensor_1</th>\n",
       "      <th>sensor_2</th>\n",
       "      <th>sensor_3</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor_7</th>\n",
       "      <th>sensor_8</th>\n",
       "      <th>dsensor_1</th>\n",
       "      <th>dsensor_2</th>\n",
       "      <th>dsensor_3</th>\n",
       "      <th>dsensor_4</th>\n",
       "      <th>dsensor_5</th>\n",
       "      <th>dsensor_6</th>\n",
       "      <th>dsensor_7</th>\n",
       "      <th>dsensor_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.920614</td>\n",
       "      <td>0.761198</td>\n",
       "      <td>168.209483</td>\n",
       "      <td>-0.070670</td>\n",
       "      <td>0.011198</td>\n",
       "      <td>-11.790739</td>\n",
       "      <td>1.085179</td>\n",
       "      <td>0.790267</td>\n",
       "      <td>0.893342</td>\n",
       "      <td>...</td>\n",
       "      <td>1.139790</td>\n",
       "      <td>1.144901</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.850135</td>\n",
       "      <td>0.775909</td>\n",
       "      <td>168.212418</td>\n",
       "      <td>-0.070479</td>\n",
       "      <td>0.014711</td>\n",
       "      <td>0.002935</td>\n",
       "      <td>0.571635</td>\n",
       "      <td>0.596799</td>\n",
       "      <td>0.883340</td>\n",
       "      <td>...</td>\n",
       "      <td>0.830057</td>\n",
       "      <td>1.028332</td>\n",
       "      <td>-0.513544</td>\n",
       "      <td>-0.193468</td>\n",
       "      <td>-0.010002</td>\n",
       "      <td>-0.430864</td>\n",
       "      <td>-0.070277</td>\n",
       "      <td>-0.387726</td>\n",
       "      <td>-0.309733</td>\n",
       "      <td>-0.116568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.779657</td>\n",
       "      <td>0.790625</td>\n",
       "      <td>168.209551</td>\n",
       "      <td>-0.070478</td>\n",
       "      <td>0.014716</td>\n",
       "      <td>-0.002867</td>\n",
       "      <td>0.581452</td>\n",
       "      <td>0.904627</td>\n",
       "      <td>0.689004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.491200</td>\n",
       "      <td>0.889130</td>\n",
       "      <td>0.009817</td>\n",
       "      <td>0.307828</td>\n",
       "      <td>-0.194336</td>\n",
       "      <td>0.239518</td>\n",
       "      <td>0.206480</td>\n",
       "      <td>0.293382</td>\n",
       "      <td>-0.338857</td>\n",
       "      <td>-0.139203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.709174</td>\n",
       "      <td>0.805340</td>\n",
       "      <td>168.212871</td>\n",
       "      <td>-0.070483</td>\n",
       "      <td>0.014715</td>\n",
       "      <td>0.003319</td>\n",
       "      <td>0.956302</td>\n",
       "      <td>0.842911</td>\n",
       "      <td>0.796714</td>\n",
       "      <td>...</td>\n",
       "      <td>1.246415</td>\n",
       "      <td>0.712158</td>\n",
       "      <td>0.374849</td>\n",
       "      <td>-0.061716</td>\n",
       "      <td>0.107710</td>\n",
       "      <td>0.075412</td>\n",
       "      <td>-0.345782</td>\n",
       "      <td>-0.084918</td>\n",
       "      <td>0.755215</td>\n",
       "      <td>-0.176971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.638698</td>\n",
       "      <td>0.820056</td>\n",
       "      <td>168.208857</td>\n",
       "      <td>-0.070477</td>\n",
       "      <td>0.014716</td>\n",
       "      <td>-0.004013</td>\n",
       "      <td>0.671731</td>\n",
       "      <td>0.779896</td>\n",
       "      <td>0.962191</td>\n",
       "      <td>...</td>\n",
       "      <td>0.567806</td>\n",
       "      <td>0.595164</td>\n",
       "      <td>-0.284570</td>\n",
       "      <td>-0.063014</td>\n",
       "      <td>0.165477</td>\n",
       "      <td>0.005216</td>\n",
       "      <td>0.128150</td>\n",
       "      <td>-0.054777</td>\n",
       "      <td>-0.678608</td>\n",
       "      <td>-0.116994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0         x         y       theta        dx        dy     dtheta  \\\n",
       "0           0  0.920614  0.761198  168.209483 -0.070670  0.011198 -11.790739   \n",
       "1           1  0.850135  0.775909  168.212418 -0.070479  0.014711   0.002935   \n",
       "2           2  0.779657  0.790625  168.209551 -0.070478  0.014716  -0.002867   \n",
       "3           3  0.709174  0.805340  168.212871 -0.070483  0.014715   0.003319   \n",
       "4           4  0.638698  0.820056  168.208857 -0.070477  0.014716  -0.004013   \n",
       "\n",
       "   sensor_1  sensor_2  sensor_3    ...      sensor_7  sensor_8  dsensor_1  \\\n",
       "0  1.085179  0.790267  0.893342    ...      1.139790  1.144901        NaN   \n",
       "1  0.571635  0.596799  0.883340    ...      0.830057  1.028332  -0.513544   \n",
       "2  0.581452  0.904627  0.689004    ...      0.491200  0.889130   0.009817   \n",
       "3  0.956302  0.842911  0.796714    ...      1.246415  0.712158   0.374849   \n",
       "4  0.671731  0.779896  0.962191    ...      0.567806  0.595164  -0.284570   \n",
       "\n",
       "   dsensor_2  dsensor_3  dsensor_4  dsensor_5  dsensor_6  dsensor_7  dsensor_8  \n",
       "0        NaN        NaN        NaN        NaN        NaN        NaN        NaN  \n",
       "1  -0.193468  -0.010002  -0.430864  -0.070277  -0.387726  -0.309733  -0.116568  \n",
       "2   0.307828  -0.194336   0.239518   0.206480   0.293382  -0.338857  -0.139203  \n",
       "3  -0.061716   0.107710   0.075412  -0.345782  -0.084918   0.755215  -0.176971  \n",
       "4  -0.063014   0.165477   0.005216   0.128150  -0.054777  -0.678608  -0.116994  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_file = 'robot_info_dataset-jumped.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data collected 1384848 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65342, 23)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set contains some null values so they should be deleted from the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data will be normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>theta</th>\n",
       "      <th>dx</th>\n",
       "      <th>dy</th>\n",
       "      <th>dtheta</th>\n",
       "      <th>sensor_1</th>\n",
       "      <th>sensor_2</th>\n",
       "      <th>sensor_3</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor_7</th>\n",
       "      <th>sensor_8</th>\n",
       "      <th>dsensor_1</th>\n",
       "      <th>dsensor_2</th>\n",
       "      <th>dsensor_3</th>\n",
       "      <th>dsensor_4</th>\n",
       "      <th>dsensor_5</th>\n",
       "      <th>dsensor_6</th>\n",
       "      <th>dsensor_7</th>\n",
       "      <th>dsensor_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.498321</td>\n",
       "      <td>0.504753</td>\n",
       "      <td>0.502063</td>\n",
       "      <td>0.499785</td>\n",
       "      <td>0.500412</td>\n",
       "      <td>0.501624</td>\n",
       "      <td>0.239976</td>\n",
       "      <td>0.236145</td>\n",
       "      <td>0.261438</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251293</td>\n",
       "      <td>0.242889</td>\n",
       "      <td>0.449485</td>\n",
       "      <td>0.468761</td>\n",
       "      <td>0.513828</td>\n",
       "      <td>0.507022</td>\n",
       "      <td>0.519272</td>\n",
       "      <td>0.531825</td>\n",
       "      <td>0.446832</td>\n",
       "      <td>0.426383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.288682</td>\n",
       "      <td>0.272549</td>\n",
       "      <td>0.264025</td>\n",
       "      <td>0.290735</td>\n",
       "      <td>0.353425</td>\n",
       "      <td>0.335002</td>\n",
       "      <td>0.114192</td>\n",
       "      <td>0.140647</td>\n",
       "      <td>0.149030</td>\n",
       "      <td>0.169722</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160999</td>\n",
       "      <td>0.143636</td>\n",
       "      <td>0.078247</td>\n",
       "      <td>0.073403</td>\n",
       "      <td>0.077416</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.081846</td>\n",
       "      <td>0.080184</td>\n",
       "      <td>0.072415</td>\n",
       "      <td>0.077850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.255777</td>\n",
       "      <td>0.269104</td>\n",
       "      <td>0.251332</td>\n",
       "      <td>0.139102</td>\n",
       "      <td>0.181098</td>\n",
       "      <td>0.496242</td>\n",
       "      <td>0.127116</td>\n",
       "      <td>0.108774</td>\n",
       "      <td>0.119219</td>\n",
       "      <td>...</td>\n",
       "      <td>0.112714</td>\n",
       "      <td>0.127502</td>\n",
       "      <td>0.412053</td>\n",
       "      <td>0.436092</td>\n",
       "      <td>0.483323</td>\n",
       "      <td>0.477268</td>\n",
       "      <td>0.488782</td>\n",
       "      <td>0.501383</td>\n",
       "      <td>0.415117</td>\n",
       "      <td>0.388997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.498537</td>\n",
       "      <td>0.503544</td>\n",
       "      <td>0.498631</td>\n",
       "      <td>0.500020</td>\n",
       "      <td>0.500797</td>\n",
       "      <td>0.501627</td>\n",
       "      <td>0.216759</td>\n",
       "      <td>0.215659</td>\n",
       "      <td>0.237666</td>\n",
       "      <td>...</td>\n",
       "      <td>0.224880</td>\n",
       "      <td>0.220298</td>\n",
       "      <td>0.442927</td>\n",
       "      <td>0.467409</td>\n",
       "      <td>0.516314</td>\n",
       "      <td>0.512401</td>\n",
       "      <td>0.524693</td>\n",
       "      <td>0.534465</td>\n",
       "      <td>0.445713</td>\n",
       "      <td>0.419784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.735371</td>\n",
       "      <td>0.740156</td>\n",
       "      <td>0.752400</td>\n",
       "      <td>0.860369</td>\n",
       "      <td>0.821281</td>\n",
       "      <td>0.506991</td>\n",
       "      <td>0.328127</td>\n",
       "      <td>0.337889</td>\n",
       "      <td>0.375692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360337</td>\n",
       "      <td>0.333973</td>\n",
       "      <td>0.479438</td>\n",
       "      <td>0.498031</td>\n",
       "      <td>0.547039</td>\n",
       "      <td>0.544322</td>\n",
       "      <td>0.557577</td>\n",
       "      <td>0.565527</td>\n",
       "      <td>0.475271</td>\n",
       "      <td>0.456114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0             x             y         theta            dx  \\\n",
       "count  65341.000000  65341.000000  65341.000000  65341.000000  65341.000000   \n",
       "mean       0.500000      0.498321      0.504753      0.502063      0.499785   \n",
       "std        0.288682      0.272549      0.264025      0.290735      0.353425   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.250000      0.255777      0.269104      0.251332      0.139102   \n",
       "50%        0.500000      0.498537      0.503544      0.498631      0.500020   \n",
       "75%        0.750000      0.735371      0.740156      0.752400      0.860369   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                 dy        dtheta      sensor_1      sensor_2      sensor_3  \\\n",
       "count  65341.000000  65341.000000  65341.000000  65341.000000  65341.000000   \n",
       "mean       0.500412      0.501624      0.239976      0.236145      0.261438   \n",
       "std        0.335002      0.114192      0.140647      0.149030      0.169722   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.181098      0.496242      0.127116      0.108774      0.119219   \n",
       "50%        0.500797      0.501627      0.216759      0.215659      0.237666   \n",
       "75%        0.821281      0.506991      0.328127      0.337889      0.375692   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "           ...           sensor_7      sensor_8     dsensor_1     dsensor_2  \\\n",
       "count      ...       65341.000000  65341.000000  65341.000000  65341.000000   \n",
       "mean       ...           0.251293      0.242889      0.449485      0.468761   \n",
       "std        ...           0.160999      0.143636      0.078247      0.073403   \n",
       "min        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "25%        ...           0.112714      0.127502      0.412053      0.436092   \n",
       "50%        ...           0.224880      0.220298      0.442927      0.467409   \n",
       "75%        ...           0.360337      0.333973      0.479438      0.498031   \n",
       "max        ...           1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "          dsensor_3     dsensor_4     dsensor_5     dsensor_6     dsensor_7  \\\n",
       "count  65341.000000  65341.000000  65341.000000  65341.000000  65341.000000   \n",
       "mean       0.513828      0.507022      0.519272      0.531825      0.446832   \n",
       "std        0.077416      0.078125      0.081846      0.080184      0.072415   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.483323      0.477268      0.488782      0.501383      0.415117   \n",
       "50%        0.516314      0.512401      0.524693      0.534465      0.445713   \n",
       "75%        0.547039      0.544322      0.557577      0.565527      0.475271   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "          dsensor_8  \n",
       "count  65341.000000  \n",
       "mean       0.426383  \n",
       "std        0.077850  \n",
       "min        0.000000  \n",
       "25%        0.388997  \n",
       "50%        0.419784  \n",
       "75%        0.456114  \n",
       "max        1.000000  \n",
       "\n",
       "[8 rows x 23 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_df=(df-df.min())/(df.max()-df.min())\n",
    "normalized_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input and output variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data will be split into training, testing and validation sets. 60% of the data will be used for training, 20% for training and 20% of validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train size\n",
    "test_size_percentage = .2\n",
    "train_size_percentage = .6\n",
    "ds_size = normalized_df.shape[0]\n",
    "train_size = int(train_size_percentage * ds_size)\n",
    "test_size = int(test_size_percentage * ds_size)\n",
    "\n",
    "# shuffle dataset\n",
    "normalized_df = normalized_df.sample(frac=1)\n",
    "\n",
    "# separate inputs from outputs\n",
    "inputs = normalized_df[['x', 'y', 'theta']]\n",
    "targets = normalized_df[['sensor_1', 'sensor_2', 'sensor_3', 'sensor_4', 'sensor_5', 'sensor_6', 'sensor_7', 'sensor_8']]\n",
    "\n",
    "# train\n",
    "train_inputs = inputs[:train_size]\n",
    "train_targets = targets[:train_size]\n",
    "\n",
    "# test\n",
    "test_inputs = inputs[train_size:(train_size + test_size)]\n",
    "test_targets = targets[train_size:(train_size + test_size)]\n",
    "\n",
    "# validation\n",
    "validation_inputs = inputs[(train_size + test_size):]\n",
    "validation_targets = targets[(train_size + test_size):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forsest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to fit a random forest tree with 5 trees and each tree will handle $\\sqrt n$ number of variables available for splitting at each tree node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=3, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=1, oob_score=False,\n",
       "                      random_state=None, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features = 8;\n",
    "max_features = round(math.sqrt(n_features))\n",
    "\n",
    "reg = RandomForestRegressor(n_estimators=5, max_features=max_features, criterion='mse', verbose=False, n_jobs=1)\n",
    "reg.fit(train_inputs, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features importances x: 0.314563, y: 0.249262, theta: 0.436175\n",
      "\n",
      "R^2 score: 0.801145 \n",
      "\n",
      "Mean Absolute Error:\n",
      "0.04795014\n",
      "\n",
      "\n",
      "Accuracy:\n",
      "76.94 %.\n",
      "\n",
      "NMSE\n",
      "0.20109603291958672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/base.py:434: FutureWarning: The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
      "  \"multioutput='uniform_average').\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "print(\"Features importances x: %f, y: %f, theta: %f\" %(reg.feature_importances_[0], reg.feature_importances_[1], reg.feature_importances_[2]))\n",
    "print()\n",
    "predictions_targets = reg.predict(test_inputs)\n",
    "\n",
    "print(\"R^2 score: %f \\n\" % reg.score(test_inputs, test_targets))\n",
    "\n",
    "# Calculate the absolute errors\n",
    "errors = abs(predictions_targets - test_targets)\n",
    "\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:')\n",
    "print(round(np.mean(np.mean(errors)), 8))\n",
    "print()\n",
    "\n",
    "# Calculate mean absolute percentage error (MAPE)\n",
    "mape = 100 * (errors / test_targets)\n",
    "\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - np.mean(mape)\n",
    "\n",
    "print()\n",
    "print('Accuracy:')\n",
    "print(round(np.mean(accuracy), 2), '%.')\n",
    "print()\n",
    "\n",
    "nmse = np.mean((predictions_targets - test_targets)**2/np.var(test_targets))\n",
    "print(\"NMSE\")\n",
    "print(np.mean(nmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As input the neural network receives the x, y coordinates and rotation angle $\\theta$. The output are the sensor measurements. The hidden layer uses relu as activation function. The loss function is MSE and it serves as a metric\n",
    "to minimize the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NN Architecture](nn_architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model_with(train_data, number_neurons):\n",
    "    # neural network with a 10-neuron hidden layer\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(number_neurons, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01),\n",
    "                           input_shape=(train_data.shape[1],)))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(8))\n",
    "    rmsprop = optimizers.RMSprop(learning_rate=0.1)\n",
    "    model.compile(optimizer=rmsprop, loss='mse', metrics=['mae'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold # 0\n",
      "Train on 19602 samples, validate on 19602 samples\n",
      "Epoch 1/80\n",
      "19602/19602 [==============================] - 29s 1ms/step - loss: 0.0507 - mae: 0.1430 - val_loss: 0.0491 - val_mae: 0.1378\n",
      "Epoch 2/80\n",
      "19602/19602 [==============================] - 32s 2ms/step - loss: 0.0507 - mae: 0.1429 - val_loss: 0.0479 - val_mae: 0.1385\n",
      "Epoch 3/80\n",
      "19602/19602 [==============================] - 32s 2ms/step - loss: 0.0508 - mae: 0.1433 - val_loss: 0.0542 - val_mae: 0.1505\n",
      "Epoch 4/80\n",
      "19602/19602 [==============================] - 33s 2ms/step - loss: 0.0508 - mae: 0.1434 - val_loss: 0.0606 - val_mae: 0.1601\n",
      "Epoch 5/80\n",
      "19602/19602 [==============================] - 28s 1ms/step - loss: 0.0509 - mae: 0.1435 - val_loss: 0.0434 - val_mae: 0.1270\n",
      "Epoch 6/80\n",
      "19602/19602 [==============================] - 31s 2ms/step - loss: 0.0507 - mae: 0.1430 - val_loss: 0.0468 - val_mae: 0.1360\n",
      "Epoch 7/80\n",
      "19602/19602 [==============================] - 30s 2ms/step - loss: 0.0508 - mae: 0.1433 - val_loss: 0.0487 - val_mae: 0.1402\n",
      "Epoch 8/80\n",
      "19602/19602 [==============================] - 30s 2ms/step - loss: 0.0507 - mae: 0.1430 - val_loss: 0.0491 - val_mae: 0.1404\n",
      "Epoch 9/80\n",
      "19602/19602 [==============================] - 27s 1ms/step - loss: 0.0507 - mae: 0.1429 - val_loss: 0.0566 - val_mae: 0.1507\n",
      "Epoch 10/80\n",
      "19602/19602 [==============================] - 27s 1ms/step - loss: 0.0508 - mae: 0.1432 - val_loss: 0.0545 - val_mae: 0.1514\n",
      "Epoch 11/80\n",
      "19602/19602 [==============================] - 31s 2ms/step - loss: 0.0507 - mae: 0.1428 - val_loss: 0.0456 - val_mae: 0.1335\n",
      "Epoch 12/80\n",
      "19602/19602 [==============================] - 29s 1ms/step - loss: 0.0507 - mae: 0.1431 - val_loss: 0.0505 - val_mae: 0.1426\n",
      "Epoch 13/80\n",
      "19602/19602 [==============================] - 32s 2ms/step - loss: 0.0507 - mae: 0.1429 - val_loss: 0.0479 - val_mae: 0.1359\n",
      "Epoch 14/80\n",
      "19602/19602 [==============================] - 29s 1ms/step - loss: 0.0508 - mae: 0.1431 - val_loss: 0.0458 - val_mae: 0.1332\n",
      "Epoch 15/80\n",
      "19602/19602 [==============================] - 30s 2ms/step - loss: 0.0507 - mae: 0.1429 - val_loss: 0.0524 - val_mae: 0.1462\n",
      "Epoch 16/80\n",
      "19602/19602 [==============================] - 29s 1ms/step - loss: 0.0508 - mae: 0.1431 - val_loss: 0.0586 - val_mae: 0.1554\n",
      "Epoch 17/80\n",
      "19602/19602 [==============================] - 27s 1ms/step - loss: 0.0508 - mae: 0.1430 - val_loss: 0.0576 - val_mae: 0.1601\n",
      "Epoch 18/80\n",
      "19602/19602 [==============================] - 28s 1ms/step - loss: 0.0508 - mae: 0.1431 - val_loss: 0.0675 - val_mae: 0.1755\n",
      "Epoch 19/80\n",
      "19602/19602 [==============================] - 30s 2ms/step - loss: 0.0507 - mae: 0.1428 - val_loss: 0.0550 - val_mae: 0.1510\n",
      "Epoch 20/80\n",
      "19602/19602 [==============================] - 31s 2ms/step - loss: 0.0507 - mae: 0.1430 - val_loss: 0.0526 - val_mae: 0.1447\n",
      "Epoch 21/80\n",
      "19602/19602 [==============================] - 32s 2ms/step - loss: 0.0507 - mae: 0.1429 - val_loss: 0.0458 - val_mae: 0.1358\n",
      "Epoch 22/80\n",
      "19602/19602 [==============================] - 31s 2ms/step - loss: 0.0507 - mae: 0.1429 - val_loss: 0.0521 - val_mae: 0.1436\n",
      "Epoch 23/80\n",
      "19602/19602 [==============================] - 29s 1ms/step - loss: 0.0507 - mae: 0.1427 - val_loss: 0.0448 - val_mae: 0.1342\n",
      "Epoch 24/80\n",
      "19602/19602 [==============================] - 29s 1ms/step - loss: 0.0508 - mae: 0.1432 - val_loss: 0.0482 - val_mae: 0.1396\n",
      "Epoch 25/80\n",
      "19602/19602 [==============================] - 29s 1ms/step - loss: 0.0507 - mae: 0.1427 - val_loss: 0.0460 - val_mae: 0.1345\n",
      "Epoch 26/80\n",
      "19602/19602 [==============================] - 29s 1ms/step - loss: 0.0507 - mae: 0.1430 - val_loss: 0.0462 - val_mae: 0.1345\n",
      "Epoch 27/80\n",
      "19602/19602 [==============================] - 30s 2ms/step - loss: 0.0507 - mae: 0.1429 - val_loss: 0.0595 - val_mae: 0.1592\n",
      "Epoch 28/80\n",
      "19602/19602 [==============================] - 31s 2ms/step - loss: 0.0507 - mae: 0.1428 - val_loss: 0.0459 - val_mae: 0.1307\n",
      "Epoch 29/80\n",
      "19602/19602 [==============================] - 31s 2ms/step - loss: 0.0508 - mae: 0.1430 - val_loss: 0.0513 - val_mae: 0.1439\n",
      "Epoch 30/80\n",
      "19602/19602 [==============================] - 29s 1ms/step - loss: 0.0508 - mae: 0.1433 - val_loss: 0.0573 - val_mae: 0.1560\n",
      "Epoch 31/80\n",
      "19602/19602 [==============================] - 29s 1ms/step - loss: 0.0506 - mae: 0.1427 - val_loss: 0.0491 - val_mae: 0.1413\n",
      "Epoch 32/80\n",
      "19602/19602 [==============================] - 29s 1ms/step - loss: 0.0509 - mae: 0.1435 - val_loss: 0.0542 - val_mae: 0.1497\n",
      "Epoch 33/80\n",
      "19602/19602 [==============================] - 29s 1ms/step - loss: 0.0507 - mae: 0.1430 - val_loss: 0.0518 - val_mae: 0.1475\n",
      "Epoch 34/80\n",
      "19602/19602 [==============================] - 30s 2ms/step - loss: 0.0507 - mae: 0.1431 - val_loss: 0.0571 - val_mae: 0.1565\n",
      "Epoch 35/80\n",
      "19602/19602 [==============================] - 32s 2ms/step - loss: 0.0506 - mae: 0.1427 - val_loss: 0.0438 - val_mae: 0.1286\n",
      "Epoch 36/80\n",
      "19602/19602 [==============================] - 31s 2ms/step - loss: 0.0508 - mae: 0.1434 - val_loss: 0.0554 - val_mae: 0.1539\n",
      "Epoch 37/80\n",
      "19602/19602 [==============================] - 33s 2ms/step - loss: 0.0508 - mae: 0.1433 - val_loss: 0.0550 - val_mae: 0.1528\n",
      "Epoch 38/80\n",
      "19602/19602 [==============================] - 27s 1ms/step - loss: 0.0507 - mae: 0.1431 - val_loss: 0.0531 - val_mae: 0.1470\n",
      "Epoch 39/80\n",
      "19602/19602 [==============================] - 30s 2ms/step - loss: 0.0506 - mae: 0.1428 - val_loss: 0.0472 - val_mae: 0.1365\n",
      "Epoch 40/80\n",
      "19602/19602 [==============================] - 30s 2ms/step - loss: 0.0506 - mae: 0.1428 - val_loss: 0.0622 - val_mae: 0.1652\n",
      "Epoch 41/80\n",
      "19602/19602 [==============================] - 29s 1ms/step - loss: 0.0506 - mae: 0.1426 - val_loss: 0.0479 - val_mae: 0.1359\n",
      "Epoch 42/80\n",
      "19602/19602 [==============================] - 30s 2ms/step - loss: 0.0507 - mae: 0.1429 - val_loss: 0.0515 - val_mae: 0.1442\n",
      "Epoch 43/80\n",
      "19602/19602 [==============================] - 31s 2ms/step - loss: 0.0508 - mae: 0.1431 - val_loss: 0.0457 - val_mae: 0.1330\n",
      "Epoch 44/80\n",
      "19602/19602 [==============================] - 39s 2ms/step - loss: 0.0508 - mae: 0.1433 - val_loss: 0.0486 - val_mae: 0.1388\n",
      "Epoch 45/80\n",
      "19602/19602 [==============================] - 32s 2ms/step - loss: 0.0508 - mae: 0.1433 - val_loss: 0.0479 - val_mae: 0.1389\n",
      "Epoch 46/80\n",
      "19602/19602 [==============================] - 28s 1ms/step - loss: 0.0507 - mae: 0.1431 - val_loss: 0.0585 - val_mae: 0.1578\n",
      "Epoch 47/80\n",
      "19602/19602 [==============================] - 30s 2ms/step - loss: 0.0508 - mae: 0.1431 - val_loss: 0.0471 - val_mae: 0.1347\n",
      "Epoch 48/80\n",
      "19602/19602 [==============================] - 42s 2ms/step - loss: 0.0508 - mae: 0.1431 - val_loss: 0.0458 - val_mae: 0.1320\n",
      "Epoch 49/80\n",
      "19602/19602 [==============================] - 32s 2ms/step - loss: 0.0507 - mae: 0.1430 - val_loss: 0.0499 - val_mae: 0.1412\n",
      "Epoch 50/80\n",
      "19602/19602 [==============================] - 30s 2ms/step - loss: 0.0507 - mae: 0.1430 - val_loss: 0.0487 - val_mae: 0.1393\n",
      "Epoch 51/80\n",
      "19602/19602 [==============================] - 28s 1ms/step - loss: 0.0506 - mae: 0.1427 - val_loss: 0.0443 - val_mae: 0.1266\n",
      "Epoch 52/80\n",
      "19602/19602 [==============================] - 27s 1ms/step - loss: 0.0508 - mae: 0.1433 - val_loss: 0.0563 - val_mae: 0.1557\n",
      "Epoch 53/80\n",
      "19602/19602 [==============================] - 29s 1ms/step - loss: 0.0508 - mae: 0.1432 - val_loss: 0.0456 - val_mae: 0.1327\n",
      "Epoch 54/80\n",
      "19602/19602 [==============================] - 28s 1ms/step - loss: 0.0508 - mae: 0.1431 - val_loss: 0.0459 - val_mae: 0.1352\n",
      "Epoch 55/80\n",
      "19602/19602 [==============================] - 29s 1ms/step - loss: 0.0507 - mae: 0.1430 - val_loss: 0.0561 - val_mae: 0.1539\n",
      "Epoch 56/80\n",
      "19602/19602 [==============================] - 26s 1ms/step - loss: 0.0506 - mae: 0.1429 - val_loss: 0.0555 - val_mae: 0.1530\n",
      "Epoch 57/80\n",
      "19602/19602 [==============================] - 28s 1ms/step - loss: 0.0507 - mae: 0.1431 - val_loss: 0.0524 - val_mae: 0.1462\n",
      "Epoch 58/80\n",
      "19602/19602 [==============================] - 27s 1ms/step - loss: 0.0508 - mae: 0.1433 - val_loss: 0.0579 - val_mae: 0.1540\n",
      "Epoch 59/80\n",
      "19602/19602 [==============================] - 27s 1ms/step - loss: 0.0507 - mae: 0.1429 - val_loss: 0.0471 - val_mae: 0.1366\n",
      "Epoch 60/80\n",
      "19602/19602 [==============================] - 29s 1ms/step - loss: 0.0507 - mae: 0.1430 - val_loss: 0.0484 - val_mae: 0.1393\n",
      "Epoch 61/80\n",
      "19602/19602 [==============================] - 27s 1ms/step - loss: 0.0507 - mae: 0.1430 - val_loss: 0.0606 - val_mae: 0.1616\n",
      "Epoch 62/80\n",
      "19602/19602 [==============================] - 27s 1ms/step - loss: 0.0507 - mae: 0.1430 - val_loss: 0.0447 - val_mae: 0.1332\n",
      "Epoch 63/80\n",
      "19602/19602 [==============================] - 28s 1ms/step - loss: 0.0507 - mae: 0.1429 - val_loss: 0.0538 - val_mae: 0.1484\n",
      "Epoch 64/80\n",
      "19602/19602 [==============================] - 29s 1ms/step - loss: 0.0507 - mae: 0.1428 - val_loss: 0.0467 - val_mae: 0.1371\n",
      "Epoch 65/80\n",
      "19602/19602 [==============================] - 27s 1ms/step - loss: 0.0507 - mae: 0.1431 - val_loss: 0.0562 - val_mae: 0.1526\n",
      "Epoch 66/80\n",
      "19602/19602 [==============================] - 28s 1ms/step - loss: 0.0507 - mae: 0.1430 - val_loss: 0.0475 - val_mae: 0.1353\n",
      "Epoch 67/80\n",
      "19602/19602 [==============================] - 31s 2ms/step - loss: 0.0507 - mae: 0.1431 - val_loss: 0.0489 - val_mae: 0.1395\n",
      "Epoch 68/80\n",
      "19602/19602 [==============================] - 29s 1ms/step - loss: 0.0507 - mae: 0.1431 - val_loss: 0.0550 - val_mae: 0.1500\n",
      "Epoch 69/80\n",
      "19602/19602 [==============================] - 31s 2ms/step - loss: 0.0507 - mae: 0.1429 - val_loss: 0.0449 - val_mae: 0.1337\n",
      "Epoch 70/80\n",
      "19602/19602 [==============================] - 29s 2ms/step - loss: 0.0506 - mae: 0.1428 - val_loss: 0.0460 - val_mae: 0.1355\n",
      "Epoch 71/80\n",
      "19602/19602 [==============================] - 30s 2ms/step - loss: 0.0507 - mae: 0.1430 - val_loss: 0.0697 - val_mae: 0.1749\n",
      "Epoch 72/80\n",
      "19602/19602 [==============================] - 31s 2ms/step - loss: 0.0506 - mae: 0.1427 - val_loss: 0.0467 - val_mae: 0.1341\n",
      "Epoch 73/80\n",
      "19602/19602 [==============================] - 26s 1ms/step - loss: 0.0506 - mae: 0.1427 - val_loss: 0.0486 - val_mae: 0.1406\n",
      "Epoch 74/80\n",
      "19602/19602 [==============================] - 26s 1ms/step - loss: 0.0508 - mae: 0.1433 - val_loss: 0.0493 - val_mae: 0.1432\n",
      "Epoch 75/80\n",
      "19602/19602 [==============================] - 27s 1ms/step - loss: 0.0507 - mae: 0.1430 - val_loss: 0.0521 - val_mae: 0.1472\n",
      "Epoch 76/80\n",
      "19602/19602 [==============================] - 25s 1ms/step - loss: 0.0507 - mae: 0.1429 - val_loss: 0.0586 - val_mae: 0.1614\n",
      "Epoch 77/80\n",
      "19602/19602 [==============================] - 25s 1ms/step - loss: 0.0508 - mae: 0.1431 - val_loss: 0.0501 - val_mae: 0.1394\n",
      "Epoch 78/80\n",
      "19602/19602 [==============================] - 25s 1ms/step - loss: 0.0506 - mae: 0.1427 - val_loss: 0.0449 - val_mae: 0.1318\n",
      "Epoch 79/80\n",
      "19602/19602 [==============================] - 26s 1ms/step - loss: 0.0506 - mae: 0.1427 - val_loss: 0.0486 - val_mae: 0.1400\n",
      "Epoch 80/80\n",
      "19602/19602 [==============================] - 25s 1ms/step - loss: 0.0508 - mae: 0.1432 - val_loss: 0.0464 - val_mae: 0.1340\n",
      "processing fold # 1\n",
      "Train on 19602 samples, validate on 19602 samples\n",
      "Epoch 1/80\n",
      "19602/19602 [==============================] - 26s 1ms/step - loss: 0.0509 - mae: 0.1430 - val_loss: 0.0606 - val_mae: 0.1628\n",
      "Epoch 2/80\n",
      "19602/19602 [==============================] - 26s 1ms/step - loss: 0.0508 - mae: 0.1431 - val_loss: 0.0521 - val_mae: 0.1454\n",
      "Epoch 3/80\n",
      "19602/19602 [==============================] - 26s 1ms/step - loss: 0.0509 - mae: 0.1434 - val_loss: 0.0458 - val_mae: 0.1315\n",
      "Epoch 4/80\n",
      "19602/19602 [==============================] - 25s 1ms/step - loss: 0.0509 - mae: 0.1433 - val_loss: 0.0601 - val_mae: 0.1618\n",
      "Epoch 5/80\n",
      "19602/19602 [==============================] - 25s 1ms/step - loss: 0.0508 - mae: 0.1431 - val_loss: 0.0466 - val_mae: 0.1338\n",
      "Epoch 6/80\n",
      "19602/19602 [==============================] - 26s 1ms/step - loss: 0.0508 - mae: 0.1430 - val_loss: 0.0537 - val_mae: 0.1509\n",
      "Epoch 7/80\n",
      "19602/19602 [==============================] - 26s 1ms/step - loss: 0.0509 - mae: 0.1435 - val_loss: 0.0571 - val_mae: 0.1554\n",
      "Epoch 8/80\n",
      "19602/19602 [==============================] - 25s 1ms/step - loss: 0.0508 - mae: 0.1431 - val_loss: 0.0468 - val_mae: 0.1377\n",
      "Epoch 9/80\n",
      "19602/19602 [==============================] - 25s 1ms/step - loss: 0.0509 - mae: 0.1435 - val_loss: 0.0559 - val_mae: 0.1542\n",
      "Epoch 10/80\n",
      "19602/19602 [==============================] - 26s 1ms/step - loss: 0.0509 - mae: 0.1436 - val_loss: 0.0450 - val_mae: 0.1321\n",
      "Epoch 11/80\n",
      "19602/19602 [==============================] - 26s 1ms/step - loss: 0.0508 - mae: 0.1430 - val_loss: 0.0485 - val_mae: 0.1368\n",
      "Epoch 12/80\n",
      "19602/19602 [==============================] - 26s 1ms/step - loss: 0.0508 - mae: 0.1431 - val_loss: 0.0464 - val_mae: 0.1381\n",
      "Epoch 13/80\n",
      "19602/19602 [==============================] - 26s 1ms/step - loss: 0.0510 - mae: 0.1436 - val_loss: 0.0470 - val_mae: 0.1324\n",
      "Epoch 14/80\n",
      "19602/19602 [==============================] - 26s 1ms/step - loss: 0.0509 - mae: 0.1434 - val_loss: 0.0441 - val_mae: 0.1309\n",
      "Epoch 15/80\n",
      "19602/19602 [==============================] - 26s 1ms/step - loss: 0.0508 - mae: 0.1432 - val_loss: 0.0440 - val_mae: 0.1300\n",
      "Epoch 16/80\n",
      "19602/19602 [==============================] - 25s 1ms/step - loss: 0.0508 - mae: 0.1431 - val_loss: 0.0504 - val_mae: 0.1451\n",
      "Epoch 17/80\n",
      "19602/19602 [==============================] - 25s 1ms/step - loss: 0.0508 - mae: 0.1431 - val_loss: 0.0573 - val_mae: 0.1560\n",
      "Epoch 18/80\n",
      "19602/19602 [==============================] - 25s 1ms/step - loss: 0.0509 - mae: 0.1435 - val_loss: 0.0495 - val_mae: 0.1434\n",
      "Epoch 19/80\n",
      "19602/19602 [==============================] - 26s 1ms/step - loss: 0.0508 - mae: 0.1432 - val_loss: 0.0436 - val_mae: 0.1265\n",
      "Epoch 20/80\n",
      "19602/19602 [==============================] - 27s 1ms/step - loss: 0.0509 - mae: 0.1434 - val_loss: 0.0436 - val_mae: 0.1287\n",
      "Epoch 21/80\n",
      "19602/19602 [==============================] - 1235s 63ms/step - loss: 0.0509 - mae: 0.1435 - val_loss: 0.0568 - val_mae: 0.1560\n",
      "Epoch 22/80\n",
      "19602/19602 [==============================] - 26s 1ms/step - loss: 0.0508 - mae: 0.1431 - val_loss: 0.0448 - val_mae: 0.1302\n",
      "Epoch 23/80\n",
      "19602/19602 [==============================] - 1236s 63ms/step - loss: 0.0509 - mae: 0.1434 - val_loss: 0.0490 - val_mae: 0.1428\n",
      "Epoch 24/80\n",
      "19602/19602 [==============================] - 1266s 65ms/step - loss: 0.0509 - mae: 0.1434 - val_loss: 0.0585 - val_mae: 0.1570\n",
      "Epoch 25/80\n",
      "19602/19602 [==============================] - 2071s 106ms/step - loss: 0.0509 - mae: 0.1435 - val_loss: 0.0440 - val_mae: 0.1300\n",
      "Epoch 26/80\n",
      "19602/19602 [==============================] - 1270s 65ms/step - loss: 0.0508 - mae: 0.1432 - val_loss: 0.0465 - val_mae: 0.1358\n",
      "Epoch 27/80\n",
      "19602/19602 [==============================] - 1270s 65ms/step - loss: 0.0508 - mae: 0.1431 - val_loss: 0.0480 - val_mae: 0.1378\n",
      "Epoch 28/80\n",
      "19602/19602 [==============================] - 1270s 65ms/step - loss: 0.0508 - mae: 0.1432 - val_loss: 0.0522 - val_mae: 0.1465\n",
      "Epoch 29/80\n",
      "19602/19602 [==============================] - 2463s 126ms/step - loss: 0.0508 - mae: 0.1431 - val_loss: 0.0448 - val_mae: 0.1314\n",
      "Epoch 30/80\n",
      "19602/19602 [==============================] - 1268s 65ms/step - loss: 0.0508 - mae: 0.1432 - val_loss: 0.0512 - val_mae: 0.1397\n",
      "Epoch 31/80\n",
      "19602/19602 [==============================] - 1179s 60ms/step - loss: 0.0509 - mae: 0.1433 - val_loss: 0.0522 - val_mae: 0.1470\n",
      "Epoch 32/80\n",
      "19602/19602 [==============================] - 69s 4ms/step - loss: 0.0509 - mae: 0.1432 - val_loss: 0.0537 - val_mae: 0.1481\n",
      "Epoch 33/80\n",
      "19602/19602 [==============================] - 67s 3ms/step - loss: 0.0508 - mae: 0.1430 - val_loss: 0.0455 - val_mae: 0.1348\n",
      "Epoch 34/80\n",
      "19602/19602 [==============================] - 63s 3ms/step - loss: 0.0508 - mae: 0.1433 - val_loss: 0.0514 - val_mae: 0.1457\n",
      "Epoch 35/80\n",
      "19602/19602 [==============================] - 65s 3ms/step - loss: 0.0509 - mae: 0.1434 - val_loss: 0.0467 - val_mae: 0.1346\n",
      "Epoch 36/80\n",
      "19602/19602 [==============================] - 66s 3ms/step - loss: 0.0509 - mae: 0.1435 - val_loss: 0.0471 - val_mae: 0.1348\n",
      "Epoch 37/80\n",
      "19602/19602 [==============================] - 67s 3ms/step - loss: 0.0508 - mae: 0.1433 - val_loss: 0.0550 - val_mae: 0.1502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/80\n",
      "19602/19602 [==============================] - 67s 3ms/step - loss: 0.0509 - mae: 0.1434 - val_loss: 0.0633 - val_mae: 0.1660\n",
      "Epoch 39/80\n",
      "19602/19602 [==============================] - 64s 3ms/step - loss: 0.0509 - mae: 0.1433 - val_loss: 0.0468 - val_mae: 0.1372\n",
      "Epoch 40/80\n",
      "19602/19602 [==============================] - 63s 3ms/step - loss: 0.0508 - mae: 0.1430 - val_loss: 0.0560 - val_mae: 0.1542\n",
      "Epoch 41/80\n",
      "19602/19602 [==============================] - 66s 3ms/step - loss: 0.0508 - mae: 0.1432 - val_loss: 0.0550 - val_mae: 0.1516\n",
      "Epoch 42/80\n",
      "19602/19602 [==============================] - 66s 3ms/step - loss: 0.0508 - mae: 0.1433 - val_loss: 0.0510 - val_mae: 0.1448\n",
      "Epoch 43/80\n",
      "19602/19602 [==============================] - 63s 3ms/step - loss: 0.0509 - mae: 0.1435 - val_loss: 0.0545 - val_mae: 0.1509\n",
      "Epoch 44/80\n",
      "19602/19602 [==============================] - 66s 3ms/step - loss: 0.0509 - mae: 0.1434 - val_loss: 0.0469 - val_mae: 0.1339\n",
      "Epoch 45/80\n",
      "19602/19602 [==============================] - 64s 3ms/step - loss: 0.0508 - mae: 0.1432 - val_loss: 0.0460 - val_mae: 0.1339\n",
      "Epoch 46/80\n",
      "19602/19602 [==============================] - 66s 3ms/step - loss: 0.0509 - mae: 0.1434 - val_loss: 0.0622 - val_mae: 0.1666\n",
      "Epoch 47/80\n",
      "19602/19602 [==============================] - 63s 3ms/step - loss: 0.0509 - mae: 0.1434 - val_loss: 0.0468 - val_mae: 0.1363\n",
      "Epoch 48/80\n",
      "19602/19602 [==============================] - 66s 3ms/step - loss: 0.0509 - mae: 0.1436 - val_loss: 0.0519 - val_mae: 0.1492\n",
      "Epoch 49/80\n",
      "19602/19602 [==============================] - 66s 3ms/step - loss: 0.0509 - mae: 0.1433 - val_loss: 0.0548 - val_mae: 0.1501\n",
      "Epoch 50/80\n",
      "19602/19602 [==============================] - 71s 4ms/step - loss: 0.0509 - mae: 0.1434 - val_loss: 0.0509 - val_mae: 0.1428\n",
      "Epoch 51/80\n",
      "19602/19602 [==============================] - 64s 3ms/step - loss: 0.0508 - mae: 0.1431 - val_loss: 0.0572 - val_mae: 0.1567\n",
      "Epoch 52/80\n",
      "19602/19602 [==============================] - 65s 3ms/step - loss: 0.0509 - mae: 0.1435 - val_loss: 0.0445 - val_mae: 0.1312\n",
      "Epoch 53/80\n",
      "19602/19602 [==============================] - 65s 3ms/step - loss: 0.0510 - mae: 0.1438 - val_loss: 0.0544 - val_mae: 0.1522\n",
      "Epoch 54/80\n",
      "19602/19602 [==============================] - 65s 3ms/step - loss: 0.0509 - mae: 0.1432 - val_loss: 0.0529 - val_mae: 0.1482\n",
      "Epoch 55/80\n",
      "19602/19602 [==============================] - 66s 3ms/step - loss: 0.0508 - mae: 0.1429 - val_loss: 0.0439 - val_mae: 0.1303\n",
      "Epoch 56/80\n",
      "19602/19602 [==============================] - 65s 3ms/step - loss: 0.0510 - mae: 0.1437 - val_loss: 0.0497 - val_mae: 0.1420\n",
      "Epoch 57/80\n",
      "19602/19602 [==============================] - 64s 3ms/step - loss: 0.0509 - mae: 0.1434 - val_loss: 0.0460 - val_mae: 0.1340\n",
      "Epoch 58/80\n",
      "19602/19602 [==============================] - 64s 3ms/step - loss: 0.0510 - mae: 0.1436 - val_loss: 0.0545 - val_mae: 0.1505\n",
      "Epoch 59/80\n",
      "19602/19602 [==============================] - 1265s 65ms/step - loss: 0.0509 - mae: 0.1435 - val_loss: 0.0466 - val_mae: 0.1359\n",
      "Epoch 60/80\n",
      "19602/19602 [==============================] - 2462s 126ms/step - loss: 0.0509 - mae: 0.1434 - val_loss: 0.0501 - val_mae: 0.1414\n",
      "Epoch 61/80\n",
      "19602/19602 [==============================] - 1277s 65ms/step - loss: 0.0509 - mae: 0.1432 - val_loss: 0.0512 - val_mae: 0.1421\n",
      "Epoch 62/80\n",
      "19602/19602 [==============================] - 1269s 65ms/step - loss: 0.0508 - mae: 0.1432 - val_loss: 0.0498 - val_mae: 0.1406\n",
      "Epoch 63/80\n",
      "19602/19602 [==============================] - 2467s 126ms/step - loss: 0.0509 - mae: 0.1435 - val_loss: 0.0452 - val_mae: 0.1309\n",
      "Epoch 64/80\n",
      "19602/19602 [==============================] - 1269s 65ms/step - loss: 0.0509 - mae: 0.1434 - val_loss: 0.0949 - val_mae: 0.2141\n",
      "Epoch 65/80\n",
      "19602/19602 [==============================] - 1271s 65ms/step - loss: 0.0509 - mae: 0.1432 - val_loss: 0.0444 - val_mae: 0.1292\n",
      "Epoch 66/80\n",
      "19602/19602 [==============================] - 2471s 126ms/step - loss: 0.0509 - mae: 0.1435 - val_loss: 0.0614 - val_mae: 0.1639\n",
      "Epoch 67/80\n",
      "19602/19602 [==============================] - 1262s 64ms/step - loss: 0.0508 - mae: 0.1432 - val_loss: 0.0476 - val_mae: 0.1344\n",
      "Epoch 68/80\n",
      "19602/19602 [==============================] - 1282s 65ms/step - loss: 0.0509 - mae: 0.1435 - val_loss: 0.0513 - val_mae: 0.1454\n",
      "Epoch 69/80\n",
      "19602/19602 [==============================] - 1264s 65ms/step - loss: 0.0508 - mae: 0.1431 - val_loss: 0.0500 - val_mae: 0.1425\n",
      "Epoch 70/80\n",
      "19602/19602 [==============================] - 1256s 64ms/step - loss: 0.0509 - mae: 0.1432 - val_loss: 0.0523 - val_mae: 0.1428\n",
      "Epoch 71/80\n",
      "19602/19602 [==============================] - 2460s 126ms/step - loss: 0.0508 - mae: 0.1433 - val_loss: 0.0485 - val_mae: 0.1397\n",
      "Epoch 72/80\n",
      "19602/19602 [==============================] - 1256s 64ms/step - loss: 0.0509 - mae: 0.1432 - val_loss: 0.0439 - val_mae: 0.1284\n",
      "Epoch 73/80\n",
      "19602/19602 [==============================] - 1256s 64ms/step - loss: 0.0509 - mae: 0.1433 - val_loss: 0.0517 - val_mae: 0.1426\n",
      "Epoch 74/80\n",
      "19602/19602 [==============================] - 1257s 64ms/step - loss: 0.0510 - mae: 0.1434 - val_loss: 0.0447 - val_mae: 0.1287\n",
      "Epoch 75/80\n",
      "19602/19602 [==============================] - 1265s 65ms/step - loss: 0.0509 - mae: 0.1432 - val_loss: 0.0614 - val_mae: 0.1638\n",
      "Epoch 76/80\n",
      "19602/19602 [==============================] - 1259s 64ms/step - loss: 0.0508 - mae: 0.1433 - val_loss: 0.0456 - val_mae: 0.1304\n",
      "Epoch 77/80\n",
      "19602/19602 [==============================] - 1253s 64ms/step - loss: 0.0509 - mae: 0.1435 - val_loss: 0.0541 - val_mae: 0.1527\n",
      "Epoch 78/80\n",
      "19602/19602 [==============================] - 1256s 64ms/step - loss: 0.0508 - mae: 0.1432 - val_loss: 0.0548 - val_mae: 0.1512\n",
      "Epoch 79/80\n",
      "19602/19602 [==============================] - 1257s 64ms/step - loss: 0.0510 - mae: 0.1437 - val_loss: 0.0519 - val_mae: 0.1476\n",
      "Epoch 80/80\n",
      "19602/19602 [==============================] - 1256s 64ms/step - loss: 0.0508 - mae: 0.1429 - val_loss: 0.0449 - val_mae: 0.1290\n"
     ]
    }
   ],
   "source": [
    "k = 2\n",
    "num_val_samples = len(train_inputs) // k\n",
    "validation_scores = []\n",
    "num_epochs = 80\n",
    "histories = []\n",
    "nmse = []\n",
    "\n",
    "for i in range(k):\n",
    "    print('processing fold #', i)\n",
    "    val_data = train_inputs[i * num_val_samples: (i + 1) * num_val_samples] \n",
    "    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    \n",
    "    partial_train_data = np.concatenate(\n",
    "        [train_inputs[:i * num_val_samples],\n",
    "         train_inputs[(i + 1) * num_val_samples:]], axis=0)\n",
    "    partial_train_targets = np.concatenate(\n",
    "        [train_targets[:i * num_val_samples],\n",
    "         train_targets[(i + 1) * num_val_samples:]], axis=0)\n",
    "    \n",
    " \n",
    "    model = get_model_with(train_inputs, 12)\n",
    "    \n",
    "    history = model.fit(partial_train_data, partial_train_targets,\n",
    "                        validation_data=(val_data, val_targets),\n",
    "                        epochs=num_epochs, batch_size=1, verbose=1)\n",
    "    histories.append(history.history)\n",
    "    \n",
    "    predictions_targets = model.predict(val_data)\n",
    "    nmse.append(np.mean((predictions_targets - val_targets)**2)/np.var(val_targets))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMSE: \n",
      "1.1523465355787998\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvDW2N/gAAIABJREFUeJzt3X+QHOWd3/H3d1crxAIGeSVTBqFdOSY+wHACLQQOjsTGnAVJMM6BD3mdQMpV8snmzCUxMZRs6nCiii/n8g98Nrbs+PxDexBCjE3ZlBHmhE1ibFiJHwZJgIxW0oKNFlXEIYQwkr75o3uk0Whmen50T//6vKqmdqanZ/bbPd397X6ep5/H3B0REZFm+tIOQEREsk/JQkREIilZiIhIJCULERGJpGQhIiKRlCxERCSSkoWIiERSshARkUhKFiIiEmlG2gHEZc6cOT4yMpJ2GCIiubJ27dqX3H1u1HyFSRYjIyNMTEykHYaISK6Y2ZZW5lMxlIiIRFKyEBGRSEoWIiISqTB1FvW88cYbTE1NsWfPnrRDSdysWbOYN28eAwMDaYciIgVU6GQxNTXFMcccw8jICGaWdjiJcXd27NjB1NQUCxYsSDscESmgQhdD7dmzh6GhoUInCgAzY2hoqBRXUCKHGR+HkRHo6wv+jo+nHVEhFfrKAih8oqgoy3KKHGJ8HJYuhd27g9dbtgSvAcbG0ourgAp9ZSEiBbd8+cFEUbF7dzBdYqVkkbCdO3fy1a9+taPPfvGLX2R37Y4gIgdt3dredOmYkkW1BMo+lSxEEjR/fnvTpWOFr7NoWUJlnzfccAO/+c1vWLhwIRdffDFvectbuOOOO3j99dd5//vfz80338yrr77KBz7wAaampti3bx+f/vSnefHFF3nhhRd417vexZw5c1izZk0MCylSMCtWHLrfAgwOBtMlVkoWFc3KPrtIFp/97Gd58skneeyxx1i9ejV33nknDz/8MO7OZZddxs9//nOmp6c54YQT+PGPfwzAyy+/zLHHHsvnP/951qxZw5w5c7pZMpHiquyby5cHRU/z5weJQpXbsVMxVEUPyj5Xr17N6tWrOfPMMznrrLPYuHEjzz77LKeffjr33Xcfn/zkJ3nwwQc59thjY/ufIoU3NgaTk7B/f/BXiSIRurKomD8/KHqqNz0m7s6NN97IRz7ykcPeW7duHffccw+f+tSnuOiii7jpppti+78iIt3SlUXFihVBWWe1GMo+jznmGF555RUA3vve9/Ktb32LXbt2AfD888+zfft2XnjhBQYHB/nQhz7E9ddfz7p16w77rIhImnRlUZFQ2efQ0BDnn38+73znO7nkkkv44Ac/yHnnnQfA0UcfzapVq9i0aRPXX389fX19DAwMcOuttwKwdOlSFi9ezAknnKAKbhFJlbl72jHEYnR01GsHP9qwYQOnnHJKShH1XtmWV0S6Z2Zr3X00aj4VQ4mISCQlCxERiaRkISIikZQsREQkkpKFiIhEUrIQEZFIShYJ67TX2UsvvZSdO3cmEJGISPuULKokMTpjo2Sxd+/epp+75557OO6447oPQEQkBrqDO5TU6IzVXZQPDAwwa9YsZs+ezcaNG3nmmWe4/PLL2bZtG3v27OG6665jafhPR0ZGmJiYYNeuXVxyySVccMEF/OIXv+DEE0/khz/8IUceeWSXSywi0gZ3L8Rj0aJFXmv9+vWHTWtkeNgdDn8MD7f8FXVt3rzZTzvtNHd3X7NmjQ8ODvpzzz134P0dO3a4u/vu3bv9tNNO85deeimMZ9inp6d98+bN3t/f748++qi7u1955ZX+ve99r+7/amd5RUTc3YEJb+EYqyuLUK9GZzznnHNYsGDBgde33HILd911FwDbtm3j2WefZWho6JDPLFiwgIULFwKwaNEiJicn4w1KRCSC6ixCvRqd8aijjjrw/IEHHuCnP/0pDz30EI8//jhnnnkme/bsOewzRxxxxIHn/f39kfUdIiJxSzRZmNliM3vazDaZ2Q113r/QzNaZ2V4zu6Jq+rvM7LGqxx4zuzzJWBPqobxpN+Mvv/wys2fPZnBwkI0bN/LLX/6yu38mIpKQxIqhzKwf+ApwMTAFPGJmd7v7+qrZtgLXAJ+o/qy7rwEWht/zZmATsDqpWCG50Rmruyg/8sgjOf744w+8t3jxYr72ta9xyimn8I53vINzzz23u38mIpKQxLooN7PzgL9y9/eGr28EcPf/VmfebwM/cvc767y3FPjn7t70sK0uysu3vCLSvSx0UX4isK3q9VQ4rV1XAbfFEpGIiHQk0xXcZvZW4HTg3gbvLzWzCTObmJ6e7m1wIiIlkmSyeB44qer1vHBaOz4A3OXub9R7091Xuvuou4/OnTu37hckVcyWNWVZThFJR5LJ4hHgZDNbYGYzCYqT7m7zO5bQRRHUrFmz2LFjR+EPpO7Ojh07mDVrVtqhiEhBJdYayt33mtm1BEVI/cC33P0pM/sMwR2Dd5vZ2cBdwGzgX5vZze5+GoCZjRBcmfys0xjmzZvH1NQUZSiimjVrFvPmzUs7DBEpqMRaQ/VavdZQIiLSXBZaQ4mISEEoWYiISCQlCxERiaRkISIikZQsREQkkpKFiIhEUrIQEZFIShYiIhJJyUJERCIpWYiISCQlCxERiaRkISIikZQsREQkkpKFiIhEUrIQEZFIShYiIhJJyUJERCIpWYiISCQlCxERiaRkISIikZQsREQkkpKFiIhEUrIQEZFIShYiIhJJyUJERCIpWYiISCQlC5E8GB+HkRHo6wv+jo+nHZGUzIy0AxCRCOPjsHQp7N4dvN6yJXgNMDaWXlxSKrqyEMm65csPJoqK3buD6SI9omQhknVbt7Y3XSQBShYiWTd/fnvTRRKgZFFLFYmSNStWwODgodMGB4PpIj2iZFGtUpG4ZQu4H6xIVMKQNI2NwcqVMDwMZsHflStVuS09Ze6edgyxGB0d9YmJie6+ZGQkSBC1hodhcrK77xYRySAzW+vuo1Hz6cqimioSRUTqSjRZmNliM3vazDaZ2Q113r/QzNaZ2V4zu6LmvflmttrMNpjZejMbSTJWQBWJIiINJJYszKwf+ApwCXAqsMTMTq2ZbStwDfD3db7iu8DfuPspwDnA9qRiPUAViSIidSV5ZXEOsMndn3P33wO3A++rnsHdJ939CWB/9fQwqcxw9/vC+Xa5e81dSQlQRaJ0Sq3opOCSTBYnAtuqXk+F01rxT4GdZvZ9M3vUzP4mvFJJ3thYUJm9f3/wt0iJQge0ZKgVnZRAViu4ZwB/DHwCOBt4G0Fx1SHMbKmZTZjZxPT0dG8jzBsd0JKj7jikBJJMFs8DJ1W9nhdOa8UU8FhYhLUX+AFwVu1M7r7S3UfdfXTu3LldB1xoOqAlR63opASSTBaPACeb2QIzmwlcBdzdxmePM7NKBng3sD6BGMtDB7TkqBWdlEBiySK8IrgWuBfYANzh7k+Z2WfM7DIAMzvbzKaAK4Gvm9lT4Wf3ERRB3W9mvwYM+EZSsZaCDmjJUSs6KYFEx7Nw93uAe2qm3VT1/BGC4ql6n70POCPJ+EplxYpDx0QAHdDiUmkEsXx5cKU2f36wXovUOEJKL6sV3BKH6tZPy5fD1VerWXBSityKTgSNlFdc9UZX+853lCBEpCO6sigqtX4SkRgpWRSVWj+JxK/EN7YqWRSVWj+JxKvkN7YqWRSVmnOKxKvkRbtKFkWlThFF4lXyol0liyJTc874lLisWkIlL9pVshCJUvKyagmVvGhXyUIkSsnLqiVU8qJdc/e0Y4jF6OioT0xMpB2GFFFfX3BFUcssKOITyTEzW+vuo1Hz6cpCJErJy6olRjmu+1KyEIlS8rJqiUnO676ULESilLysWmKS87ov1VmIiPRCRuu+VGchIpIlOa/7aposzOxNTd7LxxKKiGRBzuu+oq4sHqg8MbP7a977QezRiIgUVc7rvqIGP7Kq529u8p6IiEQZG8tNcqgVdWXhDZ7Xey0iIgUVdWXxFjP7jwRXEZXnhK/nJhqZiIhkRlSy+AZwTJ3nAN9MJCIREcmcpsnC3W9u9J6ZnR1/OCIikkVRVxaHMLNTgSXhYycQeSOHiIjkX+RNeWY2YmY3mtkTwPeAZcB7Wrnjr/Ry3GmYiEi1qJvyHgJ+THAF8qfuvgh4xd0nexBbvuW80zCR2OikqRCirixeJKjUPp6DrZ/UZLYVOe80TCQWOmkqjKbJwt0vB04H1gJ/ZWabgdlmdk4vgsu1kg/uLgJ0dtKkK5FMiqyzcPeX3f3v3P1PgHOBm4AvmNm2xKPLs5x3GiYSi3ZPmnQlkllt9Trr7i+6+5fd/XzggoRiKoacdxpWejq7jUe7J00qvs2sqAruuxs9gC/3KMZ8ynmnYYXTzsFfZ7fxafekScW3mdV08CMzmwa2AbcBv6Km80B3/1mi0bVBgx9JQ5WDf/UZ6+Bg4+Q9MhIkiFrDwzA5mVSUxTU+HlwZbN0aXFGsWNH4pEnrvudaHfwoKln0AxcT3IR3BkEz2tvc/am4Ao2LkoU01O4BKKMjmpVCu4lduhbLSHnuvs/df+LuVxNUbm8CHjCza2OKUyR57RZtqHFCelR8m1mt3MF9hJn9G2AV8DHgFuCupAOTDqhStr52D/5qnJCusbHgim///uCvEkUmRFVwfxd4CDgLuNndz3b3/+Luz7fy5Wa22MyeNrNNZnZDnfcvNLN1ZrbXzK6oeW+fmT0WPu5uY5nKSZWyjbV78NfZrchhouos9gOvhi+rZzTA3b3ZGN39wDMEdR5TwCPAEndfXzXPCPAm4BPA3e5+Z9V7u9z96FYXpPR1FqoYbK6dSlaREomrzqLP3Y8JH2+qehzTLFGEzgE2uftz7v574HbgfTXfP+nuTwCqNeyWmhw2p6KN8lLxbCzauimvTScSNLutmAqntWqWmU2Y2S/N7PJ6M5jZ0nCeienp6W5izT9VyoocTsWzsUkyWXRrOLw0+iDwRTP7J7UzuPtKdx9199G5c0s+yqsqZUUOpzvCY5NksngeOKnq9bxwWksqleju/hzwAHBmnMEVTtkrZVXUIPWoeDY2SSaLR4CTzWyBmc0ErgJaatVkZrPN7Ijw+RzgfGB9809JacvlVdQgjah4NjaJJQt33wtcC9wLbADucPenzOwzZnYZBON4m9kUcCXwdTOr3Bl+CjBhZo8Da4DPVreikoKI62pARQ3SiIpn4+PuhXgsWrTIJWWrVrkPD7ubBX9XrWo+7+Cge3AtEDwGB5t/phGzQ7+n8jDrdEmkSNrZLksImPAWjrFN77PIk9LfZ5G2NDvr0z0mIh2L5T4LiaBK1YPaLQrqZFCcRutaRQ3SDe3HrWnl8iMPj54XQ8VZjFIE7RYFDQ/Xn394+PB5W1nXKmpIT57XvfbjlouhUj/Ix/XoebJo52BXBu2uj3Z2Uq3r7Mr7wVbbVsvJQsVQnVL77UMl2Vmf1nV25b0lmratlilZdErttw/VyU2Brd4XonWdXXk/2GrbapmSRadUqXq4pG4K1LrOrrwfbLVttUzJolNl716jl7SusyvvB1ttWy3TfRYi0h2NFZJrus9Cuqf259KKsvZJFofafeyjH83sPqdkIfWpc778UpLPh3r72K23ZnafUzGU1KcuNPKp3W5XJD2N9rFaCe9zrRZDKVlIfX19wdlNLbOguEGySUk+PxrtY7US3udUZyHdyXuTyLLK+30PZdLqvpSRfU7JQurLe5PIslKSz496+1itDO1zShZSn9qfx6tXlc5K8vlRbx9btiy7+1wrHUjl4aHBj6QrSfac2uvO9vLcC6z0HOpIUAotzjP1pJsJ97qzPd33IAlQspD8ifvgnvTBXJXOEpcU76FRspD8ifvgnsTBvHqn7muwm6nSWdqR8o2ypU8Wbd9tX/WB8TkfZ2TOrpaTfNFvrO3Z8tU5iI+zhJEtD3T2v+fPDz7PZvrYxwibGWdJ5wfz2p16377D56mpdO523XXz+fGP/h9GZkzRZ/sZmTHFR9+zMdbfsejbfTPtLnvT+dMeO6SVio08PDqp4K5X71j7GBhwHxoK6wqHXvFVA9e4g69iiQ+yq+WRPoeG3GfObF7HWVsvuWxZ89e19ZZR9Zrtvh/1/6LW5SHrLuLz9b6vYSz923wVSw78o3Z/i9pYVi178LDPD/CaDx39Wkfrot7oa6tY4sNMurHPh/u3+bKLNjTdNmrXXbNtod7nK6PcRsVeb9lhf1u/Y7Pfqttli2M/6Pb7mn13t/tE1Lo6ZDs2C7ejzcF2xOZgP2g0dHGL0LCq0RqNqNg0efCaD7H9sB2q8ujvb/zDxzF/ow0vaqfs5P2miXP40A29v7+9eOvtKG3FEv4Oxj7v54221211LK3E3s6yDLPZl/HlAzv1ENt9Jnva/h9JPQ5Z9gbrrpvtLs3liSOeRr8tHD7UfLf7RCuPA9uxvXTYdjTAaz7Ut6Orhm+tJotSd/fR6t32Iu1zwNIOQkqmk27A1N1HC1S/KMlRopDeS7IKo9TJopW77UXKR5fbeZZUi+xSJ4uou+2HhmDmzNa+q78/ep6BgeA7zVqbP4/6+9tfd3H/707XbW9jP/SAPNC/78C20cn/r2xbEHxHu/rZi7GfYbawjK8wzCTGfoaYZiavt/+FdWJLa7tIWzvbVRzHiMRKTFqtQM76I6nuPlpt0dRSa6BlDx74slVDf+GDM99oOn+7LWDirPBu5f/VPtptDdZJJWMrrUaifotuW6Z12xhheOgVX9b3tUNbtQxcc0gA3bSMazf22hY3tcFXt8AZokElaxstx7ptrZRka6tWftvKKup2n2jld+x2O24Fag2VjKZNMJs1q6vzy68auMaHh17puCVD3E1lo/5/N01roz5f3Rqqk+9KetnbWZbIddOoGd7wcHtBdGjVsgd9uH/bgWa8q5Y9GB1bg+RxoPlmj2KvJ+6usNo9oB8y70UbGq/bGGKJezt2V7LInpQPEJIhdc7eD5yyJi2qU8NWbj5KK/asS6LDyB50CqlkkTVpHiDKKMs9r/b6xKF6XTRq/F/9v6vnbzVZ6KQn/t+1R70VK1m0qlcHFV1Z9E6vuwRvVy/ja/VKodFJSyt3rmZp3bqnd6IQ9wlhj44ZShatSHunzdpOVhR5SMxJF7RXvq+Vg32zdRN3Py5JS3M/i3u761FphJJFK9IsDsjaTlYkZSvya3aAbKUoKepgmqftNs0ThbgTVZmuLIDFwNPAJuCGOu9fCKwD9gJX1Hn/TcAU8LdR/6ujZJG1g0qedsosy8OVRZyaLW+j9yodDmVhO4tzu097n45zWcpSZwH0A78B3gbMBB4HTq2ZZwQ4A/hug2TxJeDvE0sWWTqoqJgqPmVbl80OkGmsi267Zs3B2XjPlKE1FHAecG/V6xuBGxvM++3aZAEsAm4HrkksWWTpoFK0jTxtZbpKi9p2erku2t2nctqCqEiykCyuAL5Z9frfNjro1yYLgm5IHgDmJZos3LNzUEn78lnyK0sHyHYP/p1s9724U61EWk0WMzrrJCRxHwXucfcpa9LRjZktBZYCzO+0Q5SxsWwMaD9/fjCyWr3pIs1Utt/ly4Ne5ObPD3rJTGO7bneI2na3+8oohJUR4ypDi8LB5c3KPl0wSXYk+DxwUtXreeG0VpwHXGtmk8DngH9nZp+tncndV7r7qLuPzp07t9t4u9fN+JH1usCtGXpTpKGxMZichP37g79pHSwbHeQbTW93u09jaNEyjwtbrZXLj04ewAzgOWABByu4T2sw77epU8EdvncNSRZDxSWOogBdPkvedbIftLPd97q4NktFfAkh7TqLIAYuBZ4haBW1PJz2GeCy8PnZBE1jXwV2AE/V+Y58JIs8VlBnKTllKRbpTpK/Za/3szzu123KRLLo5SP1ZJG3CuosXQmV4OxNYtLrbSVv+3UHlCx6LY0zkG4O1t3GG+dOW4KzN4lR0leh7Xa8mHNKFr3W6zOebv9ft2dMcR7gS3D2JjnRSseLBbvqbTVZlHpY1VjVG6N15crkWqV02yqk3VYrtdptItlJLH19aoEivVVvv4KDY6MmvV9nmJJFnHrZfLHbg3W3TXW7TTZRsQDs2xecy1Xa0ithSNIa7T/796ffLDllShZ51e3ButsroTjvC6mNpd5I9Um3pReBeE+CCkbJIq/iOFh3cyUUd7FbdSz799efp5MiLsm/uG+Ka/Z9ujm2sVYqNvLwSL2COw29GkAnqf/XSB5aR+m+kN6Iu+FIK99Xst8WtYaStkTtRBpV8KCsx1ckcZ845OFEpMeULKQ9UTuRRhU8KIl1Ubu8y5Zld/m7lWb3HmqmfZhWk4UF8+bf6OioT0xMpB1GfvX1BbtNLbOgDiHq/TKJe13U9qRaz+BgMZps1lvWZss2MlK/V9rh4aCOq11xf18BmNladx+Nmk8V3BKIagWiViIHxb0uGrXtr1aU1mDt3h8Ud4Vzryuwi9RjbSuXH3l4qBiqS1mqs8i6uNdFo6KRIhaVJDHYUbt6VcSZk30G1VlI27LSGioP4lwXjepAmtWJ5PW3KFMFc06WVckiD/K6w3eqbMvbqnb7I8rJGWtdeY69XTmpTFeyyLoy7TTu5VvedrXTGionZ6wNleWkISe/U6vJQq2h0lK2VhllW94kqWVaPrTb8islag2VdXH22poHZVveJKllWj70uifqhClZpKWVHb5Ize50gIuP+i/Kj172RJ0wJYu0RO3wlUvYLVuCIoe8d9OtA1x8CnbGKvmgOos0jY8HNyNt3RqcYa9YcXCHL2IZf7PlFZFUtFpnoWSRVarEFJEeUAV33qmMX0QyRMkiq1TGLyIZomSRVarEFJEMmZF2ANLE2JiSg4hkgq4sREQkkpKFJKdINxWKlJySRZ7k6eBbtJsKRUpOySIv8nbwbXdENBHJNCWLvMjbwTfrHQfm6SpNpJEebsdKFnmR9YNvrSzfVJi3qzTJrjRPOnq8HStZ5EWWD771ZPmmwrxdpUk2pX3S0ePtWMkiL7J88K0nyzcV5u0qTbIp7ZOOHm/HShZ5keWDbyNZ7cs/b1dpkk1pn3T0eDtWsoiSpYrQrB588yZvV2mSTWmfdPR4O040WZjZYjN72sw2mdkNdd6/0MzWmdleM7uiavpwOP0xM3vKzP48yTgbSrtMUpKRx6s0yZ60Tzp6vB0nNp6FmfUDzwAXA1PAI8ASd19fNc8I8CbgE8Dd7n5nOH1mGNvrZnY08CTwR+7+QqP/l8h4FkUcgEhE4lOAAb1aHc8iyY4EzwE2uftzYUC3A+8DDiQLd58M3ztkNB93/33VyyNIq7gs7TJJEcm2EnX2meRB+ERgW9XrqXBaS8zsJDN7IvyOv252VZGYtMskRUQyIrMV3O6+zd3PAN4OXG1mx9fOY2ZLzWzCzCamp6fjDyLtMkkRkYxIMlk8D5xU9XpeOK0t4RXFk8Af13lvpbuPuvvo3LlzOw60IVWEiogAydZZPAKcbGYLCJLEVcAHW/mgmc0Ddrj7a2Y2G7gA+EJikTZTojJJEZFGEruycPe9wLXAvcAG4A53f8rMPmNmlwGY2dlmNgVcCXzdzJ4KP34K8Cszexz4GfA5d/91UrGKiEhziTWd7bVEms6KiBRcq01nM1vBLSIi2aFkISIikZQsREQkUmHqLMxsGqjum2MO8FJK4UTJcmyQ7fiyHBtkO74sxwaKrxvdxDbs7pH3HhQmWdQys4lWKm3SkOXYINvxZTk2yHZ8WY4NFF83ehGbiqFERCSSkoWIiEQqcrJYmXYATWQ5Nsh2fFmODbIdX5ZjA8XXjcRjK2ydhYiIxKfIVxYiIhKTwiWLqKFcU4jnW2a23cyerJr2ZjO7z8yeDf/OTim2k8xsjZmtD4evvS5j8c0ys4fN7PEwvpvD6QvM7Ffhb/w/w5EVU2Fm/Wb2qJn9KIOxTZrZr8PhiSfCaVn5bY8zszvNbKOZbTCz8zIU2zvCdVZ5/KOZ/WWG4vsP4f7wpJndFu4niW93hUoW4VCuXwEuAU4FlpjZqelGxbeBxTXTbgDud/eTgfvD12nYC/wndz8VOBf4WLi+shLf68C73f0PgYXAYjM7F/hr4Avu/nbg/wEfTik+gOsIOsqsyFJsAO9y94VVzSqz8tt+CfiJu/8B8IcE6zATsbn70+E6WwgsAnYDd2UhPjM7Efg4MOru7wT6CXr0Tn67c/fCPIDzgHurXt8I3JiBuEaAJ6tePw28NXz+VuDptGMMY/khwZjpmYsPGATWAf+M4OajGfV+8x7HNI/goPFu4EeAZSW28P9PAnNqpqX+2wLHApsJ60yzFFudWP8E+L9ZiY+DI5C+mWCIiR8B7+3FdleoKwu6HMq1h45399+Gz38HHDYKYK+Z2QhwJvArMhRfWMzzGLAduA/4DbDTgy7wId3f+IvAfwYqY8gPkZ3YABxYbWZrzWxpOC0Lv+0CYBr4u7AI75tmdlRGYqt1FXBb+Dz1+Nz9eeBzwFbgt8DLwFp6sN0VLVnkjgenAqk2STOzo4H/Dfylu/9j9Xtpx+fu+zwoDpgHnAP8QVqxVDOzfwVsd/e1acfSxAXufhZBsezHzOzC6jdT/G1nAGcBt7r7mcCr1BTppL3dAYTl/pcB/6v2vbTiC+tJ3keQcE8AjuLwYu5EFC1ZxDKUaw+8aGZvBQj/bk8rEDMbIEgU4+7+/azFV+HuO4E1BJfYx5lZZZTHtH7j84HLzGwSuJ2gKOpLGYkNOHAWirtvJyhzP4ds/LZTwJS7/yp8fSdB8shCbNUuAda5+4vh6yzE9x5gs7tPu/sbwPcJtsXEt7uiJYsDQ7mGZwVXAXenHFM9dwNXh8+vJqgr6DkzM+B/ABvc/fNVb2Ulvrlmdlz4/EiC+pQNBEnjijTjc/cb3X2eu48QbGf/4O5jWYgNwMyOMrNjKs8Jyt6fJAO/rbv/DthmZu8IJ10ErM9CbDWWcLAICrIR31bgXDMbDPffyrpLfrtLuwIpgQqgS4FnCMq2l2cgntsIyhbfIDij+jBB2fb9wLPAT4E3pxTbBQSX0k8Aj4WPSzMU3xnAo2F8TwI3hdPfBjwMbCIoIjgi5d/4XwA/ylJsYRyPh4+nKvtChn7bhcBE+Nv+AJidldjC+I6qaJb2AAACFElEQVQCdgDHVk3LRHzAzcDGcJ/4HnBEL7Y73cEtIiKRilYMJSIiCVCyEBGRSEoWIiISSclCREQiKVmIiEgkJQuRCGa2r6YX0tg6kDOzEavqkVgkq2ZEzyJSeq950OWISGnpykKkQ+F4Ef89HDPiYTN7ezh9xMz+wcyeMLP7zWx+OP14M7srHJ/jcTP7o/Cr+s3sG+EYBavDu9Uxs49bMNbIE2Z2e0qLKQIoWYi04siaYqg/q3rvZXc/Hfhbgl5oAb4MfMfdzwDGgVvC6bcAP/NgfI6zCO6sBjgZ+Iq7nwbsBP40nH4DcGb4PX+e1MKJtEJ3cItEMLNd7n50nemTBIMzPRd2yPg7dx8ys5cIxj14I5z+W3efY2bTwDx3f73qO0aA+zwYUAcz+yQw4O7/1cx+Auwi6A7jB+6+K+FFFWlIVxYi3fEGz9vxetXzfRysS/yXBCM/ngU8UtWrqEjPKVmIdOfPqv4+FD7/BUFPtABjwIPh8/uBZXBgUKdjG32pmfUBJ7n7GuCTBKPLHXZ1I9IrOlMRiXZkOFpfxU/cvdJ8draZPUFwdbAknPYXBKPAXU8wIty/D6dfB6w0sw8TXEEsI+iRuJ5+YFWYUAy4xYMxPURSoToLkQ6FdRaj7v5S2rGIJE3FUCIiEklXFiIiEklXFiIiEknJQkREIilZiIhIJCULERGJpGQhIiKRlCxERCTS/wcra2xSFTrccwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"NMSE: \")\n",
    "print(np.mean(nmse))\n",
    "\n",
    "val_mae_history = [np.mean([x['val_mae'][i] for x in histories]) for i in range(num_epochs)]\n",
    "mae_history = [np.mean([x['mae'][i] for x in histories]) for i in range(num_epochs)]\n",
    "plt.plot(range(3, len(val_mae_history) + 1), val_mae_history[2:], 'ro')\n",
    "plt.plot(range(3, len(mae_history) + 1), mae_history[2:], 'bo')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend(['test', 'train'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEKCAYAAAAvlUMdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvDW2N/gAAIABJREFUeJzt3X+cHXV97/HXO5sfJIIJbAIFNuymJHhJsARZKVzlitBgUCAqqMFYsOVhuLlNW5UHNVxq7kPEXrBV1IpwU8GLJC3Q1OhWUBCBWrmCbMLP8HMDCdlEZBMhEmMSQj73j/kuOZycs+ec3TM5u8n7+XjM48x85zszn5kzcz5nfisiMDMzq7dhjQ7AzMz2Tk4wZmaWCycYMzPLhROMmZnlwgnGzMxy4QRjZma5cIIxM7NcOMGYmVkunGDMzCwXwxsdQCONHz8+2traGh2GmdmQsnz58g0RMaFSvVwTjKSZwNeBJuDbEXFlUf9RwHeB44GNwMciYrWkGcCVwEhgO3BJRNydhrkXOBT4fRrN6RHxUrlx9RVfW1sbnZ2d9ZhVM7N9hqQ11dTL7RCZpCbgGuAMYCpwnqSpRdUuBF6OiMnA1cBVqXwDcFZEvB24ALipaLg5ETE9NS9VGJeZmTVAnudgTgC6IuK5iNgO3AzMKqozC7gxtS8FTpOkiHgoItan8pXA6LSH0peS4xrwXJiZWb/kmWAOB9YWdHenspJ1ImIHsAloLqpzDrAiIrYVlH1H0sOSPl+QRKoZl5mZ7SGD+iS/pGlkh7pOLyieExHrJB0A/Bvwp2TnXqod51xgLsARRxyxW//XXnuN7u5utm7dOpDQB7399tuPlpYWRowY0ehQzGwvlWeCWQdMLOhuSWWl6nRLGg6MJTtBj6QWYBlwfkSs6h0gItalz1cl/TPZobjv9jWuQhGxCFgE0N7evtvLcLq7uznggANoa2tjbz3CFhFs3LiR7u5uJk2a1OhwzGwvlechsgeBKZImSRoJzAY6iup0kJ3EBzgXuDsiQtI44DZgQUTc11tZ0nBJ41P7COBM4PG+xlVr0Fu3bqW5ublsctm4ER59FDo7s8+Nu6WwwU8Szc3Ne/1empk1Vm57MBGxQ9J84A6yy5RviIiVki4HOiOiA7geuElSF/AbsiQEMB+YDCyUtDCVnQ78DrgjJZcm4C7gn1L/cuOqWV/JZc0a2Lkz696+PesGaB5iZ3v21r0zMxs8cj0HExG3A7cXlS0saN8KfKTEcFcAV5QZ7fFlplVyXPW0bt2u5NJr586sfKglGDOzvPlRMTXYvr228v545ZVX+Na3vtWvYb/2ta+xZcuW+gVjZjYATjA1GDly97KDfrSEPzq7DYYNg7Y2WLJkQNNwgjGzvcWgvkx5sDn88DefgznoR0to/bu5NG1NP+pr1sDcuVn7nDn9msaCBQtYtWoV06dPZ8aMGRx88MHceuutbNu2jQ996EN84Qtf4He/+x0f/ehH6e7u5vXXX+fzn/88v/71r1m/fj3vfe97GT9+PPfcc08d5tjMrP+cYGrQe55l3brssFjLtZftSi69tmyByy7rd4K58sorefzxx3n44Ye58847Wbp0Kb/85S+JCM4++2x+9rOf0dPTw2GHHcZtt90GwKZNmxg7dixf/epXueeeexg/fvxAZtPMrC6cYGrU3FxwQv/FF0pXeqFMeY3uvPNO7rzzTo477jgANm/ezLPPPsvJJ5/MxRdfzOc+9znOPPNMTj755LpMz8ysnpxgBuKII3Zdp1xcXgcRwaWXXspFF120W78VK1Zw++2387d/+7ecdtppLFy4sMQYzMwaxyf5B+JLX4IxY95cNmZMVt5PBxxwAK+++ioA73vf+7jhhhvYvHkzAOvWreOll15i/fr1jBkzhk984hNccsklrFixYrdhzcwazXswA9F7nuWyy7LDYkcckSWXfp5/AWhubuZd73oXxxxzDGeccQYf//jHOemkkwDYf//9Wbx4MV1dXVxyySUMGzaMESNGcO211wIwd+5cZs6cyWGHHeaT/GbWcOrH01T2Gu3t7VH8wrEnn3ySo48+ukER7Vn70ryaWf1IWh4R7ZXq+RCZmZnlwgnGzMxy4QRjZma5cIIxM7NcOMGYmVkunGDMzCwXuSYYSTMlPS2pS9KCEv1HSbol9X9AUlsqnyFpuaTH0uepJYbtkPR4Qfd0SfdLelhSp6QT8py3vPT3acrvf//7eeWVV3KIyMysf3JLMJKagGuAM4CpwHmSphZVuxB4OSImA1cDV6XyDcBZEfF2stcg31Q07g8Dm4vG9WXgCxExHViYunO3ZEn2lP46Pa2/bILZsWNHn8PdfvvtjBs3bmATNzOrozz3YE4AuiLiuYjYDtwMzCqqMwu4MbUvBU6TpIh4KCLWp/KVwGhJowAk7Q98lt3feBnAW1P7WGA9OVuyJHs6/5o1ELHraf0DSTKFj+t/5zvfycknn8zZZ5/N1KlZbv7gBz/I8ccfz7Rp01i0aNEbw7W1tbFhwwZWr17N0Ucfzac+9SmmTZvG6aefzu9///uBzqqZWe0iIpcGOBf4dkH3nwLfLKrzONBS0L0KGF9iPHcVdF8NfAhoAx4vKD8aeAFYC6wDWivFePzxx0exJ554YreyclpbI7LU8uamtbXqUezm+eefj2nTpkVExD333BNjxoyJ55577o3+GzdujIiILVu2xLRp02LDhg0pltbo6emJ559/PpqamuKhhx6KiIiPfOQjcdNNN5WcVi3zambWC+iMKvLAoD7JL2ka2WGzi1L3dODIiFhWovo84DMRMRH4DHB9mXHOTedoOnt6egYUX7mn8tfpaf0AnHDCCUyaNOmN7m984xsce+yxnHjiiaxdu5Znn312t2EmTZrE9OnTATj++ONZvXp1/QIyM6tSnglmHTCxoLsllZWsI2k42aGtjam7BVgGnB8Rq1L9k4B2SauBnwNHSbo39bsA+F5q/1eyQ3S7iYhFEdEeEe0TJkzo98xB+afy1+lp/QC85S1veaP93nvv5a677uIXv/gFjzzyCMcddxxbt27dbZhRo0a90d7U1FTx/I2ZWR7yTDAPAlMkTZI0EpgNdBTV6SBLDJAdCrs7IkLSOOA2YEFE3NdbOSKujYjDIqINeDfwTEScknqvB96T2k8Fdv9rX2c5PK2/z0fub9q0iQMPPJAxY8bw1FNPcf/99/d/QmZmOcvtcf0RsUPSfOAOoAm4ISJWSrqc7PhdB9lhrJskdQG/IUtCAPOBycBCSb1v0jo9Il7qY5KfAr6e9oS2AnPrP1dvlsPT+t/0uP7Ro0dzyCGHvNFv5syZXHfddRx99NG87W1v48QTTxzgHJiZ5ceP6/fj+hsdhpkNMX5cv5mZNZQTjJmZ5cIJpoR94bDhvjCPZtZYTjBF9ttvPzZu3LhX/wBHBBs3bmS//fZrdChmthfL7SqyoaqlpYXu7m4GehPmYLfffvvR0tLS6DDMbC/mBFNkxIgRb7pz3szM+seHyMzMLBdOMGZmlgsnGDMzy4UTjJmZ5cIJxszMcuEEY2ZmuXCCMTOzXDjBmJlZLpxgzMwsF7kmGEkzJT0tqUvSghL9R0m6JfV/QFJbKp8habmkx9LnqSWG7ZD0eFHZX0p6StJKSV/Oa77MzKyy3B4VI6kJuAaYAXQDD0rqiIgnCqpdCLwcEZMlzQauAj4GbADOioj1ko4heyvm4QXj/jCwuWh67wVmAcdGxDZJB+c1b2ZmVlmeezAnAF0R8VxEbAduJksAhWYBN6b2pcBpkhQRD0XE+lS+EhgtaRSApP2BzwJXFI1rHnBlRGwDqPB6ZTMzy1meCeZwYG1BdzcFeyHFdSJiB7AJaC6qcw6wojdxAF8EvgJsKap3FHByOtT2H5LeOfBZMDOz/hrUJ/klTSM7bHZR6p4OHBkRy0pUHw4cBJwIXALcKkklxjlXUqekzr39kfxmZo2UZ4JZB0ws6G5JZSXrSBoOjAU2pu4WYBlwfkSsSvVPAtolrQZ+Dhwl6d7Urxv4XmR+CewExhcHFRGLIqI9ItonTJgw4Jk0M7PS8kwwDwJTJE2SNBKYDXQU1ekALkjt5wJ3R0RIGgfcBiyIiPt6K0fEtRFxWES0Ae8GnomIU1Lv7wPvBZB0FDCS7GIBMzNrgNwSTDqnMp/sCrAngVsjYqWkyyWdnapdDzRL6iI7cd97KfN8YDKwUNLDqal0VdgNwB+mS5dvBi6Ivfm9x2Zmg5z25d/g9vb26OzsbHQYZmZDiqTlEdFeqd6gPslvZmZDlxOMmZnlwgnGzMxy4QRjZma5cIIxM7NcOMGYmVkunGDMzCwXTjBmZpYLJxgzM8uFE4yZmeXCCcbMzHLhBGNmZrlwgjEzs1w4wZiZWS6cYMzMLBdOMGZmlotcE4ykmZKeltQlaUGJ/qMk3ZL6PyCpLZXPkLRc0mPp89QSw3akt1cWl18sKSSNz2OezMysOrklGElNwDXAGcBU4DxJU4uqXQi8HBGTgauBq1L5BuCsiHg7cAFwU9G4PwxsLjHNicDpwAt1nBUzM+uHPPdgTgC6IuK5iNgO3AzMKqozC7gxtS8FTpOkiHgoItan8pXAaEmjACTtD3wWuKLENK8G/gbYd98DbWY2SOSZYA4H1hZ0d6eyknUiYgewCWguqnMOsCIitqXuLwJfAbYUVpI0C1gXEY/0FZSkuZI6JXX29PTUMDtmZlaLQX2SX9I0ssNmF6Xu6cCREbGsqN4Y4H8CCyuNMyIWRUR7RLRPmDAhh6jNzAzyTTDrgIkF3S2prGQdScOBscDG1N0CLAPOj4hVqf5JQLuk1cDPgaMk3QscCUwCHkn9WoAVkv6g7nNlZmZVyTPBPAhMkTRJ0khgNtBRVKeD7CQ+wLnA3RERksYBtwELIuK+3soRcW1EHBYRbcC7gWci4pSIeCwiDo6IttSvG3hHRLyY4/yZmVkfcksw6ZzKfOAO4Eng1ohYKelySWenatcDzZK6yE7c917KPB+YDCyU9HBqDs4rVjMzqz9F7LsXXLW3t0dnZ2ejwzAzG1IkLY+I9kr1BvVJfjMzG7qcYMzMLBdOMGZmlgsnGDMzy4UTjJmZ5cIJxszMcuEEY2ZmuXCCMTOzXDjBmJlZLpxgzMwsF04wZmaWCycYMzPLhROMmZnlwgnGzMxy4QRjZma5qCnBSBoh6bhqX/4laaakpyV1SVpQov8oSbek/g9IakvlMyQtl/RY+jy1xLAdkh4v6P57SU9JelTSsvRWTDMza5A+E4yk6yRNS+1jgUeA7wIPSTqvwrBNwDXAGcBU4DxJU4uqXQi8HBGTgauBq1L5BuCsiHg72SuVbyoa94eBzUXj+glwTET8EfAMcGlf8ZmZWb4q7cGcHBErU/ufAc+kH/3jgb+pMOwJQFdEPBcR24GbgVlFdWYBN6b2pcBpkhQRD0XE+lS+EhgtaRSApP3JXq98ReGIIuLO9JpmgPuBlgrxmZlZjiolmO0F7TOA7wNExItVjPtwYG1Bd3cqK1knJYdNQHNRnXOAFRGxLXV/EfgKsKWPaf858KMqYjQzs5xUSjCvSDpT0nHAu4AfA0gaDozOO7h0eO4q4KLUPR04MiKW9THMZcAOYEmZ/nMldUrq7OnpySFqMzODygnmImA+8B3g0wV7LqcBt1UYdh0wsaC7JZWVrJOS1lhgY+puAZYB50fEqlT/JKBd0mrg58BRku7tHZmkTwJnAnMiIkoFFRGLIqI9ItonTJhQYRbMzKy/hvfVMyKeAWaWKL8DuKPCuB8EpkiaRJZIZgMfL6rTQXYS/xfAucDdERHpCrDbgAURcV/BdK8FrgVIV5z9MCJOSd0zyc4LvSci+jp8ZmZme0Clq8g+JWlKapek70j6bboU+Li+hk3nVOaTJaIngVsjYqWkyyWdnapdDzRL6iI7cd97KfN8YDKwUNLDqal0afQ3gQOAn6T611Wob2ZmOVKZI0lZz+w+k+Mi4jVJHwcuBk4HjgP+V0ScvGfCzEd7e3t0dnY2OgwzsyFF0vKIaK9Ur9I5mB0R8VpqPxP4bkRsjIi7gLcMNEgzM9t7VUowOyUdKmk/shP7dxX0y/0qMjMzG7r6PMkPLAQ6gSago/emS0nvAZ7LOTYzMxvCKl1F9kNJrcABEfFyQa9O4GO5RmZmZkNapT0YgIOAv+h9JhnZo1u+FRG/zi8sMzMb6ipdpvwusvtZIHvI5XdT+wOpn5mZWUmV9mC+AnwwIh4qKOuQtAz4P8Af5xaZmZkNaZWuIntrUXIBICIeJrup0czMrKRKCUaSDixReFAVw5qZ2T6sUpK4GrhT0nskHZCaU8gehf+13KMzM7Mhq9JlyoskrSd7B8s0IIAngCsi4t/3QHxmZjZEVbxMOSJ+CPywuFzSpyPCezFmZlbSQM6jfLZuUZiZ2V5nIAlGdYvCzMz2OgNJMOWf829mZvu8Snfyv5peMFbcvAocVmnkkmZKelpSl6QFJfqPknRL6v9AekslkmZIWi7psfR5aolhO9L7anq7D5L0E0nPps/dLq82M7M9p88EExEHRMRbSzQHRESfFwhIagKuAc4ApgLnSZpaVO1C4OWImEx2SfRVqXwDcFZEvJ3slco3FY37w8DmonEtAH4aEVOAn7Lr7ZhmZtYAed4seQLQFRHPRcR24GZgVlGdWcCNqX0pcJokRcRDEbE+la8ERksaBSBpf7ILDK7oY1w3Ah+s69yYmVlN8kwwhwNrC7q7U1nJOhGxA9gENBfVOQdYERHbUvcXyZ6RtqWo3iER8avU/iJwyICiNzOzARnUj3tJrwi4CrgodU8HjoyIZX0NFxFBmYsQJM2V1Cmps6enp94hm5lZkmeCWQdMLOhuSWUl60gaDowFNqbuFmAZcH5ErEr1TwLaJa0Gfg4cJene1O/Xkg5Nwx4KvFQqqIhYFBHtEdE+YcKEAc2gmZmVl2eCeRCYImmSpJHAbKCjqE4H2Ul8gHOBuyMiJI0DbgMWRMR9vZUj4tqIOCwi2oB3A89ExCklxnUB8IMc5snMzKqUW4JJ51TmA3cATwK3RsRKSZdLOjtVux5oltRFduK+98qv+cBkYKGkh1NzcIVJXgnMkPQs8Cep28zMGkTZ6Yp9U3t7e3R2djY6DDOzIUXS8ohor1RvUJ/kNzOzocsJxszMcuEEY2ZmuXCCMTOzXDjBmJlZLpxgzMwsF04wZmaWCycYMzPLhROMmZnlwgnGzMxy4QRjZma5cIIxM7NcOMGYmVkunGDMzCwXTjBmZpaLXBOMpJmSnpbUJWlBif6jJN2S+j8gqS2Vz5C0XNJj6fPUgmF+LOkRSSslXSepKZVPl3R/ejlZp6QT8pw3MzPrW24JJv3wXwOcAUwFzpM0tajahcDLETEZuBq4KpVvAM6KiLeTvf74poJhPhoRxwLHABOAj6TyLwNfiIjpwMLUbWZmDZLnHswJQFdEPBcR24GbgVlFdWYBN6b2pcBpkhQRD0XE+lS+EhgtaRRARPw2lQ8HRgK9r+QM4K2pfSzQO7yZmTVAngnmcGBtQXd3KitZJyJ2AJuA5qI65wArImJbb4GkO4CXgFfJEhPAp4G/l7QW+Afg0vrMhpmZ9cegPskvaRrZYbOLCssj4n3AocAooPf8zDzgMxExEfgMcH2Zcc5N52g6e3p6covdzGxfl2eCWQdMLOhuSWUl60gaTnZoa2PqbgGWAedHxKrikUfEVuAH7DrsdgHwvdT+r2SH6HYTEYsioj0i2idMmNCP2TIzs2rkmWAeBKZImiRpJDAb6Ciq00GWGADOBe6OiJA0DrgNWBAR9/VWlrS/pENT+3DgA8BTqfd64D2p/VTg2RzmyczMqjQ8rxFHxA5J84E7gCbghohYKelyoDMiOsgOY90kqQv4DVkSApgPTAYWSlqYyk4HBHSkE/7DgHuA61L/TwFfT4lnKzA3r3kzM7PKFBGVa+2l2tvbo7Ozs9FhmJkNKZKWR0R7pXqD+iS/mZkNXU4wZmaWCycYMzPLhROMmZnlwgnGzMxy4QRjZma5cIIxM7NcOMGYmVkunGDMzCwXTjBmZpYLJxgzM8uFE4yZmeXCCcbMzHLhBGNmZrlwgjEzs1w4wZiZWS5yTTCSZkp6WlKXpAUl+o+SdEvq/4CktlQ+Q9JySY+lz1MLhvmxpEckrZR0naSmgn5/Kemp1O/Lec6bmZn1LbdXJqcf/muAGUA38KCkjoh4oqDahcDLETFZ0mzgKuBjwAbgrIhYL+kYstcuH56G+WhE/FaSgKXAR4CbJb0XmAUcGxHbJB2c17yZmVllee7BnAB0RcRzEbEduJksARSaBdyY2pcCp0lSRDwUEetT+UpgtKRRABHx21Q+HBgJ9L7zeR5wZURsS/VeymOmzMysOnkmmMOBtQXd3ezaC9mtTkTsADYBzUV1zgFW9CYOAEl3AC8Br5IlJoCjgJPTobb/kPTOUkFJmiupU1JnT09P/+bMzMwqGtQn+SVNIztsdlFheUS8DzgUGAX0np8ZDhwEnAhcAtyaDqNRNOyiiGiPiPYJEybkGb6Z2T4tzwSzDphY0N2SykrWkTQcGAtsTN0twDLg/IhYVTzyiNgK/IBdh926ge9F5pfATmB83ebGzMxqkmeCeRCYImmSpJHAbKCjqE4HcEFqPxe4OyJC0jjgNmBBRNzXW1nS/pIOTe3DgQ8AT6Xe3wfem/odRXZ+ZkMuc2ZmZhXldhVZROyQNJ/sCrAm4IaIWCnpcqAzIjqA64GbJHUBvyFLQgDzgcnAQkkLU9npgICOdMJ/GHAPcF3qfwNwg6THge3ABRHRewGAmZntYdqXf4Pb29ujs7Oz0WGYmQ0pkpZHRHuleoP6JL+ZmQ1dTjAGS5ZAWxsMG5Z9LlnS6IjMbC+Q2zkYGyKWLIG5c2HLlqx7zZqsG2DOnMbFZWZDnvdg9nWXXbYrufTasiUrNzMbACeYfd0LL9RWbmZWJSeYfd0RR9RWbmZWJSeYfd2XvgRjxry5bMyYrNysEl8gYn1wgtnXzZkDixZBaytI2eeiRT7Bb5X1XiCyZg1E7LpAxEnGEt9o6RstzfqnrS1LKsVaW2H16j0dje1BvtHSzPLlC0SsAicYM+sfXyBiFTjB5MUnP20wq8f66QtErAInmDzsyyc/65VYnaDzU836Wc3yH2oXiHidyuzJ5RAR+2xz/PHHx4AtXhzR2hohZZ+93dmm++amtXXg06tXjHlNZ8yYN8/zmDG1T6+a8eypeSoXXzXT3pMx1jKtSutnf77HRn4f1RjIujnY560WddpGyV65UvE3tuE/8o1sBpxgSn1ZfTXSwKZXrxj7+2NRaUOrV2Ltzw/giBERzc21x1yrapdnvZLtQGMqNf9S3+tnueXf1FQ6/sH+hyCi/+tmrd9jo+ezkjpto4MiwQAzgaeBLrK3Uxb3HwXckvo/ALSl8hnAcuCx9HlqwTA/Bh4BVpK9bKypaJwXAwGMrxTfgBNMuS+rXFP8JQ50Zaxm+L5WqFLDl9ug5s2rvKFV+uGqVn9/APsTc62q3UDrsSFXu36Um1Zzc+n5b24uXb+pqfyy7ytxNTX1Pa97MtmW0991s5bvsR7zmXeCqtM22vAEQ/YWy1XAH5K9vvgRYGpRnf8BXJfaZwO3pPbjgMNS+zHAuoJh3po+BfwbMLug30SyN2iuyS3BFK4AtSSX4n/Ytfz41ZIIiofvK85af3z6Spx9HRrs/eEqtcHUcoix9x90tcu+mh++WjfmvqZdr2S7eHHp76Hc+lHrulgq8eQxfKU/BM3Ne+7ffjV7Zf3Z26tmGuWSUS3bdOF20bte92eZ7S17MMBJwB0F3ZcClxbVuQM4KbUPBzaQbv4sqCOy1ymPKiofAfw78LGCsqXAscDqXBJMrYfECjekkSOrq1tqL6fU4aBhw6obvq8Nq78/MKWacnsMff1Ilpi3xSM+Ga37bwjxerTyfMzjH6OV59/oXsx55ee92kbq17/NxYsjWpvWvjmW4u+l909EpeS224hbs+EqrSulhi/4jhdz3u7Lq9z8F+yBVDVcrU1vrEU/0uWmtXjEJ6O1+dXy/0Xm/eeu5d+0NhbP+88+vqjWyj/e1ay71W5nJebzTcu7OL5a/txJ5deJKveQ3lgk7IxWrXnz9zsUz8EA5wLfLuj+U+CbRXUeB1oKulcVJ4Y0nruKyu4AXgb+ufcQGTAL+HpqL5tggLlAJ9B5xBFH1LRQS23I8Ho08Vofnzt3K2/mpWjmpT6G2fX71Pe4qxh+2OslYxjoZ/l52Fl2vneLsahc7AjYWbQN7d49jB0Di10bonnYxuqWX9OubXzgsRQvm/5+L9lvUe/vUe93XG75lY6xYN5qGq6GeR32epnv+fUS3/HrpWMY1htjmf4lYyxerjvrth1k631PNq6idaS6bbL+22LpdXbXOlbq++1dnn0m6j7sFQkGmJbKjiwx/v3SIbIZwBiyczhjo0KCKWxq3oNJ/1AWc16MYXPRyu7GjRs3Q6/pz+mwahNMnvfBrCM7J9KrJZWVrCNpODAW2Ji6W4BlwPkRsap45BGxFfgB2Z7LkcAk4BFJq9O0Vkj6gzrOzxt3KF/G37GFt9R11GZmjZDn+wXzTDAPAlMkTZI0kuwkfkdRnQ7ggtR+LnB3tqOgccBtZFee3ddbWdL+kg5N7cOBDwBPRcRjEXFwRLRFRBvQDbwjIl6s6xylO5dfwI/CMLO9R16Pjxuez2ghInZImk92vqQJuCEiVkq6nGz3qgO4HrhJUhfZifzZafD5wGRgoaSFqex0shP+HZJGkSXHe8guVd4z0h3KR3ziBdbQtscma2aWp7weH+fH9ffjcf1Lxv8Vczf+bx8my1WQ/Z8o191IgymWcvob456at51pOqWmVSmGobD8B6ve3/tdy2/MmNqf8OPH9edoztf/mEUj5tPKamAnTewAgqZhOwFoasrqZeU7d/tspofm5lRn2M5s2DJ1+/6MAQ5f/FkQexN9jrdZG2ke9ps6THP3z1ZWM49raGU1YietTd3MmydaW0vFVKm7zPKnZ2CxDVubw/IvMS9lxt/XPBQuv/4u9zfFoNf7mMfi9b7SsghaWcNiPsFi5uyaVprP1mFr+4y9v/NWn+0qBrTuVPedV7FODGC7WsycN5bQ1xLGAAAIt0lEQVS72Jn/4+OquRJgb20GdCf/QO+ir1SnXFPqBq9yN+aVulykXL1yN1r1db9IrTf4VTNv9bjEpb/3K1XTlHpcSvG6MG9e5bvcq13+hePvnT5k32M19wNVWg6FsRbHXvzInf7csQ+74iy8J6WvafZ2F85v4c2FpWKs57rY12Ng6rVeFd5A2df2UMvNn/2Zfj/R6MuUh0JTl4dd9qWam/lqXVmq+SHq68eins8m6ys5Ft50WO2NpuU2qFpVurm02mVeHHNeya6/G3tf85HHw1f7+wifSutsPR4jU+sftVpuhC2Ot9pHFfX1VIzC+at1udU6r6WaAT41wQmmiib3BBNR+Qez3MpSr3/z/YmplvGU+vFsbi79L7/Sv/laf/DKqfRDWO1ztOq5nKrZS6hVtT9Mef+Il7ljf7flPpB5qKSWvYt6Paeu0vpf7XKv9fuptKdYqanDduYEU0WzRxJMJdU8f2iwPpk1orYYKz2uo17zt6cfUNgotcRej3Wp0vT6kyzq9YDU4nkst9dc+OenXsukr3FUO41aY6n1cTh1XredYIZKgokYGomkXur10L5K09ibHrHelz0de1/T60+yrtceTK2x7q3y2lsu4gQzlBKM1d+++OMyGPTnn/hQ3YPch1WbYHwfTD/ugzGzOlqyJHtWyQsvZHf8felLg/e1ywZUfx9Mbnfym5lVZc4cJ5S9lG+0NDOzXDjBmJlZLpxgzMwsF04wZmaWCycYMzPLxT59mbKkHmBNjYONBzbkEE49Ocb6cIwDN9jjA8fYH60RMaFSpX06wfSHpM5qrv9uJMdYH45x4AZ7fOAY8+RDZGZmlgsnGDMzy4UTTO0WNTqAKjjG+nCMAzfY4wPHmBufgzEzs1x4D8bMzHLhBFMDSTMlPS2pS9KCQRDPREn3SHpC0kpJf53KD5L0E0nPps8DB0GsTZIekvTD1D1J0gNpWd4iaWSD4xsnaamkpyQ9KemkwbYcJX0mfc+PS/oXSfs1ejlKukHSS5IeLygrudyU+UaK9VFJ72hgjH+fvutHJS2TNK6g36Upxqclva9RMRb0u1hSSBqfuhuyHPvDCaZKkpqAa4AzgKnAeZKmNjYqdgAXR8RU4ETgL1JMC4CfRsQU4Kepu9H+GniyoPsq4OqImAy8DFzYkKh2+Trw44j4L8CxZLEOmuUo6XDgr4D2iDgGaAJm0/jl+H+BmUVl5ZbbGcCU1MwFrm1gjD8BjomIPwKeAS4FSNvPbGBaGuZbadtvRIxImgicDrxQUNyo5VgzJ5jqnQB0RcRzEbEduBmY1ciAIuJXEbEitb9K9qN4eIrrxlTtRuCDjYkwI6kF+ADw7dQt4FRgaarS0BgljQX+G3A9QERsj4hXGGTLkez1GqMlDQfGAL+iwcsxIn4G/KaouNxymwV8N72z6n5gnKRDGxFjRNwZETtS5/1AS0GMN0fEtoh4Hugi2/b3eIzJ1cDfAIUnyxuyHPvDCaZ6hwNrC7q7U9mgIKkNOA54ADgkIn6Ver0IHNKgsHp9jWwj2Zm6m4FXCjbwRi/LSUAP8J10GO/bkt7CIFqOEbEO+Aeyf7K/AjYByxlcy7FXueU2WLehPwd+lNoHTYySZgHrIuKRol6DJsZKnGD2ApL2B/4N+HRE/LawX3q9acMuFZR0JvBSRCxvVAxVGA68A7g2Io4DfkfR4bBBsBwPJPvnOgk4DHgLJQ6pDDaNXm6VSLqM7FDzkkbHUkjSGOB/AgsbHctAOMFUbx0wsaC7JZU1lKQRZMllSUR8LxX/uneXOX2+1Kj4gHcBZ0taTXZY8VSy8x3j0qEeaPyy7Aa6I+KB1L2ULOEMpuX4J8DzEdETEa8B3yNbtoNpOfYqt9wG1TYk6ZPAmcCc2HW/xmCJ8UiyPxOPpG2nBVgh6Q8YPDFW5ARTvQeBKemqnZFkJwI7GhlQOpdxPfBkRHy1oFcHcEFqvwD4wZ6OrVdEXBoRLRHRRrbM7o6IOcA9wLmpWqNjfBFYK+ltqeg04AkG0XIkOzR2oqQx6XvvjXHQLMcC5ZZbB3B+ugrqRGBTwaG0PUrSTLLDtmdHxJaCXh3AbEmjJE0iO5H+yz0dX0Q8FhEHR0Rb2na6gXekdXXQLMeKIsJNlQ3wfrIrTlYBlw2CeN5NdvjhUeDh1Lyf7BzHT4FngbuAgxoda4r3FOCHqf0PyTbcLuBfgVENjm060JmW5feBAwfbcgS+ADwFPA7cBIxq9HIE/oXsnNBrZD+CF5ZbboDIrsRcBTxGdkVco2LsIjuP0bvdXFdQ/7IU49PAGY2Ksaj/amB8I5djfxrfyW9mZrnwITIzM8uFE4yZmeXCCcbMzHLhBGNmZrlwgjEzs1w4wZjlQNLrkh4uaOr2oExJbaWeums22AyvXMXM+uH3ETG90UGYNZL3YMz2IEmrJX1Z0mOSfilpcipvk3R3er/HTyUdkcoPSe8reSQ1/zWNqknSPyl7P8ydkkan+n+l7P1Aj0q6uUGzaQY4wZjlZXTRIbKPFfTbFBFvB75J9qRpgH8Ebozs/SRLgG+k8m8A/xERx5I9H21lKp8CXBMR04BXgHNS+QLguDSe/57XzJlVw3fym+VA0uaI2L9E+Wrg1Ih4Lj2o9MWIaJa0ATg0Il5L5b+KiPGSeoCWiNhWMI424CeRvdALSZ8DRkTEFZJ+DGwme9zN9yNic86zalaW92DM9rwo016LbQXtr7PrfOoHyJ5T9Q7gwYInLZvtcU4wZnvexwo+f5Ha/x/Z06YB5gD/mdp/CsyD7LXd6e2bJUkaBkyMiHuAzwFjgd32osz2FP+7McvHaEkPF3T/OCJ6L1U+UNKjZHsh56WyvyR7o+YlZG/X/LNU/tfAIkkXku2pzCN76m4pTcDilIQEfCOyVz+bNYTPwZjtQekcTHtEbGh0LGZ58yEyMzPLhfdgzMwsF96DMTOzXDjBmJlZLpxgzMwsF04wZmaWCycYMzPLhROMmZnl4v8DMBP1bFvt1v0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_loss_history = [np.mean([x['val_loss'][i] for x in histories]) for i in range(num_epochs)]\n",
    "loss_history = [np.mean([x['loss'][i] for x in histories]) for i in range(num_epochs)]\n",
    "plt.plot(range(1, len(val_loss_history) + 1), val_loss_history, 'ro')\n",
    "plt.plot(range(1, len(loss_history) + 1), loss_history, 'bo')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('LOSS')\n",
    "plt.legend(['test', 'train'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing number of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING NUMBER OF NEURONS  2\n",
      "PROCESSING FOLD # 0\n",
      "Train on 26136 samples, validate on 13068 samples\n",
      "Epoch 1/75\n",
      "26136/26136 [==============================] - 29s 1ms/step - loss: 0.0240 - mae: 0.1253 - val_loss: 0.0234 - val_mae: 0.1249\n",
      "Epoch 2/75\n",
      "26136/26136 [==============================] - 26s 1ms/step - loss: 0.0233 - mae: 0.1246 - val_loss: 0.0234 - val_mae: 0.1249\n",
      "Epoch 3/75\n",
      "26136/26136 [==============================] - 26s 1ms/step - loss: 0.0233 - mae: 0.1246 - val_loss: 0.0234 - val_mae: 0.1250\n",
      "Epoch 4/75\n",
      "26136/26136 [==============================] - 26s 1000us/step - loss: 0.0233 - mae: 0.1246 - val_loss: 0.0234 - val_mae: 0.1248\n",
      "Epoch 5/75\n",
      "26136/26136 [==============================] - 29s 1ms/step - loss: 0.0233 - mae: 0.1246 - val_loss: 0.0234 - val_mae: 0.1251\n",
      "Epoch 6/75\n",
      "26136/26136 [==============================] - 30s 1ms/step - loss: 0.0233 - mae: 0.1246 - val_loss: 0.0234 - val_mae: 0.1254\n",
      "Epoch 7/75\n",
      "26136/26136 [==============================] - 29s 1ms/step - loss: 0.0233 - mae: 0.1246 - val_loss: 0.0234 - val_mae: 0.1248\n",
      "Epoch 8/75\n",
      "26136/26136 [==============================] - 34s 1ms/step - loss: 0.0233 - mae: 0.1246 - val_loss: 0.0235 - val_mae: 0.1251\n",
      "Epoch 9/75\n",
      "23048/26136 [=========================>....] - ETA: 2s - loss: 0.0233 - mae: 0.1245"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-0635ef0114e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m         history = model.fit(partial_train_data, partial_train_targets,\n\u001b[1;32m     30\u001b[0m                             \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                             epochs=num_epochs, batch_size=1, verbose=1)\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mmae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mae'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mval_mae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_mae'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    199\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/callbacks/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         if (self._delta_t_batch > 0. and\n\u001b[0m\u001b[1;32m     90\u001b[0m            \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.95\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_t_batch\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m            delta_t_median > 0.1):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "k = 3\n",
    "num_val_samples = len(train_inputs) // k\n",
    "num_epochs = 75\n",
    "maes = []\n",
    "val_maes = []\n",
    "nmses = []\n",
    "number_neurons = [2, 5, 8, 12, 16];\n",
    "\n",
    "for nn in number_neurons:\n",
    "    mae = []\n",
    "    val_mae = []\n",
    "    nmse = []\n",
    "    print('PROCESSING NUMBER OF NEURONS ', nn);\n",
    "    for i in range(k):\n",
    "        print('PROCESSING FOLD #', i)\n",
    "        val_data = train_inputs[i * num_val_samples: (i + 1) * num_val_samples] \n",
    "        val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "        partial_train_data = np.concatenate(\n",
    "            [train_inputs[:i * num_val_samples],\n",
    "             train_inputs[(i + 1) * num_val_samples:]], axis=0)\n",
    "        partial_train_targets = np.concatenate(\n",
    "            [train_targets[:i * num_val_samples],\n",
    "             train_targets[(i + 1) * num_val_samples:]], axis=0)\n",
    "\n",
    "\n",
    "        model = get_model_with(train_inputs, nn)\n",
    "\n",
    "        history = model.fit(partial_train_data, partial_train_targets,\n",
    "                            validation_data=(val_data, val_targets),\n",
    "                            epochs=num_epochs, batch_size=1, verbose=1)\n",
    "        mae.append(history.history['mae'])\n",
    "        val_mae.append(history.history['val_mae'])\n",
    "        predictions_targets = model.predict(val_data)\n",
    "        nmse.append(np.mean((predictions_targets - val_targets)**2)/np.var(val_targets))\n",
    "        \n",
    "    maes.append(np.mean([x[num_epochs-1] for x in mae]))\n",
    "    val_maes.append(np.mean([x[num_epochs-1] for x in val_mae]))\n",
    "    nmses.append(np.mean(nmse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x136a51c88>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(number_neurons, maes, 'ro')\n",
    "plt.plot(number_neurons, val_maes, 'bo')\n",
    "plt.xlabel('Number of Neurons')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend(['train', 'test'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'NMSE')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(number_neurons, nmses, 'ro')\n",
    "plt.xlabel('Number of Neurons')\n",
    "plt.ylabel('NMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "27130/65341 [===========>..................] - ETA: 31s - loss: 0.0251 - mae: 0.1267"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-2a9db6075a65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m75\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mae'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nn_12_75.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3035\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3037\u001b[0;31m     \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3038\u001b[0m     \u001b[0mfeed_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3039\u001b[0m     \u001b[0marray_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    479\u001b[0m   \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m       \u001b[0m_initialize_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_controller\u001b[0;34m(self, default)\u001b[0m\n\u001b[1;32m   5246\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mtf_contextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5247\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_controller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5248\u001b[0;31m     context.context().context_switches.push(\n\u001b[0m\u001b[1;32m   5249\u001b[0m         default.building_function, default.as_default)\n\u001b[1;32m   5250\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = get_model_with(inputs, 12)\n",
    "\n",
    "history = model.fit(inputs, targets, epochs=75, batch_size=1, verbose=1)\n",
    "history.history['mae']\n",
    "model.save(\"nn_12_75.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [X, Y] model\n",
    "\n",
    "In this part, a neural network is trained using (x,y) coordinates only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold # 0\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 31364 samples, validate on 7840 samples\n",
      "Epoch 1/75\n",
      "31364/31364 [==============================] - 26s 827us/step - loss: 0.0236 - mae: 0.1236 - val_loss: 0.0235 - val_mae: 0.1238\n",
      "Epoch 2/75\n",
      "31364/31364 [==============================] - 27s 861us/step - loss: 0.0236 - mae: 0.1241 - val_loss: 0.0236 - val_mae: 0.1239\n",
      "Epoch 3/75\n",
      "31364/31364 [==============================] - 25s 806us/step - loss: 0.0238 - mae: 0.1248 - val_loss: 0.0260 - val_mae: 0.1292\n",
      "Epoch 4/75\n",
      "31364/31364 [==============================] - 25s 801us/step - loss: 0.0239 - mae: 0.1252 - val_loss: 0.0242 - val_mae: 0.1262\n",
      "Epoch 5/75\n",
      "31364/31364 [==============================] - 31s 987us/step - loss: 0.0238 - mae: 0.1256 - val_loss: 0.0238 - val_mae: 0.1267\n",
      "Epoch 6/75\n",
      "31364/31364 [==============================] - 28s 902us/step - loss: 0.0238 - mae: 0.1256 - val_loss: 0.0239 - val_mae: 0.1258\n",
      "Epoch 7/75\n",
      "31364/31364 [==============================] - 27s 876us/step - loss: 0.0237 - mae: 0.1255 - val_loss: 0.0246 - val_mae: 0.1267\n",
      "Epoch 8/75\n",
      "31364/31364 [==============================] - 25s 795us/step - loss: 0.0237 - mae: 0.1255 - val_loss: 0.0242 - val_mae: 0.1258\n",
      "Epoch 9/75\n",
      "31364/31364 [==============================] - 28s 881us/step - loss: 0.0237 - mae: 0.1254 - val_loss: 0.0237 - val_mae: 0.1261\n",
      "Epoch 10/75\n",
      "31364/31364 [==============================] - 27s 847us/step - loss: 0.0237 - mae: 0.1254 - val_loss: 0.0241 - val_mae: 0.1257\n",
      "Epoch 11/75\n",
      "31364/31364 [==============================] - 29s 917us/step - loss: 0.0237 - mae: 0.1254 - val_loss: 0.0237 - val_mae: 0.1256\n",
      "Epoch 12/75\n",
      "31364/31364 [==============================] - 29s 930us/step - loss: 0.0236 - mae: 0.1252 - val_loss: 0.0240 - val_mae: 0.1261\n",
      "Epoch 13/75\n",
      "31364/31364 [==============================] - 31s 986us/step - loss: 0.0236 - mae: 0.1251 - val_loss: 0.0236 - val_mae: 0.1251\n",
      "Epoch 14/75\n",
      "31364/31364 [==============================] - 31s 979us/step - loss: 0.0236 - mae: 0.1251 - val_loss: 0.0237 - val_mae: 0.1253\n",
      "Epoch 15/75\n",
      "31364/31364 [==============================] - 25s 803us/step - loss: 0.0236 - mae: 0.1251 - val_loss: 0.0238 - val_mae: 0.1258\n",
      "Epoch 16/75\n",
      "31364/31364 [==============================] - 26s 828us/step - loss: 0.0236 - mae: 0.1250 - val_loss: 0.0237 - val_mae: 0.1252\n",
      "Epoch 17/75\n",
      "31364/31364 [==============================] - 28s 881us/step - loss: 0.0235 - mae: 0.1251 - val_loss: 0.0238 - val_mae: 0.1253\n",
      "Epoch 18/75\n",
      "31364/31364 [==============================] - 27s 848us/step - loss: 0.0235 - mae: 0.1251 - val_loss: 0.0236 - val_mae: 0.1257\n",
      "Epoch 19/75\n",
      "31364/31364 [==============================] - 27s 847us/step - loss: 0.0235 - mae: 0.1251 - val_loss: 0.0238 - val_mae: 0.1257\n",
      "Epoch 20/75\n",
      "31364/31364 [==============================] - 26s 821us/step - loss: 0.0235 - mae: 0.1250 - val_loss: 0.0238 - val_mae: 0.1256\n",
      "Epoch 21/75\n",
      "31364/31364 [==============================] - 26s 815us/step - loss: 0.0235 - mae: 0.1250 - val_loss: 0.0238 - val_mae: 0.1255\n",
      "Epoch 22/75\n",
      "31364/31364 [==============================] - 27s 853us/step - loss: 0.0235 - mae: 0.1249 - val_loss: 0.0236 - val_mae: 0.1253\n",
      "Epoch 23/75\n",
      "31364/31364 [==============================] - 31s 996us/step - loss: 0.0234 - mae: 0.1249 - val_loss: 0.0236 - val_mae: 0.1251\n",
      "Epoch 24/75\n",
      "31364/31364 [==============================] - 31s 996us/step - loss: 0.0234 - mae: 0.1248 - val_loss: 0.0235 - val_mae: 0.1248\n",
      "Epoch 25/75\n",
      "31364/31364 [==============================] - 28s 890us/step - loss: 0.0234 - mae: 0.1248 - val_loss: 0.0237 - val_mae: 0.1256\n",
      "Epoch 26/75\n",
      "31364/31364 [==============================] - 27s 866us/step - loss: 0.0234 - mae: 0.1248 - val_loss: 0.0235 - val_mae: 0.1255\n",
      "Epoch 27/75\n",
      "31364/31364 [==============================] - 28s 902us/step - loss: 0.0234 - mae: 0.1247 - val_loss: 0.0239 - val_mae: 0.1256\n",
      "Epoch 28/75\n",
      "31364/31364 [==============================] - 30s 950us/step - loss: 0.0233 - mae: 0.1246 - val_loss: 0.0235 - val_mae: 0.1246\n",
      "Epoch 29/75\n",
      "31364/31364 [==============================] - 28s 901us/step - loss: 0.0233 - mae: 0.1243 - val_loss: 0.0235 - val_mae: 0.1252\n",
      "Epoch 30/75\n",
      "31364/31364 [==============================] - 29s 927us/step - loss: 0.0233 - mae: 0.1242 - val_loss: 0.0234 - val_mae: 0.1249\n",
      "Epoch 31/75\n",
      "31364/31364 [==============================] - 29s 924us/step - loss: 0.0233 - mae: 0.1243 - val_loss: 0.0234 - val_mae: 0.1249\n",
      "Epoch 32/75\n",
      "31364/31364 [==============================] - 27s 849us/step - loss: 0.0233 - mae: 0.1243 - val_loss: 0.0235 - val_mae: 0.1246\n",
      "Epoch 33/75\n",
      "31364/31364 [==============================] - 27s 849us/step - loss: 0.0233 - mae: 0.1243 - val_loss: 0.0234 - val_mae: 0.1247\n",
      "Epoch 34/75\n",
      "31364/31364 [==============================] - 27s 864us/step - loss: 0.0233 - mae: 0.1243 - val_loss: 0.0235 - val_mae: 0.1247\n",
      "Epoch 35/75\n",
      "31364/31364 [==============================] - 29s 916us/step - loss: 0.0233 - mae: 0.1244 - val_loss: 0.0236 - val_mae: 0.1253\n",
      "Epoch 36/75\n",
      "31364/31364 [==============================] - 27s 853us/step - loss: 0.0233 - mae: 0.1244 - val_loss: 0.0235 - val_mae: 0.1246\n",
      "Epoch 37/75\n",
      "31364/31364 [==============================] - 26s 817us/step - loss: 0.0233 - mae: 0.1244 - val_loss: 0.0236 - val_mae: 0.1254\n",
      "Epoch 38/75\n",
      "31364/31364 [==============================] - 31s 988us/step - loss: 0.0233 - mae: 0.1245 - val_loss: 0.0236 - val_mae: 0.1249\n",
      "Epoch 39/75\n",
      "31364/31364 [==============================] - 29s 933us/step - loss: 0.0233 - mae: 0.1244 - val_loss: 0.0235 - val_mae: 0.1254\n",
      "Epoch 40/75\n",
      " 5479/31364 [====>.........................] - ETA: 22s - loss: 0.0233 - mae: 0.1244"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2483d431b0f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m     history = model.fit(partial_train_data, partial_train_targets,\n\u001b[1;32m     29\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                         epochs=num_epochs, batch_size=1, verbose=1)\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mhistories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    199\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/callbacks/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         if (self._delta_t_batch > 0. and\n\u001b[1;32m     90\u001b[0m            \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.95\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_t_batch\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mmedian\u001b[0;34m(a, axis, out, overwrite_input, keepdims)\u001b[0m\n\u001b[1;32m   3495\u001b[0m     \"\"\"\n\u001b[1;32m   3496\u001b[0m     r, k = _ureduce(a, func=_median, axis=axis, out=out,\n\u001b[0;32m-> 3497\u001b[0;31m                     overwrite_input=overwrite_input)\n\u001b[0m\u001b[1;32m   3498\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3499\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_ureduce\u001b[0;34m(a, func, **kwargs)\u001b[0m\n\u001b[1;32m   3378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3379\u001b[0m     \"\"\"\n\u001b[0;32m-> 3380\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3381\u001b[0m     \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'axis'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3382\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masanyarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m     \"\"\"\n\u001b[0;32m--> 591\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# neural network with a 12-neuron hidden layer\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(12, activation='relu',\n",
    "                       input_shape=(2,)))\n",
    "model.add(layers.Dense(8))\n",
    "model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "\n",
    "k = 5\n",
    "num_val_samples = len(train_inputs) // k\n",
    "train_inputs_xy = train_inputs[['x', 'y']]\n",
    "validation_scores = []\n",
    "num_epochs = 75\n",
    "histories = []\n",
    "nmse = []\n",
    "\n",
    "for i in range(k):\n",
    "    print('processing fold #', i)\n",
    "    val_data = train_inputs_xy[i * num_val_samples: (i + 1) * num_val_samples] \n",
    "    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    \n",
    "    partial_train_data = np.concatenate(\n",
    "        [train_inputs_xy[:i * num_val_samples],\n",
    "         train_inputs_xy[(i + 1) * num_val_samples:]], axis=0)\n",
    "    partial_train_targets = np.concatenate(\n",
    "        [train_targets[:i * num_val_samples],\n",
    "         train_targets[(i + 1) * num_val_samples:]], axis=0)\n",
    "    \n",
    "    history = model.fit(partial_train_data, partial_train_targets,\n",
    "                        validation_data=(val_data, val_targets),\n",
    "                        epochs=num_epochs, batch_size=1, verbose=1)\n",
    "    histories.append(history.history)\n",
    "    \n",
    "    predictions_targets = model.predict(val_data)\n",
    "    nmse.append(np.mean((predictions_targets - val_targets)**2)/np.var(val_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

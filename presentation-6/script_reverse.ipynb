{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Analysis of Data\n",
    "\n",
    "## Environment Settings\n",
    "\n",
    "An statistical Analysis of the data captured will be performed.\n",
    "\n",
    "The environment configuration is the following:\n",
    "\n",
    "- A rectangle area is used whose dimension is 2 x 1.5 meters. \n",
    "- A custom robot similar to an epuck was used.\n",
    "- The robot starts in the middle of the arena.\n",
    "- The robot moves in a random fashion way around the environment avoiding obstacles.\n",
    "- The robot has 8 sensors that measure the distance between the robot and the walls.\n",
    "- Some noise was introduced in the sensors measurements of the robot using the concept of [lookup tables](https://cyberbotics.com/doc/reference/distancesensor) in the Webots simulator which according to Webots documentation \"The first column of the table specifies the input distances, the second column specifies the corresponding desired response values, and the third column indicates the desired standard deviation of the noise. The noise on the return value is computed according to a gaussian random number distribution whose range is calculated as a percent of the response value (two times the standard deviation is often referred to as the signal quality)\". The following values were taken:\n",
    "    -Second experiment:\n",
    "        - (0, 0, 0.2)\n",
    "        - (10, 10, 0.2)\n",
    "- The simulator runs during 10 minutes in fast mode which is translated into 12 hours of collected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/site-packages (0.22)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/site-packages (from scikit-learn) (0.14.1)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/site-packages (from scikit-learn) (1.16.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.7/site-packages (2.3.1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.7/site-packages (from keras) (2.9.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/site-packages (from keras) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/site-packages (from keras) (1.16.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/site-packages (from keras) (1.0.7)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/site-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/site-packages (from keras) (1.0.9)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/site-packages (from keras) (5.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install scikit-learn\n",
    "!{sys.executable} -m pip install keras\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>theta</th>\n",
       "      <th>dx</th>\n",
       "      <th>dy</th>\n",
       "      <th>dtheta</th>\n",
       "      <th>sensor_1</th>\n",
       "      <th>sensor_2</th>\n",
       "      <th>sensor_3</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor_7</th>\n",
       "      <th>sensor_8</th>\n",
       "      <th>dsensor_1</th>\n",
       "      <th>dsensor_2</th>\n",
       "      <th>dsensor_3</th>\n",
       "      <th>dsensor_4</th>\n",
       "      <th>dsensor_5</th>\n",
       "      <th>dsensor_6</th>\n",
       "      <th>dsensor_7</th>\n",
       "      <th>dsensor_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.920614</td>\n",
       "      <td>0.761198</td>\n",
       "      <td>168.209483</td>\n",
       "      <td>-0.070670</td>\n",
       "      <td>0.011198</td>\n",
       "      <td>-11.790739</td>\n",
       "      <td>1.085179</td>\n",
       "      <td>0.790267</td>\n",
       "      <td>0.893342</td>\n",
       "      <td>...</td>\n",
       "      <td>1.139790</td>\n",
       "      <td>1.144901</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.850135</td>\n",
       "      <td>0.775909</td>\n",
       "      <td>168.212418</td>\n",
       "      <td>-0.070479</td>\n",
       "      <td>0.014711</td>\n",
       "      <td>0.002935</td>\n",
       "      <td>0.571635</td>\n",
       "      <td>0.596799</td>\n",
       "      <td>0.883340</td>\n",
       "      <td>...</td>\n",
       "      <td>0.830057</td>\n",
       "      <td>1.028332</td>\n",
       "      <td>-0.513544</td>\n",
       "      <td>-0.193468</td>\n",
       "      <td>-0.010002</td>\n",
       "      <td>-0.430864</td>\n",
       "      <td>-0.070277</td>\n",
       "      <td>-0.387726</td>\n",
       "      <td>-0.309733</td>\n",
       "      <td>-0.116568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.779657</td>\n",
       "      <td>0.790625</td>\n",
       "      <td>168.209551</td>\n",
       "      <td>-0.070478</td>\n",
       "      <td>0.014716</td>\n",
       "      <td>-0.002867</td>\n",
       "      <td>0.581452</td>\n",
       "      <td>0.904627</td>\n",
       "      <td>0.689004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.491200</td>\n",
       "      <td>0.889130</td>\n",
       "      <td>0.009817</td>\n",
       "      <td>0.307828</td>\n",
       "      <td>-0.194336</td>\n",
       "      <td>0.239518</td>\n",
       "      <td>0.206480</td>\n",
       "      <td>0.293382</td>\n",
       "      <td>-0.338857</td>\n",
       "      <td>-0.139203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.709174</td>\n",
       "      <td>0.805340</td>\n",
       "      <td>168.212871</td>\n",
       "      <td>-0.070483</td>\n",
       "      <td>0.014715</td>\n",
       "      <td>0.003319</td>\n",
       "      <td>0.956302</td>\n",
       "      <td>0.842911</td>\n",
       "      <td>0.796714</td>\n",
       "      <td>...</td>\n",
       "      <td>1.246415</td>\n",
       "      <td>0.712158</td>\n",
       "      <td>0.374849</td>\n",
       "      <td>-0.061716</td>\n",
       "      <td>0.107710</td>\n",
       "      <td>0.075412</td>\n",
       "      <td>-0.345782</td>\n",
       "      <td>-0.084918</td>\n",
       "      <td>0.755215</td>\n",
       "      <td>-0.176971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.638698</td>\n",
       "      <td>0.820056</td>\n",
       "      <td>168.208857</td>\n",
       "      <td>-0.070477</td>\n",
       "      <td>0.014716</td>\n",
       "      <td>-0.004013</td>\n",
       "      <td>0.671731</td>\n",
       "      <td>0.779896</td>\n",
       "      <td>0.962191</td>\n",
       "      <td>...</td>\n",
       "      <td>0.567806</td>\n",
       "      <td>0.595164</td>\n",
       "      <td>-0.284570</td>\n",
       "      <td>-0.063014</td>\n",
       "      <td>0.165477</td>\n",
       "      <td>0.005216</td>\n",
       "      <td>0.128150</td>\n",
       "      <td>-0.054777</td>\n",
       "      <td>-0.678608</td>\n",
       "      <td>-0.116994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0         x         y       theta        dx        dy     dtheta  \\\n",
       "0           0  0.920614  0.761198  168.209483 -0.070670  0.011198 -11.790739   \n",
       "1           1  0.850135  0.775909  168.212418 -0.070479  0.014711   0.002935   \n",
       "2           2  0.779657  0.790625  168.209551 -0.070478  0.014716  -0.002867   \n",
       "3           3  0.709174  0.805340  168.212871 -0.070483  0.014715   0.003319   \n",
       "4           4  0.638698  0.820056  168.208857 -0.070477  0.014716  -0.004013   \n",
       "\n",
       "   sensor_1  sensor_2  sensor_3    ...      sensor_7  sensor_8  dsensor_1  \\\n",
       "0  1.085179  0.790267  0.893342    ...      1.139790  1.144901        NaN   \n",
       "1  0.571635  0.596799  0.883340    ...      0.830057  1.028332  -0.513544   \n",
       "2  0.581452  0.904627  0.689004    ...      0.491200  0.889130   0.009817   \n",
       "3  0.956302  0.842911  0.796714    ...      1.246415  0.712158   0.374849   \n",
       "4  0.671731  0.779896  0.962191    ...      0.567806  0.595164  -0.284570   \n",
       "\n",
       "   dsensor_2  dsensor_3  dsensor_4  dsensor_5  dsensor_6  dsensor_7  dsensor_8  \n",
       "0        NaN        NaN        NaN        NaN        NaN        NaN        NaN  \n",
       "1  -0.193468  -0.010002  -0.430864  -0.070277  -0.387726  -0.309733  -0.116568  \n",
       "2   0.307828  -0.194336   0.239518   0.206480   0.293382  -0.338857  -0.139203  \n",
       "3  -0.061716   0.107710   0.075412  -0.345782  -0.084918   0.755215  -0.176971  \n",
       "4  -0.063014   0.165477   0.005216   0.128150  -0.054777  -0.678608  -0.116994  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_file = 'robot_info_dataset-jumped.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data collected 1384848 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65342, 23)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set contains some null values so they should be deleted from the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data will be normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>theta</th>\n",
       "      <th>dx</th>\n",
       "      <th>dy</th>\n",
       "      <th>dtheta</th>\n",
       "      <th>sensor_1</th>\n",
       "      <th>sensor_2</th>\n",
       "      <th>sensor_3</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor_7</th>\n",
       "      <th>sensor_8</th>\n",
       "      <th>dsensor_1</th>\n",
       "      <th>dsensor_2</th>\n",
       "      <th>dsensor_3</th>\n",
       "      <th>dsensor_4</th>\n",
       "      <th>dsensor_5</th>\n",
       "      <th>dsensor_6</th>\n",
       "      <th>dsensor_7</th>\n",
       "      <th>dsensor_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "      <td>65341.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.498321</td>\n",
       "      <td>0.504753</td>\n",
       "      <td>0.502063</td>\n",
       "      <td>0.499785</td>\n",
       "      <td>0.500412</td>\n",
       "      <td>0.501624</td>\n",
       "      <td>0.239976</td>\n",
       "      <td>0.236145</td>\n",
       "      <td>0.261438</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251293</td>\n",
       "      <td>0.242889</td>\n",
       "      <td>0.449485</td>\n",
       "      <td>0.468761</td>\n",
       "      <td>0.513828</td>\n",
       "      <td>0.507022</td>\n",
       "      <td>0.519272</td>\n",
       "      <td>0.531825</td>\n",
       "      <td>0.446832</td>\n",
       "      <td>0.426383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.288682</td>\n",
       "      <td>0.272549</td>\n",
       "      <td>0.264025</td>\n",
       "      <td>0.290735</td>\n",
       "      <td>0.353425</td>\n",
       "      <td>0.335002</td>\n",
       "      <td>0.114192</td>\n",
       "      <td>0.140647</td>\n",
       "      <td>0.149030</td>\n",
       "      <td>0.169722</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160999</td>\n",
       "      <td>0.143636</td>\n",
       "      <td>0.078247</td>\n",
       "      <td>0.073403</td>\n",
       "      <td>0.077416</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.081846</td>\n",
       "      <td>0.080184</td>\n",
       "      <td>0.072415</td>\n",
       "      <td>0.077850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.255777</td>\n",
       "      <td>0.269104</td>\n",
       "      <td>0.251332</td>\n",
       "      <td>0.139102</td>\n",
       "      <td>0.181098</td>\n",
       "      <td>0.496242</td>\n",
       "      <td>0.127116</td>\n",
       "      <td>0.108774</td>\n",
       "      <td>0.119219</td>\n",
       "      <td>...</td>\n",
       "      <td>0.112714</td>\n",
       "      <td>0.127502</td>\n",
       "      <td>0.412053</td>\n",
       "      <td>0.436092</td>\n",
       "      <td>0.483323</td>\n",
       "      <td>0.477268</td>\n",
       "      <td>0.488782</td>\n",
       "      <td>0.501383</td>\n",
       "      <td>0.415117</td>\n",
       "      <td>0.388997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.498537</td>\n",
       "      <td>0.503544</td>\n",
       "      <td>0.498631</td>\n",
       "      <td>0.500020</td>\n",
       "      <td>0.500797</td>\n",
       "      <td>0.501627</td>\n",
       "      <td>0.216759</td>\n",
       "      <td>0.215659</td>\n",
       "      <td>0.237666</td>\n",
       "      <td>...</td>\n",
       "      <td>0.224880</td>\n",
       "      <td>0.220298</td>\n",
       "      <td>0.442927</td>\n",
       "      <td>0.467409</td>\n",
       "      <td>0.516314</td>\n",
       "      <td>0.512401</td>\n",
       "      <td>0.524693</td>\n",
       "      <td>0.534465</td>\n",
       "      <td>0.445713</td>\n",
       "      <td>0.419784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.735371</td>\n",
       "      <td>0.740156</td>\n",
       "      <td>0.752400</td>\n",
       "      <td>0.860369</td>\n",
       "      <td>0.821281</td>\n",
       "      <td>0.506991</td>\n",
       "      <td>0.328127</td>\n",
       "      <td>0.337889</td>\n",
       "      <td>0.375692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360337</td>\n",
       "      <td>0.333973</td>\n",
       "      <td>0.479438</td>\n",
       "      <td>0.498031</td>\n",
       "      <td>0.547039</td>\n",
       "      <td>0.544322</td>\n",
       "      <td>0.557577</td>\n",
       "      <td>0.565527</td>\n",
       "      <td>0.475271</td>\n",
       "      <td>0.456114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0             x             y         theta            dx  \\\n",
       "count  65341.000000  65341.000000  65341.000000  65341.000000  65341.000000   \n",
       "mean       0.500000      0.498321      0.504753      0.502063      0.499785   \n",
       "std        0.288682      0.272549      0.264025      0.290735      0.353425   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.250000      0.255777      0.269104      0.251332      0.139102   \n",
       "50%        0.500000      0.498537      0.503544      0.498631      0.500020   \n",
       "75%        0.750000      0.735371      0.740156      0.752400      0.860369   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                 dy        dtheta      sensor_1      sensor_2      sensor_3  \\\n",
       "count  65341.000000  65341.000000  65341.000000  65341.000000  65341.000000   \n",
       "mean       0.500412      0.501624      0.239976      0.236145      0.261438   \n",
       "std        0.335002      0.114192      0.140647      0.149030      0.169722   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.181098      0.496242      0.127116      0.108774      0.119219   \n",
       "50%        0.500797      0.501627      0.216759      0.215659      0.237666   \n",
       "75%        0.821281      0.506991      0.328127      0.337889      0.375692   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "           ...           sensor_7      sensor_8     dsensor_1     dsensor_2  \\\n",
       "count      ...       65341.000000  65341.000000  65341.000000  65341.000000   \n",
       "mean       ...           0.251293      0.242889      0.449485      0.468761   \n",
       "std        ...           0.160999      0.143636      0.078247      0.073403   \n",
       "min        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "25%        ...           0.112714      0.127502      0.412053      0.436092   \n",
       "50%        ...           0.224880      0.220298      0.442927      0.467409   \n",
       "75%        ...           0.360337      0.333973      0.479438      0.498031   \n",
       "max        ...           1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "          dsensor_3     dsensor_4     dsensor_5     dsensor_6     dsensor_7  \\\n",
       "count  65341.000000  65341.000000  65341.000000  65341.000000  65341.000000   \n",
       "mean       0.513828      0.507022      0.519272      0.531825      0.446832   \n",
       "std        0.077416      0.078125      0.081846      0.080184      0.072415   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.483323      0.477268      0.488782      0.501383      0.415117   \n",
       "50%        0.516314      0.512401      0.524693      0.534465      0.445713   \n",
       "75%        0.547039      0.544322      0.557577      0.565527      0.475271   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "          dsensor_8  \n",
       "count  65341.000000  \n",
       "mean       0.426383  \n",
       "std        0.077850  \n",
       "min        0.000000  \n",
       "25%        0.388997  \n",
       "50%        0.419784  \n",
       "75%        0.456114  \n",
       "max        1.000000  \n",
       "\n",
       "[8 rows x 23 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_df=(df-df.min())/(df.max()-df.min())\n",
    "normalized_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input and output variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data will be split into training, testing and validation sets. 60% of the data will be used for training, 20% for training and 20% of validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train size\n",
    "test_size_percentage = .2\n",
    "train_size_percentage = .8\n",
    "ds_size = normalized_df.shape[0]\n",
    "train_size = int(train_size_percentage * ds_size)\n",
    "test_size = int(test_size_percentage * ds_size)\n",
    "\n",
    "# shuffle dataset\n",
    "normalized_df = normalized_df.sample(frac=1)\n",
    "\n",
    "# separate inputs from outputs\n",
    "inputs = normalized_df[['sensor_1', 'sensor_2', 'sensor_3', 'sensor_4', 'sensor_5', 'sensor_6', 'sensor_7', 'sensor_8']]\n",
    "targets = normalized_df[['x', 'y', 'theta']]\n",
    "\n",
    "# train\n",
    "train_inputs = inputs[:train_size]\n",
    "train_targets = targets[:train_size]\n",
    "\n",
    "# test\n",
    "test_inputs = inputs[train_size:(train_size + test_size)]\n",
    "test_targets = targets[train_size:(train_size + test_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As input the neural network receives the sensor measurements as input and the target are the x, y, $\\theta$ coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model():\n",
    "    # neural network with a 10-neuron hidden layer\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(10, activation='relu', input_shape=(8,)))\n",
    "    model.add(layers.Dense(3))\n",
    "    \n",
    "#     rmsprop = optimizers.RMSprop(learning_rate=0.01)\n",
    "    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "              \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold(num_epochs=10, k=5):\n",
    "    num_val_samples = len(train_inputs) // k\n",
    "    validation_scores = []\n",
    "    histories = []\n",
    "    nmse = []\n",
    "\n",
    "    for i in range(k):\n",
    "        print('processing fold #', i)\n",
    "        val_data = train_inputs[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "        val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "        partial_train_data = np.concatenate(\n",
    "            [train_inputs[:i * num_val_samples],\n",
    "             train_inputs[(i + 1) * num_val_samples:]], axis=0)\n",
    "        partial_train_targets = np.concatenate(\n",
    "            [train_targets[:i * num_val_samples],\n",
    "             train_targets[(i + 1) * num_val_samples:]], axis=0)\n",
    "\n",
    "\n",
    "        model = get_model()\n",
    "\n",
    "        history = model.fit(partial_train_data, partial_train_targets,\n",
    "                            validation_data=(val_data, val_targets),\n",
    "                            epochs=num_epochs, batch_size=1, verbose=1)\n",
    "        histories.append(history.history)\n",
    "\n",
    "        predictions_targets = model.predict(val_data)\n",
    "        nmse.append(np.mean((predictions_targets - val_targets)**2)/np.var(val_targets))\n",
    "        \n",
    "    return histories, nmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold # 0\n",
      "Train on 41818 samples, validate on 10454 samples\n",
      "Epoch 1/150\n",
      "41818/41818 [==============================] - 50s 1ms/step - loss: 0.0778 - mae: 0.2421 - val_loss: 0.0751 - val_mae: 0.2399\n",
      "Epoch 2/150\n",
      "41818/41818 [==============================] - 49s 1ms/step - loss: 0.0749 - mae: 0.2401 - val_loss: 0.0745 - val_mae: 0.2398074\n",
      "Epoch 3/150\n",
      "41818/41818 [==============================] - 56s 1ms/step - loss: 0.0746 - mae: 0.2401 - val_loss: 0.0740 - val_mae: 0.2395\n",
      "Epoch 4/150\n",
      "41818/41818 [==============================] - 43s 1ms/step - loss: 0.0744 - mae: 0.2400 - val_loss: 0.0745 - val_mae: 0.2401\n",
      "Epoch 5/150\n",
      "41818/41818 [==============================] - 40s 968us/step - loss: 0.0743 - mae: 0.2401 - val_loss: 0.0739 - val_mae: 0.2398\n",
      "Epoch 6/150\n",
      "41818/41818 [==============================] - 40s 945us/step - loss: 0.0742 - mae: 0.2399 - val_loss: 0.0738 - val_mae: 0.2397\n",
      "Epoch 7/150\n",
      "41818/41818 [==============================] - 39s 939us/step - loss: 0.0742 - mae: 0.2400 - val_loss: 0.0736 - val_mae: 0.2396\n",
      "Epoch 8/150\n",
      "41818/41818 [==============================] - 40s 961us/step - loss: 0.0741 - mae: 0.2400 - val_loss: 0.0743 - val_mae: 0.2401\n",
      "Epoch 9/150\n",
      "41818/41818 [==============================] - 40s 965us/step - loss: 0.0742 - mae: 0.2400 - val_loss: 0.0743 - val_mae: 0.2403\n",
      "Epoch 10/150\n",
      "41818/41818 [==============================] - 40s 951us/step - loss: 0.0742 - mae: 0.2400 - val_loss: 0.0739 - val_mae: 0.2397\n",
      "Epoch 11/150\n",
      "41818/41818 [==============================] - 40s 956us/step - loss: 0.0742 - mae: 0.2399 - val_loss: 0.0738 - val_mae: 0.2396\n",
      "Epoch 12/150\n",
      "41818/41818 [==============================] - 40s 959us/step - loss: 0.0742 - mae: 0.2399 - val_loss: 0.0740 - val_mae: 0.2399\n",
      "Epoch 13/150\n",
      "41818/41818 [==============================] - 46s 1ms/step - loss: 0.0742 - mae: 0.2399 - val_loss: 0.0739 - val_mae: 0.2396\n",
      "Epoch 14/150\n",
      "41818/41818 [==============================] - 48s 1ms/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0741 - val_mae: 0.2399: 0s - loss: 0.\n",
      "Epoch 15/150\n",
      "41818/41818 [==============================] - 42s 999us/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0739 - val_mae: 0.2399\n",
      "Epoch 16/150\n",
      "41818/41818 [==============================] - 42s 1ms/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0748 - val_mae: 0.2405\n",
      "Epoch 17/150\n",
      "41818/41818 [==============================] - 42s 1ms/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0743 - val_mae: 0.2400\n",
      "Epoch 18/150\n",
      "41818/41818 [==============================] - 43s 1ms/step - loss: 0.0741 - mae: 0.2400 - val_loss: 0.0742 - val_mae: 0.2399\n",
      "Epoch 19/150\n",
      "41818/41818 [==============================] - 42s 998us/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0738 - val_mae: 0.2399\n",
      "Epoch 20/150\n",
      "41818/41818 [==============================] - 42s 1000us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0741 - val_mae: 0.2402\n",
      "Epoch 21/150\n",
      "41818/41818 [==============================] - 42s 1ms/step - loss: 0.0740 - mae: 0.2397 - val_loss: 0.0740 - val_mae: 0.2398\n",
      "Epoch 22/150\n",
      "41818/41818 [==============================] - 42s 1ms/step - loss: 0.0740 - mae: 0.2399 - val_loss: 0.0741 - val_mae: 0.2399\n",
      "Epoch 23/150\n",
      "41818/41818 [==============================] - 42s 1ms/step - loss: 0.0740 - mae: 0.2399 - val_loss: 0.0738 - val_mae: 0.2398\n",
      "Epoch 24/150\n",
      "41818/41818 [==============================] - 43s 1ms/step - loss: 0.0741 - mae: 0.2400 - val_loss: 0.0742 - val_mae: 0.2402\n",
      "Epoch 25/150\n",
      "41818/41818 [==============================] - 42s 1ms/step - loss: 0.0741 - mae: 0.2400 - val_loss: 0.0737 - val_mae: 0.2397\n",
      "Epoch 26/150\n",
      "41818/41818 [==============================] - 42s 1ms/step - loss: 0.0740 - mae: 0.2399 - val_loss: 0.0740 - val_mae: 0.2400\n",
      "Epoch 27/150\n",
      "41818/41818 [==============================] - 44s 1ms/step - loss: 0.0741 - mae: 0.2400 - val_loss: 0.0746 - val_mae: 0.2398\n",
      "Epoch 28/150\n",
      "41818/41818 [==============================] - 42s 994us/step - loss: 0.0740 - mae: 0.2399 - val_loss: 0.0738 - val_mae: 0.2399\n",
      "Epoch 29/150\n",
      "41818/41818 [==============================] - 42s 994us/step - loss: 0.0740 - mae: 0.2399 - val_loss: 0.0743 - val_mae: 0.2400\n",
      "Epoch 30/150\n",
      "41818/41818 [==============================] - 42s 1ms/step - loss: 0.0741 - mae: 0.2400 - val_loss: 0.0740 - val_mae: 0.2398\n",
      "Epoch 31/150\n",
      "41818/41818 [==============================] - 43s 1ms/step - loss: 0.0740 - mae: 0.2398 - val_loss: 0.0737 - val_mae: 0.2398\n",
      "Epoch 32/150\n",
      "41818/41818 [==============================] - 42s 1ms/step - loss: 0.0740 - mae: 0.2400 - val_loss: 0.0736 - val_mae: 0.2397\n",
      "Epoch 33/150\n",
      "41818/41818 [==============================] - 40s 954us/step - loss: 0.0740 - mae: 0.2400 - val_loss: 0.0742 - val_mae: 0.2403\n",
      "Epoch 34/150\n",
      "41818/41818 [==============================] - 40s 955us/step - loss: 0.0740 - mae: 0.2398 - val_loss: 0.0741 - val_mae: 0.2400\n",
      "Epoch 35/150\n",
      "41818/41818 [==============================] - 41s 978us/step - loss: 0.0740 - mae: 0.2399 - val_loss: 0.0738 - val_mae: 0.2398\n",
      "Epoch 36/150\n",
      "41818/41818 [==============================] - 41s 984us/step - loss: 0.0741 - mae: 0.2400 - val_loss: 0.0737 - val_mae: 0.2397\n",
      "Epoch 37/150\n",
      "41818/41818 [==============================] - 42s 1ms/step - loss: 0.0740 - mae: 0.2399 - val_loss: 0.0743 - val_mae: 0.2402\n",
      "Epoch 38/150\n",
      "41818/41818 [==============================] - 44s 1ms/step - loss: 0.0740 - mae: 0.2399 - val_loss: 0.0739 - val_mae: 0.2399\n",
      "Epoch 39/150\n",
      "41818/41818 [==============================] - 45s 1ms/step - loss: 0.0740 - mae: 0.2399 - val_loss: 0.0738 - val_mae: 0.2399\n",
      "Epoch 40/150\n",
      "41818/41818 [==============================] - 46s 1ms/step - loss: 0.0741 - mae: 0.2400 - val_loss: 0.0737 - val_mae: 0.2397\n",
      "Epoch 41/150\n",
      "41818/41818 [==============================] - 41s 977us/step - loss: 0.0741 - mae: 0.2400 - val_loss: 0.0742 - val_mae: 0.2399\n",
      "Epoch 42/150\n",
      "41818/41818 [==============================] - 44s 1ms/step - loss: 0.0741 - mae: 0.2400 - val_loss: 0.0740 - val_mae: 0.2400\n",
      "Epoch 43/150\n",
      "41818/41818 [==============================] - 43s 1ms/step - loss: 0.0741 - mae: 0.2400 - val_loss: 0.0739 - val_mae: 0.2399\n",
      "Epoch 44/150\n",
      "41818/41818 [==============================] - 44s 1ms/step - loss: 0.0740 - mae: 0.2399 - val_loss: 0.0739 - val_mae: 0.2398\n",
      "Epoch 45/150\n",
      "41818/41818 [==============================] - 42s 1ms/step - loss: 0.0741 - mae: 0.2400 - val_loss: 0.0737 - val_mae: 0.2397\n",
      "Epoch 46/150\n",
      "41818/41818 [==============================] - 41s 985us/step - loss: 0.0740 - mae: 0.2399 - val_loss: 0.0738 - val_mae: 0.2398\n",
      "Epoch 47/150\n",
      "41818/41818 [==============================] - 39s 937us/step - loss: 0.0741 - mae: 0.2401 - val_loss: 0.0737 - val_mae: 0.2397\n",
      "Epoch 48/150\n",
      "41818/41818 [==============================] - 40s 950us/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0745 - val_mae: 0.2402\n",
      "Epoch 49/150\n",
      "41818/41818 [==============================] - 40s 958us/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0741 - val_mae: 0.2401\n",
      "Epoch 50/150\n",
      "41818/41818 [==============================] - 40s 950us/step - loss: 0.0740 - mae: 0.2399 - val_loss: 0.0742 - val_mae: 0.2401\n",
      "Epoch 51/150\n",
      "41818/41818 [==============================] - 40s 955us/step - loss: 0.0740 - mae: 0.2400 - val_loss: 0.0742 - val_mae: 0.2402\n",
      "Epoch 52/150\n",
      "41818/41818 [==============================] - 43s 1ms/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0743 - val_mae: 0.2396\n",
      "Epoch 53/150\n",
      "41818/41818 [==============================] - 44s 1ms/step - loss: 0.0740 - mae: 0.2398 - val_loss: 0.0742 - val_mae: 0.2398\n",
      "Epoch 54/150\n",
      "41818/41818 [==============================] - 46s 1ms/step - loss: 0.0740 - mae: 0.2398 - val_loss: 0.0740 - val_mae: 0.2401\n",
      "Epoch 55/150\n",
      "41818/41818 [==============================] - 46s 1ms/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0738 - val_mae: 0.2396\n",
      "Epoch 56/150\n",
      "41818/41818 [==============================] - 41s 980us/step - loss: 0.0741 - mae: 0.2400 - val_loss: 0.0744 - val_mae: 0.2404\n",
      "Epoch 57/150\n",
      "41818/41818 [==============================] - 42s 999us/step - loss: 0.0741 - mae: 0.2400 - val_loss: 0.0739 - val_mae: 0.2399\n",
      "Epoch 58/150\n",
      "41818/41818 [==============================] - 45s 1ms/step - loss: 0.0741 - mae: 0.2400 - val_loss: 0.0739 - val_mae: 0.2399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/150\n",
      "41818/41818 [==============================] - 40s 958us/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0741 - val_mae: 0.2398\n",
      "Epoch 60/150\n",
      "41818/41818 [==============================] - 41s 974us/step - loss: 0.0741 - mae: 0.2400 - val_loss: 0.0740 - val_mae: 0.2401 loss: 0.0\n",
      "Epoch 61/150\n",
      "41818/41818 [==============================] - 43s 1ms/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0743 - val_mae: 0.2402\n",
      "Epoch 62/150\n",
      "41818/41818 [==============================] - 43s 1ms/step - loss: 0.0740 - mae: 0.2399 - val_loss: 0.0748 - val_mae: 0.2407\n",
      "Epoch 63/150\n",
      "41818/41818 [==============================] - 42s 1ms/step - loss: 0.0741 - mae: 0.2400 - val_loss: 0.0739 - val_mae: 0.2399\n",
      "Epoch 64/150\n",
      "41818/41818 [==============================] - 42s 1ms/step - loss: 0.0741 - mae: 0.2400 - val_loss: 0.0739 - val_mae: 0.2400\n",
      "Epoch 65/150\n",
      "41818/41818 [==============================] - 40s 961us/step - loss: 0.0741 - mae: 0.2400 - val_loss: 0.0741 - val_mae: 0.2403\n",
      "Epoch 66/150\n",
      "41818/41818 [==============================] - 42s 1ms/step - loss: 0.0741 - mae: 0.2400 - val_loss: 0.0746 - val_mae: 0.2406\n",
      "Epoch 67/150\n",
      "41818/41818 [==============================] - 41s 989us/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0739 - val_mae: 0.2400\n",
      "Epoch 68/150\n",
      "41818/41818 [==============================] - 40s 967us/step - loss: 0.0741 - mae: 0.2400 - val_loss: 0.0743 - val_mae: 0.2403\n",
      "Epoch 69/150\n",
      "41818/41818 [==============================] - 42s 1ms/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0740 - val_mae: 0.2400\n",
      "Epoch 70/150\n",
      "41818/41818 [==============================] - 43s 1ms/step - loss: 0.0741 - mae: 0.2401 - val_loss: 0.0739 - val_mae: 0.2400\n",
      "Epoch 71/150\n",
      "41818/41818 [==============================] - 42s 998us/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0738 - val_mae: 0.2399\n",
      "Epoch 72/150\n",
      "41818/41818 [==============================] - 40s 945us/step - loss: 0.0741 - mae: 0.2400 - val_loss: 0.0753 - val_mae: 0.2409\n",
      "Epoch 73/150\n",
      "41818/41818 [==============================] - 39s 943us/step - loss: 0.0741 - mae: 0.2400 - val_loss: 0.0743 - val_mae: 0.2401\n",
      "Epoch 74/150\n",
      "41818/41818 [==============================] - 40s 949us/step - loss: 0.0741 - mae: 0.2400 - val_loss: 0.0740 - val_mae: 0.2401\n",
      "Epoch 75/150\n",
      "41818/41818 [==============================] - 40s 949us/step - loss: 0.0741 - mae: 0.2400 - val_loss: 0.0742 - val_mae: 0.2403\n",
      "Epoch 76/150\n",
      "41818/41818 [==============================] - 39s 944us/step - loss: 0.0742 - mae: 0.2400 - val_loss: 0.0741 - val_mae: 0.2401\n",
      "Epoch 77/150\n",
      "41818/41818 [==============================] - 40s 952us/step - loss: 0.0742 - mae: 0.2400 - val_loss: 0.0739 - val_mae: 0.2399\n",
      "Epoch 78/150\n",
      "41818/41818 [==============================] - 42s 999us/step - loss: 0.0742 - mae: 0.2400 - val_loss: 0.0741 - val_mae: 0.2399\n",
      "Epoch 79/150\n",
      "41818/41818 [==============================] - 43s 1ms/step - loss: 0.0742 - mae: 0.2400 - val_loss: 0.0743 - val_mae: 0.2403\n",
      "Epoch 80/150\n",
      "41818/41818 [==============================] - 43s 1ms/step - loss: 0.0742 - mae: 0.2400 - val_loss: 0.0740 - val_mae: 0.2399\n",
      "Epoch 81/150\n",
      "41818/41818 [==============================] - 42s 1ms/step - loss: 0.0742 - mae: 0.2399 - val_loss: 0.0740 - val_mae: 0.2401\n",
      "Epoch 82/150\n",
      "41818/41818 [==============================] - 43s 1ms/step - loss: 0.0742 - mae: 0.2401 - val_loss: 0.0746 - val_mae: 0.2401\n",
      "Epoch 83/150\n",
      "41818/41818 [==============================] - 42s 999us/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0743 - val_mae: 0.2403\n",
      "Epoch 84/150\n",
      "41818/41818 [==============================] - 40s 953us/step - loss: 0.0742 - mae: 0.2400 - val_loss: 0.0743 - val_mae: 0.2402\n",
      "Epoch 85/150\n",
      "41818/41818 [==============================] - 40s 948us/step - loss: 0.0742 - mae: 0.2400 - val_loss: 0.0747 - val_mae: 0.2405\n",
      "Epoch 86/150\n",
      "41818/41818 [==============================] - 38s 907us/step - loss: 0.0742 - mae: 0.2399 - val_loss: 0.0758 - val_mae: 0.2413\n",
      "Epoch 87/150\n",
      "41818/41818 [==============================] - 39s 932us/step - loss: 0.0743 - mae: 0.2399 - val_loss: 0.0744 - val_mae: 0.2404\n",
      "Epoch 88/150\n",
      "41818/41818 [==============================] - 39s 938us/step - loss: 0.0742 - mae: 0.2399 - val_loss: 0.0743 - val_mae: 0.2401\n",
      "Epoch 89/150\n",
      "41818/41818 [==============================] - 39s 938us/step - loss: 0.0743 - mae: 0.2400 - val_loss: 0.0741 - val_mae: 0.2400\n",
      "Epoch 90/150\n",
      "41818/41818 [==============================] - 40s 948us/step - loss: 0.0743 - mae: 0.2400 - val_loss: 0.0750 - val_mae: 0.2407\n",
      "Epoch 91/150\n",
      "41818/41818 [==============================] - 39s 942us/step - loss: 0.0742 - mae: 0.2399 - val_loss: 0.0742 - val_mae: 0.2402\n",
      "Epoch 92/150\n",
      "41818/41818 [==============================] - 40s 953us/step - loss: 0.0743 - mae: 0.2400 - val_loss: 0.0747 - val_mae: 0.2406\n",
      "Epoch 93/150\n",
      "41818/41818 [==============================] - 43s 1ms/step - loss: 0.0743 - mae: 0.2400 - val_loss: 0.0743 - val_mae: 0.2402\n",
      "Epoch 94/150\n",
      "41818/41818 [==============================] - 44s 1ms/step - loss: 0.0743 - mae: 0.2399 - val_loss: 0.0749 - val_mae: 0.2406\n",
      "Epoch 95/150\n",
      "41818/41818 [==============================] - 47s 1ms/step - loss: 0.0743 - mae: 0.2399 - val_loss: 0.0748 - val_mae: 0.2404\n",
      "Epoch 96/150\n",
      "41818/41818 [==============================] - 45s 1ms/step - loss: 0.0742 - mae: 0.2398 - val_loss: 0.0742 - val_mae: 0.2401\n",
      "Epoch 97/150\n",
      "41818/41818 [==============================] - 40s 963us/step - loss: 0.0742 - mae: 0.2399 - val_loss: 0.0742 - val_mae: 0.2403\n",
      "Epoch 98/150\n",
      "41818/41818 [==============================] - 41s 989us/step - loss: 0.0743 - mae: 0.2399 - val_loss: 0.0747 - val_mae: 0.2408\n",
      "Epoch 99/150\n",
      "41818/41818 [==============================] - 40s 961us/step - loss: 0.0743 - mae: 0.2399 - val_loss: 0.0744 - val_mae: 0.2403\n",
      "Epoch 100/150\n",
      "41818/41818 [==============================] - 40s 950us/step - loss: 0.0742 - mae: 0.2399 - val_loss: 0.0748 - val_mae: 0.2406\n",
      "Epoch 101/150\n",
      "41818/41818 [==============================] - 40s 964us/step - loss: 0.0742 - mae: 0.2399 - val_loss: 0.0745 - val_mae: 0.2403\n",
      "Epoch 102/150\n",
      "41818/41818 [==============================] - 46s 1ms/step - loss: 0.0742 - mae: 0.2399 - val_loss: 0.0742 - val_mae: 0.2401\n",
      "Epoch 103/150\n",
      "41818/41818 [==============================] - 46s 1ms/step - loss: 0.0742 - mae: 0.2400 - val_loss: 0.0744 - val_mae: 0.2403\n",
      "Epoch 104/150\n",
      "41818/41818 [==============================] - 46s 1ms/step - loss: 0.0743 - mae: 0.2399 - val_loss: 0.0755 - val_mae: 0.2416\n",
      "Epoch 105/150\n",
      "41818/41818 [==============================] - 45s 1ms/step - loss: 0.0743 - mae: 0.2399 - val_loss: 0.0746 - val_mae: 0.2406\n",
      "Epoch 106/150\n",
      "41818/41818 [==============================] - 47s 1ms/step - loss: 0.0742 - mae: 0.2400 - val_loss: 0.0740 - val_mae: 0.2401\n",
      "Epoch 107/150\n",
      "41818/41818 [==============================] - 46s 1ms/step - loss: 0.0742 - mae: 0.2398 - val_loss: 0.0741 - val_mae: 0.2401\n",
      "Epoch 108/150\n",
      "41818/41818 [==============================] - 47s 1ms/step - loss: 0.0743 - mae: 0.2400 - val_loss: 0.0742 - val_mae: 0.2402\n",
      "Epoch 109/150\n",
      "41818/41818 [==============================] - 44s 1ms/step - loss: 0.0743 - mae: 0.2400 - val_loss: 0.0743 - val_mae: 0.2402\n",
      "Epoch 110/150\n",
      "41818/41818 [==============================] - 40s 954us/step - loss: 0.0743 - mae: 0.2399 - val_loss: 0.0743 - val_mae: 0.2402\n",
      "Epoch 111/150\n",
      "41818/41818 [==============================] - 39s 942us/step - loss: 0.0743 - mae: 0.2399 - val_loss: 0.0741 - val_mae: 0.2401\n",
      "Epoch 112/150\n",
      "41818/41818 [==============================] - 39s 935us/step - loss: 0.0743 - mae: 0.2399 - val_loss: 0.0742 - val_mae: 0.2401\n",
      "Epoch 113/150\n",
      "41818/41818 [==============================] - 2291s 55ms/step - loss: 0.0743 - mae: 0.2400 - val_loss: 0.0746 - val_mae: 0.2400\n",
      "Epoch 114/150\n",
      "41818/41818 [==============================] - 42s 1ms/step - loss: 0.0743 - mae: 0.2400 - val_loss: 0.0744 - val_mae: 0.2402\n",
      "Epoch 115/150\n",
      "41818/41818 [==============================] - 43s 1ms/step - loss: 0.0744 - mae: 0.2401 - val_loss: 0.0742 - val_mae: 0.2402\n",
      "Epoch 116/150\n",
      "41818/41818 [==============================] - 43s 1ms/step - loss: 0.0743 - mae: 0.2399 - val_loss: 0.0740 - val_mae: 0.2401\n",
      "Epoch 117/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41818/41818 [==============================] - 39s 928us/step - loss: 0.0743 - mae: 0.2400 - val_loss: 0.0745 - val_mae: 0.2404\n",
      "Epoch 118/150\n",
      "41818/41818 [==============================] - 39s 930us/step - loss: 0.0743 - mae: 0.2400 - val_loss: 0.0746 - val_mae: 0.2405\n",
      "Epoch 119/150\n",
      "41818/41818 [==============================] - 39s 933us/step - loss: 0.0743 - mae: 0.2400 - val_loss: 0.0748 - val_mae: 0.2405\n",
      "Epoch 120/150\n",
      "41818/41818 [==============================] - 38s 920us/step - loss: 0.0743 - mae: 0.2399 - val_loss: 0.0750 - val_mae: 0.2407\n",
      "Epoch 121/150\n",
      "41818/41818 [==============================] - 39s 938us/step - loss: 0.0743 - mae: 0.2401 - val_loss: 0.0743 - val_mae: 0.2401\n",
      "Epoch 122/150\n",
      "41818/41818 [==============================] - 39s 929us/step - loss: 0.0743 - mae: 0.2399 - val_loss: 0.0751 - val_mae: 0.2409\n",
      "Epoch 123/150\n",
      "41818/41818 [==============================] - 39s 921us/step - loss: 0.0743 - mae: 0.2400 - val_loss: 0.0753 - val_mae: 0.2409\n",
      "Epoch 124/150\n",
      "41818/41818 [==============================] - 40s 954us/step - loss: 0.0744 - mae: 0.2399 - val_loss: 0.0749 - val_mae: 0.2408\n",
      "Epoch 125/150\n",
      "41818/41818 [==============================] - 39s 937us/step - loss: 0.0743 - mae: 0.2400 - val_loss: 0.0742 - val_mae: 0.2402\n",
      "Epoch 126/150\n",
      "41818/41818 [==============================] - 39s 934us/step - loss: 0.0744 - mae: 0.2401 - val_loss: 0.0744 - val_mae: 0.2403\n",
      "Epoch 127/150\n",
      "41818/41818 [==============================] - 39s 936us/step - loss: 0.0744 - mae: 0.2400 - val_loss: 0.0743 - val_mae: 0.2400\n",
      "Epoch 128/150\n",
      "41818/41818 [==============================] - 39s 927us/step - loss: 0.0744 - mae: 0.2399 - val_loss: 0.0744 - val_mae: 0.2402\n",
      "Epoch 129/150\n",
      "41818/41818 [==============================] - 39s 935us/step - loss: 0.0744 - mae: 0.2400 - val_loss: 0.0747 - val_mae: 0.2401\n",
      "Epoch 130/150\n",
      "41818/41818 [==============================] - 39s 943us/step - loss: 0.0744 - mae: 0.2399 - val_loss: 0.0742 - val_mae: 0.2400\n",
      "Epoch 131/150\n",
      "41818/41818 [==============================] - 39s 937us/step - loss: 0.0744 - mae: 0.2400 - val_loss: 0.0749 - val_mae: 0.2409\n",
      "Epoch 132/150\n",
      "41818/41818 [==============================] - 39s 933us/step - loss: 0.0744 - mae: 0.2400 - val_loss: 0.0744 - val_mae: 0.2405\n",
      "Epoch 133/150\n",
      "41818/41818 [==============================] - 39s 929us/step - loss: 0.0743 - mae: 0.2399 - val_loss: 0.0742 - val_mae: 0.2403\n",
      "Epoch 134/150\n",
      "41818/41818 [==============================] - 39s 933us/step - loss: 0.0743 - mae: 0.2400 - val_loss: 0.0746 - val_mae: 0.2404\n",
      "Epoch 135/150\n",
      "41818/41818 [==============================] - 39s 934us/step - loss: 0.0743 - mae: 0.2400 - val_loss: 0.0742 - val_mae: 0.2402\n",
      "Epoch 136/150\n",
      "41818/41818 [==============================] - 39s 934us/step - loss: 0.0744 - mae: 0.2399 - val_loss: 0.0744 - val_mae: 0.2402\n",
      "Epoch 137/150\n",
      "41818/41818 [==============================] - 39s 933us/step - loss: 0.0744 - mae: 0.2399 - val_loss: 0.0747 - val_mae: 0.2406\n",
      "Epoch 138/150\n",
      "41818/41818 [==============================] - 39s 935us/step - loss: 0.0744 - mae: 0.2401 - val_loss: 0.0743 - val_mae: 0.2402\n",
      "Epoch 139/150\n",
      "41818/41818 [==============================] - 39s 936us/step - loss: 0.0743 - mae: 0.2400 - val_loss: 0.0745 - val_mae: 0.2402\n",
      "Epoch 140/150\n",
      "41818/41818 [==============================] - 39s 924us/step - loss: 0.0744 - mae: 0.2400 - val_loss: 0.0745 - val_mae: 0.2404\n",
      "Epoch 141/150\n",
      "41818/41818 [==============================] - 39s 931us/step - loss: 0.0744 - mae: 0.2402 - val_loss: 0.0746 - val_mae: 0.2402\n",
      "Epoch 142/150\n",
      "41818/41818 [==============================] - 39s 940us/step - loss: 0.0744 - mae: 0.2400 - val_loss: 0.0750 - val_mae: 0.2406\n",
      "Epoch 143/150\n",
      "41818/41818 [==============================] - 39s 933us/step - loss: 0.0744 - mae: 0.2399 - val_loss: 0.0746 - val_mae: 0.2402\n",
      "Epoch 144/150\n",
      "41818/41818 [==============================] - 39s 927us/step - loss: 0.0743 - mae: 0.2399 - val_loss: 0.0749 - val_mae: 0.2401\n",
      "Epoch 145/150\n",
      "41818/41818 [==============================] - 38s 917us/step - loss: 0.0744 - mae: 0.2400 - val_loss: 0.0743 - val_mae: 0.2403\n",
      "Epoch 146/150\n",
      "41818/41818 [==============================] - 38s 916us/step - loss: 0.0744 - mae: 0.2398 - val_loss: 0.0749 - val_mae: 0.2405\n",
      "Epoch 147/150\n",
      "41818/41818 [==============================] - 38s 918us/step - loss: 0.0744 - mae: 0.2400 - val_loss: 0.0747 - val_mae: 0.2402\n",
      "Epoch 148/150\n",
      "41818/41818 [==============================] - 38s 919us/step - loss: 0.0744 - mae: 0.2399 - val_loss: 0.0749 - val_mae: 0.2406\n",
      "Epoch 149/150\n",
      "41818/41818 [==============================] - 40s 968us/step - loss: 0.0744 - mae: 0.2399 - val_loss: 0.0745 - val_mae: 0.2403\n",
      "Epoch 150/150\n",
      "41818/41818 [==============================] - 40s 945us/step - loss: 0.0744 - mae: 0.2400 - val_loss: 0.0745 - val_mae: 0.2403\n",
      "processing fold # 1\n",
      "Train on 41818 samples, validate on 10454 samples\n",
      "Epoch 1/150\n",
      "41818/41818 [==============================] - 40s 950us/step - loss: 0.0762 - mae: 0.2413 - val_loss: 0.0753 - val_mae: 0.2404\n",
      "Epoch 2/150\n",
      "41818/41818 [==============================] - 39s 936us/step - loss: 0.0753 - mae: 0.2407 - val_loss: 0.0756 - val_mae: 0.2406\n",
      "Epoch 3/150\n",
      "41818/41818 [==============================] - 39s 929us/step - loss: 0.0751 - mae: 0.2406 - val_loss: 0.0751 - val_mae: 0.2400\n",
      "Epoch 4/150\n",
      "41818/41818 [==============================] - 42s 1ms/step - loss: 0.0748 - mae: 0.2403 - val_loss: 0.0742 - val_mae: 0.2398\n",
      "Epoch 5/150\n",
      "41818/41818 [==============================] - 38s 919us/step - loss: 0.0747 - mae: 0.2403 - val_loss: 0.0746 - val_mae: 0.2400\n",
      "Epoch 6/150\n",
      "41818/41818 [==============================] - 39s 922us/step - loss: 0.0745 - mae: 0.2401 - val_loss: 0.0745 - val_mae: 0.2398\n",
      "Epoch 7/150\n",
      "41818/41818 [==============================] - 39s 933us/step - loss: 0.0744 - mae: 0.2403 - val_loss: 0.0743 - val_mae: 0.2396\n",
      "Epoch 8/150\n",
      "41818/41818 [==============================] - 40s 949us/step - loss: 0.0743 - mae: 0.2401 - val_loss: 0.0741 - val_mae: 0.2398\n",
      "Epoch 9/150\n",
      "41818/41818 [==============================] - 40s 953us/step - loss: 0.0742 - mae: 0.2400 - val_loss: 0.0743 - val_mae: 0.2399\n",
      "Epoch 10/150\n",
      "41818/41818 [==============================] - 39s 943us/step - loss: 0.0741 - mae: 0.2401 - val_loss: 0.0738 - val_mae: 0.2395\n",
      "Epoch 11/150\n",
      "41818/41818 [==============================] - 40s 953us/step - loss: 0.0741 - mae: 0.2401 - val_loss: 0.0737 - val_mae: 0.2396\n",
      "Epoch 12/150\n",
      "41818/41818 [==============================] - 40s 946us/step - loss: 0.0741 - mae: 0.2401 - val_loss: 0.0742 - val_mae: 0.2398\n",
      "Epoch 13/150\n",
      "41818/41818 [==============================] - 40s 950us/step - loss: 0.0741 - mae: 0.2401 - val_loss: 0.0739 - val_mae: 0.2395\n",
      "Epoch 14/150\n",
      "41818/41818 [==============================] - 39s 931us/step - loss: 0.0742 - mae: 0.2402 - val_loss: 0.0742 - val_mae: 0.2396\n",
      "Epoch 15/150\n",
      "41818/41818 [==============================] - 39s 928us/step - loss: 0.0742 - mae: 0.2402 - val_loss: 0.0746 - val_mae: 0.2400\n",
      "Epoch 16/150\n",
      "41818/41818 [==============================] - 39s 933us/step - loss: 0.0742 - mae: 0.2402 - val_loss: 0.0741 - val_mae: 0.2398\n",
      "Epoch 17/150\n",
      "41818/41818 [==============================] - 39s 936us/step - loss: 0.0742 - mae: 0.2402 - val_loss: 0.0739 - val_mae: 0.2397\n",
      "Epoch 18/150\n",
      "41818/41818 [==============================] - 39s 926us/step - loss: 0.0742 - mae: 0.2401 - val_loss: 0.0742 - val_mae: 0.2400\n",
      "Epoch 19/150\n",
      "41818/41818 [==============================] - 39s 922us/step - loss: 0.0743 - mae: 0.2402 - val_loss: 0.0744 - val_mae: 0.2396\n",
      "Epoch 20/150\n",
      "41818/41818 [==============================] - 39s 921us/step - loss: 0.0744 - mae: 0.2402 - val_loss: 0.0742 - val_mae: 0.2399\n",
      "Epoch 21/150\n",
      "41818/41818 [==============================] - 39s 923us/step - loss: 0.0743 - mae: 0.2399 - val_loss: 0.0739 - val_mae: 0.2395\n",
      "Epoch 22/150\n",
      "41818/41818 [==============================] - 38s 904us/step - loss: 0.0743 - mae: 0.2402 - val_loss: 0.0740 - val_mae: 0.2399\n",
      "Epoch 23/150\n",
      "41818/41818 [==============================] - 39s 926us/step - loss: 0.0742 - mae: 0.2401 - val_loss: 0.0739 - val_mae: 0.2395\n",
      "Epoch 24/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41818/41818 [==============================] - 38s 902us/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0740 - val_mae: 0.2394\n",
      "Epoch 25/150\n",
      "41818/41818 [==============================] - 38s 916us/step - loss: 0.0740 - mae: 0.2400 - val_loss: 0.0753 - val_mae: 0.2400\n",
      "Epoch 26/150\n",
      "41818/41818 [==============================] - 38s 916us/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0741 - val_mae: 0.2394\n",
      "Epoch 27/150\n",
      "41818/41818 [==============================] - 38s 919us/step - loss: 0.0742 - mae: 0.2400 - val_loss: 0.0741 - val_mae: 0.2395\n",
      "Epoch 28/150\n",
      "41818/41818 [==============================] - 38s 910us/step - loss: 0.0741 - mae: 0.2400 - val_loss: 0.0739 - val_mae: 0.2396\n",
      "Epoch 29/150\n",
      "41818/41818 [==============================] - 38s 915us/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0738 - val_mae: 0.2393\n",
      "Epoch 30/150\n",
      "41818/41818 [==============================] - 38s 918us/step - loss: 0.0741 - mae: 0.2400 - val_loss: 0.0736 - val_mae: 0.2392\n",
      "Epoch 31/150\n",
      "41818/41818 [==============================] - 39s 922us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0743 - val_mae: 0.2394\n",
      "Epoch 32/150\n",
      "41818/41818 [==============================] - 39s 921us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0740 - val_mae: 0.2393\n",
      "Epoch 33/150\n",
      "41818/41818 [==============================] - 39s 929us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0740 - val_mae: 0.2398\n",
      "Epoch 34/150\n",
      "41818/41818 [==============================] - 39s 927us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0743 - val_mae: 0.2401\n",
      "Epoch 35/150\n",
      "41818/41818 [==============================] - 39s 930us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0739 - val_mae: 0.2393\n",
      "Epoch 36/150\n",
      "41818/41818 [==============================] - 41s 990us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0742 - val_mae: 0.2398\n",
      "Epoch 37/150\n",
      "41818/41818 [==============================] - 38s 917us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0742 - val_mae: 0.2393\n",
      "Epoch 38/150\n",
      "41818/41818 [==============================] - 38s 919us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0740 - val_mae: 0.2394\n",
      "Epoch 39/150\n",
      "41818/41818 [==============================] - 38s 919us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0739 - val_mae: 0.2393\n",
      "Epoch 40/150\n",
      "41818/41818 [==============================] - 40s 948us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0738 - val_mae: 0.2392\n",
      "Epoch 41/150\n",
      "41818/41818 [==============================] - 39s 923us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0742 - val_mae: 0.2394\n",
      "Epoch 42/150\n",
      "41818/41818 [==============================] - 39s 921us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0742 - val_mae: 0.2394\n",
      "Epoch 43/150\n",
      "41818/41818 [==============================] - 39s 923us/step - loss: 0.0742 - mae: 0.2397 - val_loss: 0.0742 - val_mae: 0.2396\n",
      "Epoch 44/150\n",
      "41818/41818 [==============================] - 39s 934us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0745 - val_mae: 0.2403\n",
      "Epoch 45/150\n",
      "41818/41818 [==============================] - 39s 926us/step - loss: 0.0740 - mae: 0.2397 - val_loss: 0.0739 - val_mae: 0.2395\n",
      "Epoch 46/150\n",
      "41818/41818 [==============================] - 39s 923us/step - loss: 0.0740 - mae: 0.2397 - val_loss: 0.0743 - val_mae: 0.2397\n",
      "Epoch 47/150\n",
      "41818/41818 [==============================] - 39s 922us/step - loss: 0.0740 - mae: 0.2397 - val_loss: 0.0743 - val_mae: 0.2393\n",
      "Epoch 48/150\n",
      "41818/41818 [==============================] - 39s 932us/step - loss: 0.0740 - mae: 0.2396 - val_loss: 0.0740 - val_mae: 0.2394\n",
      "Epoch 49/150\n",
      "41818/41818 [==============================] - 39s 922us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0743 - val_mae: 0.2397\n",
      "Epoch 50/150\n",
      "41818/41818 [==============================] - 38s 917us/step - loss: 0.0740 - mae: 0.2396 - val_loss: 0.0743 - val_mae: 0.2395\n",
      "Epoch 51/150\n",
      "41818/41818 [==============================] - 38s 919us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0746 - val_mae: 0.2395\n",
      "Epoch 52/150\n",
      "41818/41818 [==============================] - 39s 924us/step - loss: 0.0740 - mae: 0.2397 - val_loss: 0.0735 - val_mae: 0.2393\n",
      "Epoch 53/150\n",
      "41818/41818 [==============================] - 38s 913us/step - loss: 0.0740 - mae: 0.2396 - val_loss: 0.0748 - val_mae: 0.2396\n",
      "Epoch 54/150\n",
      "41818/41818 [==============================] - 39s 923us/step - loss: 0.0740 - mae: 0.2397 - val_loss: 0.0742 - val_mae: 0.2395\n",
      "Epoch 55/150\n",
      "41818/41818 [==============================] - 38s 918us/step - loss: 0.0740 - mae: 0.2397 - val_loss: 0.0744 - val_mae: 0.2396\n",
      "Epoch 56/150\n",
      "41818/41818 [==============================] - 39s 930us/step - loss: 0.0740 - mae: 0.2396 - val_loss: 0.0740 - val_mae: 0.2394\n",
      "Epoch 57/150\n",
      "41818/41818 [==============================] - 39s 925us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0741 - val_mae: 0.2396\n",
      "Epoch 58/150\n",
      "41818/41818 [==============================] - 38s 920us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0740 - val_mae: 0.2393\n",
      "Epoch 59/150\n",
      "41818/41818 [==============================] - 38s 918us/step - loss: 0.0741 - mae: 0.2396 - val_loss: 0.0736 - val_mae: 0.2391\n",
      "Epoch 60/150\n",
      "41818/41818 [==============================] - 39s 922us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0743 - val_mae: 0.2399\n",
      "Epoch 61/150\n",
      "41818/41818 [==============================] - 39s 922us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0738 - val_mae: 0.2395\n",
      "Epoch 62/150\n",
      "41818/41818 [==============================] - 39s 937us/step - loss: 0.0740 - mae: 0.2398 - val_loss: 0.0739 - val_mae: 0.2394\n",
      "Epoch 63/150\n",
      "41818/41818 [==============================] - 39s 934us/step - loss: 0.0740 - mae: 0.2397 - val_loss: 0.0742 - val_mae: 0.2399\n",
      "Epoch 64/150\n",
      "41818/41818 [==============================] - 39s 943us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0740 - val_mae: 0.2394\n",
      "Epoch 65/150\n",
      "41818/41818 [==============================] - 40s 950us/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0737 - val_mae: 0.2392\n",
      "Epoch 66/150\n",
      "41818/41818 [==============================] - 39s 925us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0741 - val_mae: 0.2394\n",
      "Epoch 67/150\n",
      "41818/41818 [==============================] - 39s 922us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0738 - val_mae: 0.2394\n",
      "Epoch 68/150\n",
      "41818/41818 [==============================] - 39s 925us/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0755 - val_mae: 0.2399\n",
      "Epoch 69/150\n",
      "41818/41818 [==============================] - 39s 930us/step - loss: 0.0742 - mae: 0.2400 - val_loss: 0.0740 - val_mae: 0.2394\n",
      "Epoch 70/150\n",
      "41818/41818 [==============================] - 39s 926us/step - loss: 0.0742 - mae: 0.2398 - val_loss: 0.0739 - val_mae: 0.2395\n",
      "Epoch 71/150\n",
      "41818/41818 [==============================] - 39s 925us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0741 - val_mae: 0.2394\n",
      "Epoch 72/150\n",
      "41818/41818 [==============================] - 39s 932us/step - loss: 0.0742 - mae: 0.2397 - val_loss: 0.0744 - val_mae: 0.2396\n",
      "Epoch 73/150\n",
      "41818/41818 [==============================] - 39s 932us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0740 - val_mae: 0.2395\n",
      "Epoch 74/150\n",
      "41818/41818 [==============================] - 39s 933us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0739 - val_mae: 0.2393\n",
      "Epoch 75/150\n",
      "41818/41818 [==============================] - 39s 924us/step - loss: 0.0742 - mae: 0.2397 - val_loss: 0.0742 - val_mae: 0.2395\n",
      "Epoch 76/150\n",
      "41818/41818 [==============================] - 39s 923us/step - loss: 0.0742 - mae: 0.2399 - val_loss: 0.0738 - val_mae: 0.2393\n",
      "Epoch 77/150\n",
      "41818/41818 [==============================] - 39s 925us/step - loss: 0.0742 - mae: 0.2398 - val_loss: 0.0748 - val_mae: 0.2396\n",
      "Epoch 78/150\n",
      "41818/41818 [==============================] - 38s 918us/step - loss: 0.0742 - mae: 0.2399 - val_loss: 0.0744 - val_mae: 0.2396\n",
      "Epoch 79/150\n",
      "41818/41818 [==============================] - 39s 922us/step - loss: 0.0742 - mae: 0.2398 - val_loss: 0.0741 - val_mae: 0.2393\n",
      "Epoch 80/150\n",
      "41818/41818 [==============================] - 38s 920us/step - loss: 0.0742 - mae: 0.2399 - val_loss: 0.0740 - val_mae: 0.2396\n",
      "Epoch 81/150\n",
      "41818/41818 [==============================] - 39s 921us/step - loss: 0.0742 - mae: 0.2397 - val_loss: 0.0747 - val_mae: 0.2397\n",
      "Epoch 82/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41818/41818 [==============================] - 39s 923us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0742 - val_mae: 0.2394\n",
      "Epoch 83/150\n",
      "41818/41818 [==============================] - 38s 916us/step - loss: 0.0742 - mae: 0.2399 - val_loss: 0.0740 - val_mae: 0.2395\n",
      "Epoch 84/150\n",
      "41818/41818 [==============================] - 38s 917us/step - loss: 0.0743 - mae: 0.2398 - val_loss: 0.0740 - val_mae: 0.2394\n",
      "Epoch 85/150\n",
      "41818/41818 [==============================] - 38s 918us/step - loss: 0.0742 - mae: 0.2396 - val_loss: 0.0741 - val_mae: 0.2394\n",
      "Epoch 86/150\n",
      "41818/41818 [==============================] - 38s 913us/step - loss: 0.0742 - mae: 0.2399 - val_loss: 0.0742 - val_mae: 0.2395\n",
      "Epoch 87/150\n",
      "41818/41818 [==============================] - 39s 928us/step - loss: 0.0743 - mae: 0.2397 - val_loss: 0.0742 - val_mae: 0.2397\n",
      "Epoch 88/150\n",
      "41818/41818 [==============================] - 38s 919us/step - loss: 0.0744 - mae: 0.2399 - val_loss: 0.0743 - val_mae: 0.2398\n",
      "Epoch 89/150\n",
      "41818/41818 [==============================] - 39s 921us/step - loss: 0.0743 - mae: 0.2398 - val_loss: 0.0739 - val_mae: 0.2393\n",
      "Epoch 90/150\n",
      "41818/41818 [==============================] - 39s 930us/step - loss: 0.0742 - mae: 0.2397 - val_loss: 0.0743 - val_mae: 0.2395\n",
      "Epoch 91/150\n",
      "41818/41818 [==============================] - 43s 1ms/step - loss: 0.0742 - mae: 0.2397 - val_loss: 0.0748 - val_mae: 0.2401\n",
      "Epoch 92/150\n",
      "41818/41818 [==============================] - 38s 915us/step - loss: 0.0742 - mae: 0.2398 - val_loss: 0.0742 - val_mae: 0.2398\n",
      "Epoch 93/150\n",
      "41818/41818 [==============================] - 38s 918us/step - loss: 0.0743 - mae: 0.2398 - val_loss: 0.0746 - val_mae: 0.2396\n",
      "Epoch 94/150\n",
      "41818/41818 [==============================] - 38s 916us/step - loss: 0.0743 - mae: 0.2397 - val_loss: 0.0749 - val_mae: 0.2402\n",
      "Epoch 95/150\n",
      "41818/41818 [==============================] - 38s 916us/step - loss: 0.0744 - mae: 0.2399 - val_loss: 0.0742 - val_mae: 0.2394\n",
      "Epoch 96/150\n",
      "41818/41818 [==============================] - 38s 919us/step - loss: 0.0743 - mae: 0.2398 - val_loss: 0.0742 - val_mae: 0.2398\n",
      "Epoch 97/150\n",
      "41818/41818 [==============================] - 39s 924us/step - loss: 0.0742 - mae: 0.2398 - val_loss: 0.0744 - val_mae: 0.2400\n",
      "Epoch 98/150\n",
      "41818/41818 [==============================] - 39s 921us/step - loss: 0.0743 - mae: 0.2398 - val_loss: 0.0745 - val_mae: 0.2397\n",
      "Epoch 99/150\n",
      "41818/41818 [==============================] - 39s 925us/step - loss: 0.0742 - mae: 0.2398 - val_loss: 0.0748 - val_mae: 0.2400\n",
      "Epoch 100/150\n",
      "41818/41818 [==============================] - 39s 921us/step - loss: 0.0743 - mae: 0.2397 - val_loss: 0.0745 - val_mae: 0.2399\n",
      "Epoch 101/150\n",
      "41818/41818 [==============================] - 39s 927us/step - loss: 0.0743 - mae: 0.2398 - val_loss: 0.0745 - val_mae: 0.2396\n",
      "Epoch 102/150\n",
      "41818/41818 [==============================] - 38s 921us/step - loss: 0.0743 - mae: 0.2397 - val_loss: 0.0744 - val_mae: 0.2397\n",
      "Epoch 103/150\n",
      "41818/41818 [==============================] - 39s 924us/step - loss: 0.0743 - mae: 0.2399 - val_loss: 0.0741 - val_mae: 0.2395\n",
      "Epoch 104/150\n",
      "41818/41818 [==============================] - 39s 922us/step - loss: 0.0743 - mae: 0.2397 - val_loss: 0.0740 - val_mae: 0.2396\n",
      "Epoch 105/150\n",
      "41818/41818 [==============================] - 38s 920us/step - loss: 0.0744 - mae: 0.2398 - val_loss: 0.0741 - val_mae: 0.2398\n",
      "Epoch 106/150\n",
      "41818/41818 [==============================] - 39s 922us/step - loss: 0.0742 - mae: 0.2397 - val_loss: 0.0744 - val_mae: 0.2399\n",
      "Epoch 107/150\n",
      "41818/41818 [==============================] - 39s 922us/step - loss: 0.0743 - mae: 0.2398 - val_loss: 0.0739 - val_mae: 0.2393\n",
      "Epoch 108/150\n",
      "41818/41818 [==============================] - 38s 919us/step - loss: 0.0743 - mae: 0.2398 - val_loss: 0.0739 - val_mae: 0.2394\n",
      "Epoch 109/150\n",
      "41818/41818 [==============================] - 38s 915us/step - loss: 0.0742 - mae: 0.2397 - val_loss: 0.0739 - val_mae: 0.2394\n",
      "Epoch 110/150\n",
      "41818/41818 [==============================] - 38s 920us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0740 - val_mae: 0.2394\n",
      "Epoch 111/150\n",
      "41818/41818 [==============================] - 38s 915us/step - loss: 0.0741 - mae: 0.2396 - val_loss: 0.0746 - val_mae: 0.2396\n",
      "Epoch 112/150\n",
      "41818/41818 [==============================] - 39s 921us/step - loss: 0.0742 - mae: 0.2398 - val_loss: 0.0745 - val_mae: 0.2398\n",
      "Epoch 113/150\n",
      "41818/41818 [==============================] - 38s 915us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0738 - val_mae: 0.2392\n",
      "Epoch 114/150\n",
      "41818/41818 [==============================] - 38s 917us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0737 - val_mae: 0.2393\n",
      "Epoch 115/150\n",
      "41818/41818 [==============================] - 38s 916us/step - loss: 0.0740 - mae: 0.2397 - val_loss: 0.0740 - val_mae: 0.2394\n",
      "Epoch 116/150\n",
      "41818/41818 [==============================] - 38s 920us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0743 - val_mae: 0.2395\n",
      "Epoch 117/150\n",
      "41818/41818 [==============================] - 39s 938us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0739 - val_mae: 0.2394\n",
      "Epoch 118/150\n",
      "41818/41818 [==============================] - 39s 940us/step - loss: 0.0741 - mae: 0.2396 - val_loss: 0.0743 - val_mae: 0.2397\n",
      "Epoch 119/150\n",
      "41818/41818 [==============================] - 40s 951us/step - loss: 0.0741 - mae: 0.2396 - val_loss: 0.0742 - val_mae: 0.2395\n",
      "Epoch 120/150\n",
      "41818/41818 [==============================] - 39s 930us/step - loss: 0.0742 - mae: 0.2398 - val_loss: 0.0742 - val_mae: 0.2396\n",
      "Epoch 121/150\n",
      "41818/41818 [==============================] - 38s 915us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0749 - val_mae: 0.2398\n",
      "Epoch 122/150\n",
      "41818/41818 [==============================] - 39s 922us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0747 - val_mae: 0.2398\n",
      "Epoch 123/150\n",
      "41818/41818 [==============================] - 38s 919us/step - loss: 0.0740 - mae: 0.2396 - val_loss: 0.0742 - val_mae: 0.2396\n",
      "Epoch 124/150\n",
      "41818/41818 [==============================] - 39s 930us/step - loss: 0.0740 - mae: 0.2398 - val_loss: 0.0736 - val_mae: 0.2393\n",
      "Epoch 125/150\n",
      "41818/41818 [==============================] - 39s 940us/step - loss: 0.0740 - mae: 0.2397 - val_loss: 0.0738 - val_mae: 0.2393\n",
      "Epoch 126/150\n",
      "41818/41818 [==============================] - 40s 961us/step - loss: 0.0740 - mae: 0.2396 - val_loss: 0.0744 - val_mae: 0.2395\n",
      "Epoch 127/150\n",
      "41818/41818 [==============================] - 39s 938us/step - loss: 0.0740 - mae: 0.2396 - val_loss: 0.0739 - val_mae: 0.2392\n",
      "Epoch 128/150\n",
      "41818/41818 [==============================] - 39s 943us/step - loss: 0.0740 - mae: 0.2398 - val_loss: 0.0741 - val_mae: 0.2395\n",
      "Epoch 129/150\n",
      "41818/41818 [==============================] - 40s 950us/step - loss: 0.0740 - mae: 0.2397 - val_loss: 0.0740 - val_mae: 0.2395\n",
      "Epoch 130/150\n",
      "41818/41818 [==============================] - 40s 948us/step - loss: 0.0740 - mae: 0.2396 - val_loss: 0.0742 - val_mae: 0.2397\n",
      "Epoch 131/150\n",
      "41818/41818 [==============================] - 39s 935us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0740 - val_mae: 0.2396\n",
      "Epoch 132/150\n",
      "41818/41818 [==============================] - 39s 931us/step - loss: 0.0740 - mae: 0.2397 - val_loss: 0.0743 - val_mae: 0.2398\n",
      "Epoch 133/150\n",
      "41818/41818 [==============================] - 39s 930us/step - loss: 0.0742 - mae: 0.2397 - val_loss: 0.0740 - val_mae: 0.2396\n",
      "Epoch 134/150\n",
      "41818/41818 [==============================] - 39s 934us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0742 - val_mae: 0.2396\n",
      "Epoch 135/150\n",
      "41818/41818 [==============================] - 39s 928us/step - loss: 0.0741 - mae: 0.2396 - val_loss: 0.0743 - val_mae: 0.2397\n",
      "Epoch 136/150\n",
      "41818/41818 [==============================] - 39s 927us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0742 - val_mae: 0.2396\n",
      "Epoch 137/150\n",
      "41818/41818 [==============================] - 38s 920us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0743 - val_mae: 0.2397\n",
      "Epoch 138/150\n",
      "41818/41818 [==============================] - 39s 931us/step - loss: 0.0741 - mae: 0.2396 - val_loss: 0.0744 - val_mae: 0.2399\n",
      "Epoch 139/150\n",
      "41818/41818 [==============================] - 38s 919us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0739 - val_mae: 0.2395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140/150\n",
      "41818/41818 [==============================] - 38s 913us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0742 - val_mae: 0.2396\n",
      "Epoch 141/150\n",
      "41818/41818 [==============================] - 38s 916us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0742 - val_mae: 0.2395\n",
      "Epoch 142/150\n",
      "41818/41818 [==============================] - 38s 918us/step - loss: 0.0742 - mae: 0.2398 - val_loss: 0.0740 - val_mae: 0.2396\n",
      "Epoch 143/150\n",
      "41818/41818 [==============================] - 38s 911us/step - loss: 0.0740 - mae: 0.2397 - val_loss: 0.0745 - val_mae: 0.2398\n",
      "Epoch 144/150\n",
      "41818/41818 [==============================] - 38s 915us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0739 - val_mae: 0.2394\n",
      "Epoch 145/150\n",
      "41818/41818 [==============================] - 39s 937us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0742 - val_mae: 0.2396\n",
      "Epoch 146/150\n",
      "41818/41818 [==============================] - 39s 925us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0742 - val_mae: 0.2398\n",
      "Epoch 147/150\n",
      "41818/41818 [==============================] - 42s 1ms/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0747 - val_mae: 0.2401\n",
      "Epoch 148/150\n",
      "41818/41818 [==============================] - 38s 917us/step - loss: 0.0742 - mae: 0.2397 - val_loss: 0.0743 - val_mae: 0.2398\n",
      "Epoch 149/150\n",
      "41818/41818 [==============================] - 38s 917us/step - loss: 0.0742 - mae: 0.2397 - val_loss: 0.0737 - val_mae: 0.2394\n",
      "Epoch 150/150\n",
      "41818/41818 [==============================] - 38s 919us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0743 - val_mae: 0.2396\n",
      "processing fold # 2\n",
      "Train on 41818 samples, validate on 10454 samples\n",
      "Epoch 1/150\n",
      "41818/41818 [==============================] - 39s 931us/step - loss: 0.0770 - mae: 0.2414 - val_loss: 0.0767 - val_mae: 0.2411\n",
      "Epoch 2/150\n",
      "41818/41818 [==============================] - 39s 924us/step - loss: 0.0753 - mae: 0.2405 - val_loss: 0.0745 - val_mae: 0.2405\n",
      "Epoch 3/150\n",
      "41818/41818 [==============================] - 39s 927us/step - loss: 0.0749 - mae: 0.2403 - val_loss: 0.0761 - val_mae: 0.2412\n",
      "Epoch 4/150\n",
      "41818/41818 [==============================] - 39s 924us/step - loss: 0.0749 - mae: 0.2404 - val_loss: 0.0742 - val_mae: 0.2402\n",
      "Epoch 5/150\n",
      "41818/41818 [==============================] - 39s 926us/step - loss: 0.0747 - mae: 0.2401 - val_loss: 0.0741 - val_mae: 0.2401\n",
      "Epoch 6/150\n",
      "41818/41818 [==============================] - 39s 932us/step - loss: 0.0745 - mae: 0.2399 - val_loss: 0.0750 - val_mae: 0.2406\n",
      "Epoch 7/150\n",
      "41818/41818 [==============================] - 39s 929us/step - loss: 0.0745 - mae: 0.2401 - val_loss: 0.0746 - val_mae: 0.2404\n",
      "Epoch 8/150\n",
      "41818/41818 [==============================] - 39s 934us/step - loss: 0.0742 - mae: 0.2398 - val_loss: 0.0748 - val_mae: 0.2402\n",
      "Epoch 9/150\n",
      "41818/41818 [==============================] - 39s 932us/step - loss: 0.0742 - mae: 0.2399 - val_loss: 0.0738 - val_mae: 0.2397\n",
      "Epoch 10/150\n",
      "41818/41818 [==============================] - 39s 934us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0755 - val_mae: 0.2407\n",
      "Epoch 11/150\n",
      "41818/41818 [==============================] - 38s 913us/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0742 - val_mae: 0.2400\n",
      "Epoch 12/150\n",
      "41818/41818 [==============================] - 39s 924us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0741 - val_mae: 0.2403\n",
      "Epoch 13/150\n",
      "41818/41818 [==============================] - 39s 935us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0740 - val_mae: 0.2402\n",
      "Epoch 14/150\n",
      "41818/41818 [==============================] - 39s 931us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0743 - val_mae: 0.2401\n",
      "Epoch 15/150\n",
      "41818/41818 [==============================] - 39s 939us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0752 - val_mae: 0.2409\n",
      "Epoch 16/150\n",
      "41818/41818 [==============================] - 39s 934us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0745 - val_mae: 0.2405\n",
      "Epoch 17/150\n",
      "41818/41818 [==============================] - 39s 928us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0758 - val_mae: 0.2404\n",
      "Epoch 18/150\n",
      "41818/41818 [==============================] - 39s 932us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0741 - val_mae: 0.2402\n",
      "Epoch 19/150\n",
      "41818/41818 [==============================] - 39s 924us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0742 - val_mae: 0.2399\n",
      "Epoch 20/150\n",
      "41818/41818 [==============================] - 39s 930us/step - loss: 0.0741 - mae: 0.2396 - val_loss: 0.0744 - val_mae: 0.2402\n",
      "Epoch 21/150\n",
      "41818/41818 [==============================] - 39s 926us/step - loss: 0.0740 - mae: 0.2395 - val_loss: 0.0739 - val_mae: 0.2396\n",
      "Epoch 22/150\n",
      "41818/41818 [==============================] - 39s 927us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0739 - val_mae: 0.2401\n",
      "Epoch 23/150\n",
      "41818/41818 [==============================] - 39s 924us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0737 - val_mae: 0.2398\n",
      "Epoch 24/150\n",
      "41818/41818 [==============================] - 39s 927us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0740 - val_mae: 0.2401\n",
      "Epoch 25/150\n",
      "41818/41818 [==============================] - 39s 931us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0744 - val_mae: 0.2404\n",
      "Epoch 26/150\n",
      "41818/41818 [==============================] - 39s 927us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0739 - val_mae: 0.2397\n",
      "Epoch 27/150\n",
      "41818/41818 [==============================] - 39s 925us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0751 - val_mae: 0.2410\n",
      "Epoch 28/150\n",
      "41818/41818 [==============================] - 39s 927us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0748 - val_mae: 0.2404\n",
      "Epoch 29/150\n",
      "41818/41818 [==============================] - 39s 926us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0739 - val_mae: 0.2399\n",
      "Epoch 30/150\n",
      "41818/41818 [==============================] - 40s 946us/step - loss: 0.0742 - mae: 0.2398 - val_loss: 0.0741 - val_mae: 0.2397\n",
      "Epoch 31/150\n",
      "41818/41818 [==============================] - 39s 945us/step - loss: 0.0742 - mae: 0.2398 - val_loss: 0.0739 - val_mae: 0.2397\n",
      "Epoch 32/150\n",
      "41818/41818 [==============================] - 40s 955us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0742 - val_mae: 0.2398\n",
      "Epoch 33/150\n",
      "41818/41818 [==============================] - 40s 949us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0745 - val_mae: 0.2399\n",
      "Epoch 34/150\n",
      "41818/41818 [==============================] - 40s 955us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0749 - val_mae: 0.2408\n",
      "Epoch 35/150\n",
      "41818/41818 [==============================] - 39s 934us/step - loss: 0.0741 - mae: 0.2396 - val_loss: 0.0745 - val_mae: 0.2399\n",
      "Epoch 36/150\n",
      "41818/41818 [==============================] - 39s 936us/step - loss: 0.0742 - mae: 0.2398 - val_loss: 0.0747 - val_mae: 0.2398\n",
      "Epoch 37/150\n",
      "41818/41818 [==============================] - 39s 935us/step - loss: 0.0742 - mae: 0.2398 - val_loss: 0.0741 - val_mae: 0.2402\n",
      "Epoch 38/150\n",
      "41818/41818 [==============================] - 39s 936us/step - loss: 0.0742 - mae: 0.2396 - val_loss: 0.0738 - val_mae: 0.2398\n",
      "Epoch 39/150\n",
      "41818/41818 [==============================] - 40s 968us/step - loss: 0.0742 - mae: 0.2397 - val_loss: 0.0747 - val_mae: 0.2407\n",
      "Epoch 40/150\n",
      "41818/41818 [==============================] - 39s 941us/step - loss: 0.0742 - mae: 0.2397 - val_loss: 0.0741 - val_mae: 0.2397\n",
      "Epoch 41/150\n",
      "41818/41818 [==============================] - 39s 942us/step - loss: 0.0742 - mae: 0.2397 - val_loss: 0.0741 - val_mae: 0.2400\n",
      "Epoch 42/150\n",
      "41818/41818 [==============================] - 39s 939us/step - loss: 0.0742 - mae: 0.2397 - val_loss: 0.0753 - val_mae: 0.2415\n",
      "Epoch 43/150\n",
      "41818/41818 [==============================] - 39s 941us/step - loss: 0.0742 - mae: 0.2399 - val_loss: 0.0738 - val_mae: 0.2399\n",
      "Epoch 44/150\n",
      "41818/41818 [==============================] - 40s 952us/step - loss: 0.0742 - mae: 0.2398 - val_loss: 0.0747 - val_mae: 0.2398\n",
      "Epoch 45/150\n",
      "41818/41818 [==============================] - 39s 938us/step - loss: 0.0742 - mae: 0.2397 - val_loss: 0.0747 - val_mae: 0.2405\n",
      "Epoch 46/150\n",
      "41818/41818 [==============================] - 39s 941us/step - loss: 0.0742 - mae: 0.2398 - val_loss: 0.0747 - val_mae: 0.2403\n",
      "Epoch 47/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41818/41818 [==============================] - 39s 925us/step - loss: 0.0742 - mae: 0.2398 - val_loss: 0.0746 - val_mae: 0.2403\n",
      "Epoch 48/150\n",
      "41818/41818 [==============================] - 39s 930us/step - loss: 0.0742 - mae: 0.2398 - val_loss: 0.0745 - val_mae: 0.2403\n",
      "Epoch 49/150\n",
      "41818/41818 [==============================] - 39s 921us/step - loss: 0.0742 - mae: 0.2398 - val_loss: 0.0743 - val_mae: 0.2401\n",
      "Epoch 50/150\n",
      "41818/41818 [==============================] - 39s 924us/step - loss: 0.0742 - mae: 0.2399 - val_loss: 0.0741 - val_mae: 0.2402\n",
      "Epoch 51/150\n",
      "41818/41818 [==============================] - 38s 921us/step - loss: 0.0742 - mae: 0.2398 - val_loss: 0.0738 - val_mae: 0.2397\n",
      "Epoch 52/150\n",
      "41818/41818 [==============================] - 39s 927us/step - loss: 0.0742 - mae: 0.2398 - val_loss: 0.0742 - val_mae: 0.2401\n",
      "Epoch 53/150\n",
      "41818/41818 [==============================] - 39s 922us/step - loss: 0.0742 - mae: 0.2399 - val_loss: 0.0750 - val_mae: 0.2400\n",
      "Epoch 54/150\n",
      "41818/41818 [==============================] - 39s 923us/step - loss: 0.0743 - mae: 0.2399 - val_loss: 0.0739 - val_mae: 0.2400\n",
      "Epoch 55/150\n",
      "41818/41818 [==============================] - 39s 923us/step - loss: 0.0742 - mae: 0.2397 - val_loss: 0.0750 - val_mae: 0.2409\n",
      "Epoch 56/150\n",
      "41818/41818 [==============================] - 39s 929us/step - loss: 0.0742 - mae: 0.2397 - val_loss: 0.0747 - val_mae: 0.2399\n",
      "Epoch 57/150\n",
      "41818/41818 [==============================] - 39s 929us/step - loss: 0.0743 - mae: 0.2398 - val_loss: 0.0743 - val_mae: 0.2405\n",
      "Epoch 58/150\n",
      "41818/41818 [==============================] - 39s 926us/step - loss: 0.0743 - mae: 0.2399 - val_loss: 0.0741 - val_mae: 0.2398\n",
      "Epoch 59/150\n",
      "41818/41818 [==============================] - 39s 928us/step - loss: 0.0743 - mae: 0.2398 - val_loss: 0.0742 - val_mae: 0.2403\n",
      "Epoch 60/150\n",
      "41818/41818 [==============================] - 39s 927us/step - loss: 0.0742 - mae: 0.2397 - val_loss: 0.0760 - val_mae: 0.2415\n",
      "Epoch 61/150\n",
      "41818/41818 [==============================] - 39s 927us/step - loss: 0.0742 - mae: 0.2398 - val_loss: 0.0751 - val_mae: 0.2401\n",
      "Epoch 62/150\n",
      "41818/41818 [==============================] - 39s 931us/step - loss: 0.0742 - mae: 0.2398 - val_loss: 0.0738 - val_mae: 0.2398\n",
      "Epoch 63/150\n",
      "41818/41818 [==============================] - 39s 925us/step - loss: 0.0742 - mae: 0.2397 - val_loss: 0.0740 - val_mae: 0.2399\n",
      "Epoch 64/150\n",
      "41818/41818 [==============================] - 39s 927us/step - loss: 0.0742 - mae: 0.2398 - val_loss: 0.0746 - val_mae: 0.2406\n",
      "Epoch 65/150\n",
      "41818/41818 [==============================] - 39s 927us/step - loss: 0.0742 - mae: 0.2397 - val_loss: 0.0752 - val_mae: 0.2401\n",
      "Epoch 66/150\n",
      "41818/41818 [==============================] - 39s 935us/step - loss: 0.0742 - mae: 0.2398 - val_loss: 0.0738 - val_mae: 0.2397\n",
      "Epoch 67/150\n",
      "41818/41818 [==============================] - 39s 926us/step - loss: 0.0742 - mae: 0.2398 - val_loss: 0.0739 - val_mae: 0.2398\n",
      "Epoch 68/150\n",
      "41818/41818 [==============================] - 39s 930us/step - loss: 0.0742 - mae: 0.2397 - val_loss: 0.0747 - val_mae: 0.2401\n",
      "Epoch 69/150\n",
      "41818/41818 [==============================] - 38s 913us/step - loss: 0.0742 - mae: 0.2397 - val_loss: 0.0743 - val_mae: 0.2404\n",
      "Epoch 70/150\n",
      "41818/41818 [==============================] - 39s 930us/step - loss: 0.0743 - mae: 0.2398 - val_loss: 0.0749 - val_mae: 0.2410\n",
      "Epoch 71/150\n",
      "41818/41818 [==============================] - 39s 932us/step - loss: 0.0742 - mae: 0.2399 - val_loss: 0.0745 - val_mae: 0.2401\n",
      "Epoch 72/150\n",
      "41818/41818 [==============================] - 39s 930us/step - loss: 0.0742 - mae: 0.2397 - val_loss: 0.0741 - val_mae: 0.2398\n",
      "Epoch 73/150\n",
      "41818/41818 [==============================] - 39s 924us/step - loss: 0.0742 - mae: 0.2398 - val_loss: 0.0748 - val_mae: 0.2408\n",
      "Epoch 74/150\n",
      "41818/41818 [==============================] - 39s 929us/step - loss: 0.0743 - mae: 0.2398 - val_loss: 0.0749 - val_mae: 0.2405\n",
      "Epoch 75/150\n",
      "41818/41818 [==============================] - 39s 922us/step - loss: 0.0742 - mae: 0.2399 - val_loss: 0.0742 - val_mae: 0.2403\n",
      "Epoch 76/150\n",
      "41818/41818 [==============================] - 39s 939us/step - loss: 0.0743 - mae: 0.2399 - val_loss: 0.0740 - val_mae: 0.2401\n",
      "Epoch 77/150\n",
      "41818/41818 [==============================] - 39s 931us/step - loss: 0.0742 - mae: 0.2398 - val_loss: 0.0750 - val_mae: 0.2408\n",
      "Epoch 78/150\n",
      "41818/41818 [==============================] - 39s 924us/step - loss: 0.0743 - mae: 0.2398 - val_loss: 0.0745 - val_mae: 0.2405\n",
      "Epoch 79/150\n",
      "41818/41818 [==============================] - 39s 927us/step - loss: 0.0743 - mae: 0.2399 - val_loss: 0.0745 - val_mae: 0.2405\n",
      "Epoch 80/150\n",
      "41818/41818 [==============================] - 39s 927us/step - loss: 0.0743 - mae: 0.2400 - val_loss: 0.0742 - val_mae: 0.2399\n",
      "Epoch 81/150\n",
      "41818/41818 [==============================] - 39s 929us/step - loss: 0.0743 - mae: 0.2397 - val_loss: 0.0746 - val_mae: 0.2400\n",
      "Epoch 82/150\n",
      "41818/41818 [==============================] - 39s 931us/step - loss: 0.0743 - mae: 0.2399 - val_loss: 0.0739 - val_mae: 0.2400\n",
      "Epoch 83/150\n",
      "41818/41818 [==============================] - 39s 929us/step - loss: 0.0743 - mae: 0.2399 - val_loss: 0.0758 - val_mae: 0.2413\n",
      "Epoch 84/150\n",
      "41818/41818 [==============================] - 39s 928us/step - loss: 0.0742 - mae: 0.2398 - val_loss: 0.0743 - val_mae: 0.2402\n",
      "Epoch 85/150\n",
      "41818/41818 [==============================] - 39s 932us/step - loss: 0.0742 - mae: 0.2398 - val_loss: 0.0739 - val_mae: 0.2400\n",
      "Epoch 86/150\n",
      "41818/41818 [==============================] - 39s 928us/step - loss: 0.0742 - mae: 0.2399 - val_loss: 0.0739 - val_mae: 0.2400\n",
      "Epoch 87/150\n",
      "41818/41818 [==============================] - 39s 929us/step - loss: 0.0743 - mae: 0.2398 - val_loss: 0.0744 - val_mae: 0.2399\n",
      "Epoch 88/150\n",
      "41818/41818 [==============================] - 39s 927us/step - loss: 0.0742 - mae: 0.2398 - val_loss: 0.0744 - val_mae: 0.2399\n",
      "Epoch 89/150\n",
      "41818/41818 [==============================] - 39s 931us/step - loss: 0.0742 - mae: 0.2398 - val_loss: 0.0744 - val_mae: 0.2403\n",
      "Epoch 90/150\n",
      "41818/41818 [==============================] - 39s 925us/step - loss: 0.0742 - mae: 0.2398 - val_loss: 0.0740 - val_mae: 0.2400\n",
      "Epoch 91/150\n",
      "41818/41818 [==============================] - 39s 928us/step - loss: 0.0743 - mae: 0.2399 - val_loss: 0.0747 - val_mae: 0.2407\n",
      "Epoch 92/150\n",
      "41818/41818 [==============================] - 39s 927us/step - loss: 0.0742 - mae: 0.2398 - val_loss: 0.0747 - val_mae: 0.2406\n",
      "Epoch 93/150\n",
      "41818/41818 [==============================] - 39s 935us/step - loss: 0.0742 - mae: 0.2397 - val_loss: 0.0747 - val_mae: 0.2401\n",
      "Epoch 94/150\n",
      "41818/41818 [==============================] - 39s 939us/step - loss: 0.0743 - mae: 0.2399 - val_loss: 0.0740 - val_mae: 0.2397\n",
      "Epoch 95/150\n",
      "41818/41818 [==============================] - 39s 935us/step - loss: 0.0743 - mae: 0.2398 - val_loss: 0.0743 - val_mae: 0.2404\n",
      "Epoch 96/150\n",
      "41818/41818 [==============================] - 39s 935us/step - loss: 0.0743 - mae: 0.2400 - val_loss: 0.0745 - val_mae: 0.2404\n",
      "Epoch 97/150\n",
      "41818/41818 [==============================] - 39s 939us/step - loss: 0.0743 - mae: 0.2399 - val_loss: 0.0749 - val_mae: 0.2406\n",
      "Epoch 98/150\n",
      "41818/41818 [==============================] - 39s 932us/step - loss: 0.0743 - mae: 0.2398 - val_loss: 0.0745 - val_mae: 0.2403\n",
      "Epoch 99/150\n",
      "41818/41818 [==============================] - 39s 936us/step - loss: 0.0743 - mae: 0.2400 - val_loss: 0.0740 - val_mae: 0.2398\n",
      "Epoch 100/150\n",
      "41818/41818 [==============================] - 39s 938us/step - loss: 0.0743 - mae: 0.2398 - val_loss: 0.0739 - val_mae: 0.2399\n",
      "Epoch 101/150\n",
      "41818/41818 [==============================] - 39s 937us/step - loss: 0.0743 - mae: 0.2398 - val_loss: 0.0742 - val_mae: 0.2395\n",
      "Epoch 102/150\n",
      "41818/41818 [==============================] - 39s 933us/step - loss: 0.0743 - mae: 0.2398 - val_loss: 0.0742 - val_mae: 0.2403\n",
      "Epoch 103/150\n",
      "41818/41818 [==============================] - 39s 935us/step - loss: 0.0743 - mae: 0.2399 - val_loss: 0.0742 - val_mae: 0.2398\n",
      "Epoch 104/150\n",
      "41818/41818 [==============================] - 39s 944us/step - loss: 0.0742 - mae: 0.2398 - val_loss: 0.0739 - val_mae: 0.2399\n",
      "Epoch 105/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41818/41818 [==============================] - 39s 930us/step - loss: 0.0742 - mae: 0.2398 - val_loss: 0.0747 - val_mae: 0.2407\n",
      "Epoch 106/150\n",
      "41818/41818 [==============================] - 39s 926us/step - loss: 0.0743 - mae: 0.2398 - val_loss: 0.0743 - val_mae: 0.2401\n",
      "Epoch 107/150\n",
      "41818/41818 [==============================] - 39s 926us/step - loss: 0.0743 - mae: 0.2400 - val_loss: 0.0745 - val_mae: 0.2399\n",
      "Epoch 108/150\n",
      "41818/41818 [==============================] - 39s 927us/step - loss: 0.0743 - mae: 0.2398 - val_loss: 0.0742 - val_mae: 0.2399\n",
      "Epoch 109/150\n",
      "41818/41818 [==============================] - 39s 922us/step - loss: 0.0743 - mae: 0.2397 - val_loss: 0.0737 - val_mae: 0.2397\n",
      "Epoch 110/150\n",
      "41818/41818 [==============================] - 39s 923us/step - loss: 0.0743 - mae: 0.2398 - val_loss: 0.0742 - val_mae: 0.2398\n",
      "Epoch 111/150\n",
      "41818/41818 [==============================] - 39s 921us/step - loss: 0.0742 - mae: 0.2398 - val_loss: 0.0751 - val_mae: 0.2399\n",
      "Epoch 112/150\n",
      "41818/41818 [==============================] - 39s 922us/step - loss: 0.0743 - mae: 0.2398 - val_loss: 0.0742 - val_mae: 0.2398\n",
      "Epoch 113/150\n",
      "41818/41818 [==============================] - 39s 932us/step - loss: 0.0742 - mae: 0.2398 - val_loss: 0.0738 - val_mae: 0.2397\n",
      "Epoch 114/150\n",
      "41818/41818 [==============================] - 39s 930us/step - loss: 0.0742 - mae: 0.2399 - val_loss: 0.0738 - val_mae: 0.2395\n",
      "Epoch 115/150\n",
      "41818/41818 [==============================] - 39s 924us/step - loss: 0.0742 - mae: 0.2398 - val_loss: 0.0753 - val_mae: 0.2409\n",
      "Epoch 116/150\n",
      "41818/41818 [==============================] - 39s 930us/step - loss: 0.0742 - mae: 0.2399 - val_loss: 0.0743 - val_mae: 0.2399\n",
      "Epoch 117/150\n",
      "41818/41818 [==============================] - 39s 929us/step - loss: 0.0743 - mae: 0.2399 - val_loss: 0.0741 - val_mae: 0.2400\n",
      "Epoch 118/150\n",
      "41818/41818 [==============================] - 39s 928us/step - loss: 0.0743 - mae: 0.2399 - val_loss: 0.0741 - val_mae: 0.2399\n",
      "Epoch 119/150\n",
      "41818/41818 [==============================] - 39s 936us/step - loss: 0.0743 - mae: 0.2398 - val_loss: 0.0745 - val_mae: 0.2404\n",
      "Epoch 120/150\n",
      "41818/41818 [==============================] - 39s 934us/step - loss: 0.0742 - mae: 0.2397 - val_loss: 0.0742 - val_mae: 0.2399\n",
      "Epoch 121/150\n",
      "41818/41818 [==============================] - 39s 921us/step - loss: 0.0743 - mae: 0.2398 - val_loss: 0.0742 - val_mae: 0.2396\n",
      "Epoch 122/150\n",
      "41818/41818 [==============================] - 39s 941us/step - loss: 0.0743 - mae: 0.2398 - val_loss: 0.0743 - val_mae: 0.2401\n",
      "Epoch 123/150\n",
      "41818/41818 [==============================] - 39s 934us/step - loss: 0.0742 - mae: 0.2399 - val_loss: 0.0747 - val_mae: 0.2404\n",
      "Epoch 124/150\n",
      "41818/41818 [==============================] - 39s 929us/step - loss: 0.0743 - mae: 0.2398 - val_loss: 0.0741 - val_mae: 0.2396\n",
      "Epoch 125/150\n",
      "41818/41818 [==============================] - 39s 926us/step - loss: 0.0742 - mae: 0.2398 - val_loss: 0.0741 - val_mae: 0.2402\n",
      "Epoch 126/150\n",
      "41818/41818 [==============================] - 39s 931us/step - loss: 0.0743 - mae: 0.2399 - val_loss: 0.0746 - val_mae: 0.2404\n",
      "Epoch 127/150\n",
      "41818/41818 [==============================] - 39s 929us/step - loss: 0.0743 - mae: 0.2398 - val_loss: 0.0742 - val_mae: 0.2402\n",
      "Epoch 128/150\n",
      "41818/41818 [==============================] - 39s 931us/step - loss: 0.0743 - mae: 0.2400 - val_loss: 0.0741 - val_mae: 0.2400\n",
      "Epoch 129/150\n",
      "41818/41818 [==============================] - 39s 932us/step - loss: 0.0744 - mae: 0.2399 - val_loss: 0.0743 - val_mae: 0.2399\n",
      "Epoch 130/150\n",
      "41818/41818 [==============================] - 39s 933us/step - loss: 0.0743 - mae: 0.2399 - val_loss: 0.0754 - val_mae: 0.2402\n",
      "Epoch 131/150\n",
      "41818/41818 [==============================] - 39s 937us/step - loss: 0.0744 - mae: 0.2398 - val_loss: 0.0741 - val_mae: 0.2399\n",
      "Epoch 132/150\n",
      "41818/41818 [==============================] - 39s 932us/step - loss: 0.0743 - mae: 0.2397 - val_loss: 0.0742 - val_mae: 0.2400\n",
      "Epoch 133/150\n",
      "41818/41818 [==============================] - 39s 930us/step - loss: 0.0743 - mae: 0.2397 - val_loss: 0.0745 - val_mae: 0.2398\n",
      "Epoch 134/150\n",
      "41818/41818 [==============================] - 39s 933us/step - loss: 0.0743 - mae: 0.2397 - val_loss: 0.0744 - val_mae: 0.2398\n",
      "Epoch 135/150\n",
      "41818/41818 [==============================] - 39s 926us/step - loss: 0.0743 - mae: 0.2397 - val_loss: 0.0742 - val_mae: 0.2398\n",
      "Epoch 136/150\n",
      "41818/41818 [==============================] - 39s 930us/step - loss: 0.0743 - mae: 0.2398 - val_loss: 0.0740 - val_mae: 0.2399\n",
      "Epoch 137/150\n",
      "41818/41818 [==============================] - 39s 930us/step - loss: 0.0743 - mae: 0.2398 - val_loss: 0.0743 - val_mae: 0.2400\n",
      "Epoch 138/150\n",
      "41818/41818 [==============================] - 39s 929us/step - loss: 0.0743 - mae: 0.2399 - val_loss: 0.0741 - val_mae: 0.2399\n",
      "Epoch 139/150\n",
      "41818/41818 [==============================] - 39s 924us/step - loss: 0.0743 - mae: 0.2398 - val_loss: 0.0742 - val_mae: 0.2398\n",
      "Epoch 140/150\n",
      "41818/41818 [==============================] - 40s 949us/step - loss: 0.0744 - mae: 0.2399 - val_loss: 0.0746 - val_mae: 0.2403\n",
      "Epoch 141/150\n",
      "41818/41818 [==============================] - 39s 925us/step - loss: 0.0743 - mae: 0.2398 - val_loss: 0.0751 - val_mae: 0.2410\n",
      "Epoch 142/150\n",
      "41818/41818 [==============================] - 39s 933us/step - loss: 0.0743 - mae: 0.2398 - val_loss: 0.0744 - val_mae: 0.2400\n",
      "Epoch 143/150\n",
      "41818/41818 [==============================] - 39s 929us/step - loss: 0.0743 - mae: 0.2398 - val_loss: 0.0756 - val_mae: 0.2402\n",
      "Epoch 144/150\n",
      "41818/41818 [==============================] - 39s 928us/step - loss: 0.0743 - mae: 0.2398 - val_loss: 0.0746 - val_mae: 0.2404\n",
      "Epoch 145/150\n",
      "41818/41818 [==============================] - 39s 937us/step - loss: 0.0743 - mae: 0.2399 - val_loss: 0.0742 - val_mae: 0.2400\n",
      "Epoch 146/150\n",
      "41818/41818 [==============================] - 39s 927us/step - loss: 0.0743 - mae: 0.2397 - val_loss: 0.0745 - val_mae: 0.2401\n",
      "Epoch 147/150\n",
      "41818/41818 [==============================] - 39s 925us/step - loss: 0.0743 - mae: 0.2399 - val_loss: 0.0739 - val_mae: 0.2399\n",
      "Epoch 148/150\n",
      "41818/41818 [==============================] - 39s 930us/step - loss: 0.0743 - mae: 0.2398 - val_loss: 0.0750 - val_mae: 0.2404\n",
      "Epoch 149/150\n",
      "41818/41818 [==============================] - 39s 927us/step - loss: 0.0744 - mae: 0.2399 - val_loss: 0.0741 - val_mae: 0.2401\n",
      "Epoch 150/150\n",
      "41818/41818 [==============================] - 39s 943us/step - loss: 0.0744 - mae: 0.2399 - val_loss: 0.0749 - val_mae: 0.2406\n",
      "processing fold # 3\n",
      "Train on 41818 samples, validate on 10454 samples\n",
      "Epoch 1/150\n",
      "41818/41818 [==============================] - 41s 973us/step - loss: 0.0772 - mae: 0.2415 - val_loss: 0.0751 - val_mae: 0.2406\n",
      "Epoch 2/150\n",
      "41818/41818 [==============================] - 40s 957us/step - loss: 0.0755 - mae: 0.2407 - val_loss: 0.0749 - val_mae: 0.2404\n",
      "Epoch 3/150\n",
      "41818/41818 [==============================] - 40s 959us/step - loss: 0.0754 - mae: 0.2406 - val_loss: 0.0748 - val_mae: 0.2405\n",
      "Epoch 4/150\n",
      "41818/41818 [==============================] - 40s 946us/step - loss: 0.0754 - mae: 0.2406 - val_loss: 0.0754 - val_mae: 0.2403\n",
      "Epoch 5/150\n",
      "41818/41818 [==============================] - 40s 949us/step - loss: 0.0752 - mae: 0.2405 - val_loss: 0.0747 - val_mae: 0.2400\n",
      "Epoch 6/150\n",
      "41818/41818 [==============================] - 40s 956us/step - loss: 0.0747 - mae: 0.2402 - val_loss: 0.0746 - val_mae: 0.2400\n",
      "Epoch 7/150\n",
      "41818/41818 [==============================] - 40s 946us/step - loss: 0.0746 - mae: 0.2402 - val_loss: 0.0742 - val_mae: 0.2402\n",
      "Epoch 8/150\n",
      "41818/41818 [==============================] - 40s 945us/step - loss: 0.0745 - mae: 0.2400 - val_loss: 0.0743 - val_mae: 0.2399\n",
      "Epoch 9/150\n",
      "41818/41818 [==============================] - 40s 959us/step - loss: 0.0744 - mae: 0.2402 - val_loss: 0.0741 - val_mae: 0.2400\n",
      "Epoch 10/150\n",
      "41818/41818 [==============================] - 40s 954us/step - loss: 0.0745 - mae: 0.2403 - val_loss: 0.0739 - val_mae: 0.2400\n",
      "Epoch 11/150\n",
      "41818/41818 [==============================] - 40s 952us/step - loss: 0.0745 - mae: 0.2402 - val_loss: 0.0755 - val_mae: 0.2415\n",
      "Epoch 12/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41818/41818 [==============================] - 40s 956us/step - loss: 0.0744 - mae: 0.2401 - val_loss: 0.0747 - val_mae: 0.2411\n",
      "Epoch 13/150\n",
      "41818/41818 [==============================] - 40s 950us/step - loss: 0.0744 - mae: 0.2403 - val_loss: 0.0740 - val_mae: 0.2402\n",
      "Epoch 14/150\n",
      "41818/41818 [==============================] - 39s 940us/step - loss: 0.0743 - mae: 0.2401 - val_loss: 0.0739 - val_mae: 0.2401\n",
      "Epoch 15/150\n",
      "41818/41818 [==============================] - 39s 944us/step - loss: 0.0742 - mae: 0.2401 - val_loss: 0.0737 - val_mae: 0.2397\n",
      "Epoch 16/150\n",
      "41818/41818 [==============================] - 39s 940us/step - loss: 0.0742 - mae: 0.2400 - val_loss: 0.0739 - val_mae: 0.2398\n",
      "Epoch 17/150\n",
      "41818/41818 [==============================] - 39s 940us/step - loss: 0.0741 - mae: 0.2401 - val_loss: 0.0739 - val_mae: 0.2401\n",
      "Epoch 18/150\n",
      "41818/41818 [==============================] - 39s 932us/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0741 - val_mae: 0.2400\n",
      "Epoch 19/150\n",
      "41818/41818 [==============================] - 39s 938us/step - loss: 0.0741 - mae: 0.2400 - val_loss: 0.0756 - val_mae: 0.2409\n",
      "Epoch 20/150\n",
      "41818/41818 [==============================] - 40s 958us/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0740 - val_mae: 0.2399\n",
      "Epoch 21/150\n",
      "41818/41818 [==============================] - 39s 937us/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0745 - val_mae: 0.2403\n",
      "Epoch 22/150\n",
      "41818/41818 [==============================] - 39s 922us/step - loss: 0.0741 - mae: 0.2401 - val_loss: 0.0738 - val_mae: 0.2397\n",
      "Epoch 23/150\n",
      "41818/41818 [==============================] - 39s 938us/step - loss: 0.0740 - mae: 0.2400 - val_loss: 0.0741 - val_mae: 0.2403\n",
      "Epoch 24/150\n",
      "41818/41818 [==============================] - 39s 934us/step - loss: 0.0740 - mae: 0.2398 - val_loss: 0.0741 - val_mae: 0.2402\n",
      "Epoch 25/150\n",
      "41818/41818 [==============================] - 39s 928us/step - loss: 0.0741 - mae: 0.2400 - val_loss: 0.0744 - val_mae: 0.2405\n",
      "Epoch 26/150\n",
      "41818/41818 [==============================] - 39s 943us/step - loss: 0.0740 - mae: 0.2399 - val_loss: 0.0741 - val_mae: 0.2401\n",
      "Epoch 27/150\n",
      "41818/41818 [==============================] - 39s 943us/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0737 - val_mae: 0.2397\n",
      "Epoch 28/150\n",
      "41818/41818 [==============================] - 39s 944us/step - loss: 0.0741 - mae: 0.2400 - val_loss: 0.0745 - val_mae: 0.2404\n",
      "Epoch 29/150\n",
      "41818/41818 [==============================] - 39s 938us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0739 - val_mae: 0.2399\n",
      "Epoch 30/150\n",
      "41818/41818 [==============================] - 40s 951us/step - loss: 0.0741 - mae: 0.2400 - val_loss: 0.0740 - val_mae: 0.2402\n",
      "Epoch 31/150\n",
      "41818/41818 [==============================] - 40s 946us/step - loss: 0.0742 - mae: 0.2401 - val_loss: 0.0738 - val_mae: 0.2400\n",
      "Epoch 32/150\n",
      "41818/41818 [==============================] - 39s 944us/step - loss: 0.0742 - mae: 0.2402 - val_loss: 0.0741 - val_mae: 0.2403\n",
      "Epoch 33/150\n",
      "41818/41818 [==============================] - 40s 947us/step - loss: 0.0742 - mae: 0.2401 - val_loss: 0.0745 - val_mae: 0.2404\n",
      "Epoch 34/150\n",
      "41818/41818 [==============================] - 39s 944us/step - loss: 0.0742 - mae: 0.2400 - val_loss: 0.0741 - val_mae: 0.2402\n",
      "Epoch 35/150\n",
      "41818/41818 [==============================] - 39s 944us/step - loss: 0.0742 - mae: 0.2401 - val_loss: 0.0739 - val_mae: 0.2401\n",
      "Epoch 36/150\n",
      "41818/41818 [==============================] - 40s 961us/step - loss: 0.0742 - mae: 0.2401 - val_loss: 0.0742 - val_mae: 0.2399\n",
      "Epoch 37/150\n",
      "41818/41818 [==============================] - 40s 951us/step - loss: 0.0742 - mae: 0.2401 - val_loss: 0.0742 - val_mae: 0.2401\n",
      "Epoch 38/150\n",
      "41818/41818 [==============================] - 40s 948us/step - loss: 0.0742 - mae: 0.2401 - val_loss: 0.0743 - val_mae: 0.2402\n",
      "Epoch 39/150\n",
      "41818/41818 [==============================] - 39s 944us/step - loss: 0.0744 - mae: 0.2401 - val_loss: 0.0739 - val_mae: 0.2402\n",
      "Epoch 40/150\n",
      "41818/41818 [==============================] - 40s 959us/step - loss: 0.0742 - mae: 0.2401 - val_loss: 0.0748 - val_mae: 0.2407\n",
      "Epoch 41/150\n",
      "41818/41818 [==============================] - 39s 940us/step - loss: 0.0742 - mae: 0.2400 - val_loss: 0.0741 - val_mae: 0.2403\n",
      "Epoch 42/150\n",
      "41818/41818 [==============================] - 39s 943us/step - loss: 0.0743 - mae: 0.2402 - val_loss: 0.0737 - val_mae: 0.2399\n",
      "Epoch 43/150\n",
      "41818/41818 [==============================] - 40s 951us/step - loss: 0.0742 - mae: 0.2400 - val_loss: 0.0737 - val_mae: 0.2399\n",
      "Epoch 44/150\n",
      "41818/41818 [==============================] - 39s 944us/step - loss: 0.0743 - mae: 0.2401 - val_loss: 0.0740 - val_mae: 0.2398\n",
      "Epoch 45/150\n",
      "41818/41818 [==============================] - 40s 957us/step - loss: 0.0742 - mae: 0.2399 - val_loss: 0.0741 - val_mae: 0.2401\n",
      "Epoch 46/150\n",
      "41818/41818 [==============================] - 39s 942us/step - loss: 0.0743 - mae: 0.2400 - val_loss: 0.0741 - val_mae: 0.2401\n",
      "Epoch 47/150\n",
      "41818/41818 [==============================] - 39s 943us/step - loss: 0.0742 - mae: 0.2401 - val_loss: 0.0741 - val_mae: 0.2401\n",
      "Epoch 48/150\n",
      "41818/41818 [==============================] - 39s 940us/step - loss: 0.0742 - mae: 0.2400 - val_loss: 0.0738 - val_mae: 0.2400\n",
      "Epoch 49/150\n",
      "41818/41818 [==============================] - 40s 961us/step - loss: 0.0742 - mae: 0.2400 - val_loss: 0.0744 - val_mae: 0.2401\n",
      "Epoch 50/150\n",
      "41818/41818 [==============================] - 40s 945us/step - loss: 0.0743 - mae: 0.2400 - val_loss: 0.0746 - val_mae: 0.2406\n",
      "Epoch 51/150\n",
      "41818/41818 [==============================] - 39s 941us/step - loss: 0.0743 - mae: 0.2400 - val_loss: 0.0742 - val_mae: 0.2402\n",
      "Epoch 52/150\n",
      "41818/41818 [==============================] - 40s 946us/step - loss: 0.0742 - mae: 0.2400 - val_loss: 0.0739 - val_mae: 0.2401\n",
      "Epoch 53/150\n",
      "41818/41818 [==============================] - 39s 942us/step - loss: 0.0742 - mae: 0.2400 - val_loss: 0.0738 - val_mae: 0.2397\n",
      "Epoch 54/150\n",
      "41818/41818 [==============================] - 40s 948us/step - loss: 0.0742 - mae: 0.2399 - val_loss: 0.0751 - val_mae: 0.2403\n",
      "Epoch 55/150\n",
      "41818/41818 [==============================] - 39s 944us/step - loss: 0.0743 - mae: 0.2399 - val_loss: 0.0742 - val_mae: 0.2401\n",
      "Epoch 56/150\n",
      "41818/41818 [==============================] - 40s 955us/step - loss: 0.0742 - mae: 0.2400 - val_loss: 0.0740 - val_mae: 0.2400\n",
      "Epoch 57/150\n",
      "41818/41818 [==============================] - 39s 943us/step - loss: 0.0742 - mae: 0.2400 - val_loss: 0.0738 - val_mae: 0.2397\n",
      "Epoch 58/150\n",
      "41818/41818 [==============================] - 40s 959us/step - loss: 0.0742 - mae: 0.2400 - val_loss: 0.0747 - val_mae: 0.2403\n",
      "Epoch 59/150\n",
      "41818/41818 [==============================] - 40s 946us/step - loss: 0.0742 - mae: 0.2399 - val_loss: 0.0740 - val_mae: 0.2398\n",
      "Epoch 60/150\n",
      "41818/41818 [==============================] - 40s 945us/step - loss: 0.0743 - mae: 0.2401 - val_loss: 0.0738 - val_mae: 0.2398\n",
      "Epoch 61/150\n",
      "41818/41818 [==============================] - 40s 951us/step - loss: 0.0743 - mae: 0.2400 - val_loss: 0.0740 - val_mae: 0.2399\n",
      "Epoch 62/150\n",
      "41818/41818 [==============================] - 39s 944us/step - loss: 0.0743 - mae: 0.2400 - val_loss: 0.0742 - val_mae: 0.2401\n",
      "Epoch 63/150\n",
      "41818/41818 [==============================] - 41s 971us/step - loss: 0.0742 - mae: 0.2400 - val_loss: 0.0739 - val_mae: 0.2399\n",
      "Epoch 64/150\n",
      "41818/41818 [==============================] - 41s 980us/step - loss: 0.0742 - mae: 0.2399 - val_loss: 0.0738 - val_mae: 0.2397\n",
      "Epoch 65/150\n",
      "41818/41818 [==============================] - 40s 968us/step - loss: 0.0742 - mae: 0.2399 - val_loss: 0.0743 - val_mae: 0.2401\n",
      "Epoch 66/150\n",
      "41818/41818 [==============================] - 41s 971us/step - loss: 0.0742 - mae: 0.2400 - val_loss: 0.0742 - val_mae: 0.2399\n",
      "Epoch 67/150\n",
      "41818/41818 [==============================] - 40s 953us/step - loss: 0.0743 - mae: 0.2400 - val_loss: 0.0740 - val_mae: 0.2398\n",
      "Epoch 68/150\n",
      "41818/41818 [==============================] - 40s 953us/step - loss: 0.0743 - mae: 0.2399 - val_loss: 0.0745 - val_mae: 0.2398\n",
      "Epoch 69/150\n",
      "41818/41818 [==============================] - 40s 958us/step - loss: 0.0742 - mae: 0.2399 - val_loss: 0.0742 - val_mae: 0.2400\n",
      "Epoch 70/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41818/41818 [==============================] - 39s 940us/step - loss: 0.0743 - mae: 0.2399 - val_loss: 0.0738 - val_mae: 0.2398\n",
      "Epoch 71/150\n",
      "41818/41818 [==============================] - 39s 943us/step - loss: 0.0743 - mae: 0.2399 - val_loss: 0.0744 - val_mae: 0.2400\n",
      "Epoch 72/150\n",
      "41818/41818 [==============================] - 39s 944us/step - loss: 0.0743 - mae: 0.2400 - val_loss: 0.0741 - val_mae: 0.2397\n",
      "Epoch 73/150\n",
      "41818/41818 [==============================] - 40s 946us/step - loss: 0.0743 - mae: 0.2400 - val_loss: 0.0741 - val_mae: 0.2399\n",
      "Epoch 74/150\n",
      "41818/41818 [==============================] - 40s 965us/step - loss: 0.0743 - mae: 0.2399 - val_loss: 0.0739 - val_mae: 0.2399\n",
      "Epoch 75/150\n",
      "41818/41818 [==============================] - 40s 963us/step - loss: 0.0743 - mae: 0.2399 - val_loss: 0.0739 - val_mae: 0.2400\n",
      "Epoch 76/150\n",
      "41818/41818 [==============================] - 43s 1ms/step - loss: 0.0743 - mae: 0.2399 - val_loss: 0.0741 - val_mae: 0.2400\n",
      "Epoch 77/150\n",
      "41818/41818 [==============================] - 40s 961us/step - loss: 0.0743 - mae: 0.2399 - val_loss: 0.0738 - val_mae: 0.2397\n",
      "Epoch 78/150\n",
      "41818/41818 [==============================] - 39s 936us/step - loss: 0.0743 - mae: 0.2401 - val_loss: 0.0741 - val_mae: 0.2398\n",
      "Epoch 79/150\n",
      "41818/41818 [==============================] - 39s 933us/step - loss: 0.0743 - mae: 0.2400 - val_loss: 0.0739 - val_mae: 0.2398\n",
      "Epoch 80/150\n",
      "41818/41818 [==============================] - 39s 939us/step - loss: 0.0743 - mae: 0.2398 - val_loss: 0.0764 - val_mae: 0.2410\n",
      "Epoch 81/150\n",
      "41818/41818 [==============================] - 39s 941us/step - loss: 0.0743 - mae: 0.2400 - val_loss: 0.0737 - val_mae: 0.2397\n",
      "Epoch 82/150\n",
      "41818/41818 [==============================] - 39s 934us/step - loss: 0.0742 - mae: 0.2400 - val_loss: 0.0744 - val_mae: 0.2401\n",
      "Epoch 83/150\n",
      "41818/41818 [==============================] - 39s 939us/step - loss: 0.0743 - mae: 0.2399 - val_loss: 0.0742 - val_mae: 0.2401\n",
      "Epoch 84/150\n",
      "41818/41818 [==============================] - 39s 938us/step - loss: 0.0745 - mae: 0.2400 - val_loss: 0.0743 - val_mae: 0.2401\n",
      "Epoch 85/150\n",
      "41818/41818 [==============================] - 39s 938us/step - loss: 0.0743 - mae: 0.2400 - val_loss: 0.0745 - val_mae: 0.2404\n",
      "Epoch 86/150\n",
      "41818/41818 [==============================] - 40s 948us/step - loss: 0.0745 - mae: 0.2400 - val_loss: 0.0738 - val_mae: 0.2398\n",
      "Epoch 87/150\n",
      "41818/41818 [==============================] - 39s 942us/step - loss: 0.0743 - mae: 0.2400 - val_loss: 0.0752 - val_mae: 0.2404\n",
      "Epoch 88/150\n",
      "41818/41818 [==============================] - 39s 937us/step - loss: 0.0743 - mae: 0.2398 - val_loss: 0.0744 - val_mae: 0.2401\n",
      "Epoch 89/150\n",
      "41818/41818 [==============================] - 40s 950us/step - loss: 0.0744 - mae: 0.2399 - val_loss: 0.0750 - val_mae: 0.2401\n",
      "Epoch 90/150\n",
      "41818/41818 [==============================] - 39s 943us/step - loss: 0.0745 - mae: 0.2401 - val_loss: 0.0761 - val_mae: 0.2408\n",
      "Epoch 91/150\n",
      "41818/41818 [==============================] - 39s 943us/step - loss: 0.0745 - mae: 0.2400 - val_loss: 0.0743 - val_mae: 0.2399\n",
      "Epoch 92/150\n",
      "41818/41818 [==============================] - 40s 945us/step - loss: 0.0744 - mae: 0.2401 - val_loss: 0.0751 - val_mae: 0.2401\n",
      "Epoch 93/150\n",
      "41818/41818 [==============================] - 40s 951us/step - loss: 0.0745 - mae: 0.2401 - val_loss: 0.0739 - val_mae: 0.2399\n",
      "Epoch 94/150\n",
      "41818/41818 [==============================] - 41s 973us/step - loss: 0.0744 - mae: 0.2399 - val_loss: 0.0743 - val_mae: 0.2398\n",
      "Epoch 95/150\n",
      "41818/41818 [==============================] - 41s 977us/step - loss: 0.0745 - mae: 0.2399 - val_loss: 0.0763 - val_mae: 0.2408\n",
      "Epoch 96/150\n",
      "41818/41818 [==============================] - 41s 972us/step - loss: 0.0745 - mae: 0.2400 - val_loss: 0.0743 - val_mae: 0.2399\n",
      "Epoch 97/150\n",
      "41818/41818 [==============================] - 40s 965us/step - loss: 0.0744 - mae: 0.2401 - val_loss: 0.0739 - val_mae: 0.2397\n",
      "Epoch 98/150\n",
      "41818/41818 [==============================] - 40s 954us/step - loss: 0.0744 - mae: 0.2399 - val_loss: 0.0746 - val_mae: 0.2401\n",
      "Epoch 99/150\n",
      "41818/41818 [==============================] - 40s 958us/step - loss: 0.0745 - mae: 0.2400 - val_loss: 0.0738 - val_mae: 0.2396\n",
      "Epoch 100/150\n",
      "41818/41818 [==============================] - 40s 945us/step - loss: 0.0746 - mae: 0.2400 - val_loss: 0.0740 - val_mae: 0.2397\n",
      "Epoch 101/150\n",
      "41818/41818 [==============================] - 39s 937us/step - loss: 0.0746 - mae: 0.2401 - val_loss: 0.0743 - val_mae: 0.2397\n",
      "Epoch 102/150\n",
      "41818/41818 [==============================] - 39s 940us/step - loss: 0.0747 - mae: 0.2401 - val_loss: 0.0745 - val_mae: 0.2401\n",
      "Epoch 103/150\n",
      "41818/41818 [==============================] - 39s 945us/step - loss: 0.0748 - mae: 0.2401 - val_loss: 0.0748 - val_mae: 0.2403\n",
      "Epoch 104/150\n",
      "41818/41818 [==============================] - 41s 991us/step - loss: 0.0748 - mae: 0.2401 - val_loss: 0.0740 - val_mae: 0.2396\n",
      "Epoch 105/150\n",
      "41818/41818 [==============================] - 41s 975us/step - loss: 0.0748 - mae: 0.2400 - val_loss: 0.0747 - val_mae: 0.2397\n",
      "Epoch 106/150\n",
      "41818/41818 [==============================] - 40s 968us/step - loss: 0.0748 - mae: 0.2401 - val_loss: 0.0747 - val_mae: 0.2402\n",
      "Epoch 107/150\n",
      "41818/41818 [==============================] - 40s 962us/step - loss: 0.0747 - mae: 0.2402 - val_loss: 0.0742 - val_mae: 0.2396\n",
      "Epoch 108/150\n",
      "41818/41818 [==============================] - 40s 951us/step - loss: 0.0748 - mae: 0.2402 - val_loss: 0.0744 - val_mae: 0.2399\n",
      "Epoch 109/150\n",
      "41818/41818 [==============================] - 39s 937us/step - loss: 0.0748 - mae: 0.2401 - val_loss: 0.0751 - val_mae: 0.2401\n",
      "Epoch 110/150\n",
      "41818/41818 [==============================] - 40s 953us/step - loss: 0.0749 - mae: 0.2401 - val_loss: 0.0744 - val_mae: 0.2400\n",
      "Epoch 111/150\n",
      "41818/41818 [==============================] - 39s 940us/step - loss: 0.0747 - mae: 0.2401 - val_loss: 0.0749 - val_mae: 0.2401\n",
      "Epoch 112/150\n",
      "41818/41818 [==============================] - 40s 945us/step - loss: 0.0748 - mae: 0.2399 - val_loss: 0.0746 - val_mae: 0.2401\n",
      "Epoch 113/150\n",
      "41818/41818 [==============================] - 40s 957us/step - loss: 0.0748 - mae: 0.2400 - val_loss: 0.0746 - val_mae: 0.2400\n",
      "Epoch 114/150\n",
      "41818/41818 [==============================] - 39s 942us/step - loss: 0.0747 - mae: 0.2402 - val_loss: 0.0748 - val_mae: 0.2401\n",
      "Epoch 115/150\n",
      "41818/41818 [==============================] - 39s 942us/step - loss: 0.0748 - mae: 0.2401 - val_loss: 0.0746 - val_mae: 0.2399\n",
      "Epoch 116/150\n",
      "41818/41818 [==============================] - 40s 945us/step - loss: 0.0748 - mae: 0.2401 - val_loss: 0.0745 - val_mae: 0.2400\n",
      "Epoch 117/150\n",
      "41818/41818 [==============================] - 40s 958us/step - loss: 0.0748 - mae: 0.2401 - val_loss: 0.0750 - val_mae: 0.2402\n",
      "Epoch 118/150\n",
      "41818/41818 [==============================] - 40s 951us/step - loss: 0.0748 - mae: 0.2400 - val_loss: 0.0747 - val_mae: 0.2402\n",
      "Epoch 119/150\n",
      "41818/41818 [==============================] - 40s 952us/step - loss: 0.0747 - mae: 0.2400 - val_loss: 0.0754 - val_mae: 0.2404\n",
      "Epoch 120/150\n",
      "41818/41818 [==============================] - 40s 950us/step - loss: 0.0748 - mae: 0.2401 - val_loss: 0.0744 - val_mae: 0.2401\n",
      "Epoch 121/150\n",
      "41818/41818 [==============================] - 40s 955us/step - loss: 0.0747 - mae: 0.2401 - val_loss: 0.0748 - val_mae: 0.2402\n",
      "Epoch 122/150\n",
      "41818/41818 [==============================] - 40s 958us/step - loss: 0.0748 - mae: 0.2401 - val_loss: 0.0743 - val_mae: 0.2398\n",
      "Epoch 123/150\n",
      "41818/41818 [==============================] - 40s 949us/step - loss: 0.0747 - mae: 0.2401 - val_loss: 0.0747 - val_mae: 0.2401\n",
      "Epoch 124/150\n",
      "41818/41818 [==============================] - 40s 947us/step - loss: 0.0748 - mae: 0.2401 - val_loss: 0.0743 - val_mae: 0.2398\n",
      "Epoch 125/150\n",
      "41818/41818 [==============================] - 40s 954us/step - loss: 0.0748 - mae: 0.2401 - val_loss: 0.0745 - val_mae: 0.2401\n",
      "Epoch 126/150\n",
      "41818/41818 [==============================] - 40s 945us/step - loss: 0.0748 - mae: 0.2402 - val_loss: 0.0747 - val_mae: 0.2401\n",
      "Epoch 127/150\n",
      "41818/41818 [==============================] - 41s 979us/step - loss: 0.0747 - mae: 0.2401 - val_loss: 0.0743 - val_mae: 0.2398\n",
      "Epoch 128/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41818/41818 [==============================] - 41s 988us/step - loss: 0.0748 - mae: 0.2400 - val_loss: 0.0744 - val_mae: 0.2398\n",
      "Epoch 129/150\n",
      "41818/41818 [==============================] - 40s 947us/step - loss: 0.0748 - mae: 0.2401 - val_loss: 0.0740 - val_mae: 0.2398\n",
      "Epoch 130/150\n",
      "41818/41818 [==============================] - 39s 940us/step - loss: 0.0746 - mae: 0.2401 - val_loss: 0.0746 - val_mae: 0.2399\n",
      "Epoch 131/150\n",
      "41818/41818 [==============================] - 40s 959us/step - loss: 0.0747 - mae: 0.2400 - val_loss: 0.0747 - val_mae: 0.2402\n",
      "Epoch 132/150\n",
      "41818/41818 [==============================] - 39s 936us/step - loss: 0.0747 - mae: 0.2400 - val_loss: 0.0743 - val_mae: 0.2401\n",
      "Epoch 133/150\n",
      "41818/41818 [==============================] - 39s 938us/step - loss: 0.0747 - mae: 0.2401 - val_loss: 0.0746 - val_mae: 0.2403\n",
      "Epoch 134/150\n",
      "41818/41818 [==============================] - 39s 943us/step - loss: 0.0747 - mae: 0.2402 - val_loss: 0.0749 - val_mae: 0.2404\n",
      "Epoch 135/150\n",
      "41818/41818 [==============================] - 40s 945us/step - loss: 0.0748 - mae: 0.2402 - val_loss: 0.0748 - val_mae: 0.2401\n",
      "Epoch 136/150\n",
      "41818/41818 [==============================] - 39s 936us/step - loss: 0.0747 - mae: 0.2401 - val_loss: 0.0745 - val_mae: 0.2402\n",
      "Epoch 137/150\n",
      "41818/41818 [==============================] - 41s 971us/step - loss: 0.0748 - mae: 0.2401 - val_loss: 0.0741 - val_mae: 0.2398\n",
      "Epoch 138/150\n",
      "41818/41818 [==============================] - 41s 969us/step - loss: 0.0748 - mae: 0.2401 - val_loss: 0.0746 - val_mae: 0.2401\n",
      "Epoch 139/150\n",
      "41818/41818 [==============================] - 40s 961us/step - loss: 0.0747 - mae: 0.2402 - val_loss: 0.0746 - val_mae: 0.2402\n",
      "Epoch 140/150\n",
      "41818/41818 [==============================] - 40s 946us/step - loss: 0.0748 - mae: 0.2401 - val_loss: 0.0750 - val_mae: 0.2403\n",
      "Epoch 141/150\n",
      "41818/41818 [==============================] - 39s 938us/step - loss: 0.0748 - mae: 0.2401 - val_loss: 0.0746 - val_mae: 0.2400\n",
      "Epoch 142/150\n",
      "41818/41818 [==============================] - 39s 934us/step - loss: 0.0747 - mae: 0.2401 - val_loss: 0.0747 - val_mae: 0.2400\n",
      "Epoch 143/150\n",
      "41818/41818 [==============================] - 40s 945us/step - loss: 0.0747 - mae: 0.2400 - val_loss: 0.0742 - val_mae: 0.2398\n",
      "Epoch 144/150\n",
      "41818/41818 [==============================] - 39s 940us/step - loss: 0.0747 - mae: 0.2401 - val_loss: 0.0744 - val_mae: 0.2400\n",
      "Epoch 145/150\n",
      "41818/41818 [==============================] - 39s 941us/step - loss: 0.0746 - mae: 0.2401 - val_loss: 0.0748 - val_mae: 0.2400\n",
      "Epoch 146/150\n",
      "41818/41818 [==============================] - 39s 937us/step - loss: 0.0747 - mae: 0.2401 - val_loss: 0.0753 - val_mae: 0.2406\n",
      "Epoch 147/150\n",
      "41818/41818 [==============================] - 39s 939us/step - loss: 0.0748 - mae: 0.2401 - val_loss: 0.0750 - val_mae: 0.2403\n",
      "Epoch 148/150\n",
      "41818/41818 [==============================] - 40s 945us/step - loss: 0.0748 - mae: 0.2401 - val_loss: 0.0751 - val_mae: 0.2404\n",
      "Epoch 149/150\n",
      "41818/41818 [==============================] - 40s 951us/step - loss: 0.0748 - mae: 0.2401 - val_loss: 0.0744 - val_mae: 0.2399\n",
      "Epoch 150/150\n",
      "41818/41818 [==============================] - 40s 959us/step - loss: 0.0748 - mae: 0.2401 - val_loss: 0.0745 - val_mae: 0.2401\n",
      "processing fold # 4\n",
      "Train on 41818 samples, validate on 10454 samples\n",
      "Epoch 1/150\n",
      "41818/41818 [==============================] - 40s 965us/step - loss: 0.0766 - mae: 0.2410 - val_loss: 0.0754 - val_mae: 0.2405\n",
      "Epoch 2/150\n",
      "41818/41818 [==============================] - 41s 988us/step - loss: 0.0757 - mae: 0.2406 - val_loss: 0.0755 - val_mae: 0.2409\n",
      "Epoch 3/150\n",
      "41818/41818 [==============================] - 42s 1ms/step - loss: 0.0753 - mae: 0.2405 - val_loss: 0.0754 - val_mae: 0.2413\n",
      "Epoch 4/150\n",
      "41818/41818 [==============================] - 42s 994us/step - loss: 0.0754 - mae: 0.2403 - val_loss: 0.0753 - val_mae: 0.2404\n",
      "Epoch 5/150\n",
      "41818/41818 [==============================] - 41s 986us/step - loss: 0.0753 - mae: 0.2402 - val_loss: 0.0756 - val_mae: 0.2408\n",
      "Epoch 6/150\n",
      "41818/41818 [==============================] - 41s 990us/step - loss: 0.0750 - mae: 0.2399 - val_loss: 0.0750 - val_mae: 0.2403\n",
      "Epoch 7/150\n",
      "41818/41818 [==============================] - 40s 952us/step - loss: 0.0746 - mae: 0.2397 - val_loss: 0.0747 - val_mae: 0.2404\n",
      "Epoch 8/150\n",
      "41818/41818 [==============================] - 41s 985us/step - loss: 0.0744 - mae: 0.2398 - val_loss: 0.0743 - val_mae: 0.2400\n",
      "Epoch 9/150\n",
      "41818/41818 [==============================] - 42s 998us/step - loss: 0.0742 - mae: 0.2397 - val_loss: 0.0741 - val_mae: 0.2401\n",
      "Epoch 10/150\n",
      "41818/41818 [==============================] - 41s 971us/step - loss: 0.0741 - mae: 0.2396 - val_loss: 0.0745 - val_mae: 0.2406\n",
      "Epoch 11/150\n",
      "41818/41818 [==============================] - 40s 952us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0745 - val_mae: 0.2404\n",
      "Epoch 12/150\n",
      "41818/41818 [==============================] - 43s 1ms/step - loss: 0.0740 - mae: 0.2397 - val_loss: 0.0744 - val_mae: 0.2403\n",
      "Epoch 13/150\n",
      "41818/41818 [==============================] - 40s 953us/step - loss: 0.0740 - mae: 0.2398 - val_loss: 0.0740 - val_mae: 0.2401\n",
      "Epoch 14/150\n",
      "41818/41818 [==============================] - 41s 970us/step - loss: 0.0740 - mae: 0.2397 - val_loss: 0.0739 - val_mae: 0.2399\n",
      "Epoch 15/150\n",
      "41818/41818 [==============================] - 40s 958us/step - loss: 0.0740 - mae: 0.2398 - val_loss: 0.0738 - val_mae: 0.2399\n",
      "Epoch 16/150\n",
      "41818/41818 [==============================] - 40s 950us/step - loss: 0.0740 - mae: 0.2396 - val_loss: 0.0741 - val_mae: 0.2403\n",
      "Epoch 17/150\n",
      "41818/41818 [==============================] - 40s 959us/step - loss: 0.0740 - mae: 0.2398 - val_loss: 0.0741 - val_mae: 0.2403\n",
      "Epoch 18/150\n",
      "41818/41818 [==============================] - 41s 975us/step - loss: 0.0740 - mae: 0.2397 - val_loss: 0.0743 - val_mae: 0.2404\n",
      "Epoch 19/150\n",
      "41818/41818 [==============================] - 40s 953us/step - loss: 0.0740 - mae: 0.2397 - val_loss: 0.0745 - val_mae: 0.2404\n",
      "Epoch 20/150\n",
      "41818/41818 [==============================] - 40s 958us/step - loss: 0.0740 - mae: 0.2398 - val_loss: 0.0747 - val_mae: 0.2406\n",
      "Epoch 21/150\n",
      "41818/41818 [==============================] - 40s 954us/step - loss: 0.0740 - mae: 0.2398 - val_loss: 0.0739 - val_mae: 0.2401\n",
      "Epoch 22/150\n",
      "41818/41818 [==============================] - 40s 949us/step - loss: 0.0740 - mae: 0.2398 - val_loss: 0.0738 - val_mae: 0.2400\n",
      "Epoch 23/150\n",
      "41818/41818 [==============================] - 39s 943us/step - loss: 0.0740 - mae: 0.2397 - val_loss: 0.0754 - val_mae: 0.2410\n",
      "Epoch 24/150\n",
      "41818/41818 [==============================] - 40s 952us/step - loss: 0.0740 - mae: 0.2398 - val_loss: 0.0750 - val_mae: 0.2405\n",
      "Epoch 25/150\n",
      "41818/41818 [==============================] - 40s 963us/step - loss: 0.0740 - mae: 0.2397 - val_loss: 0.0744 - val_mae: 0.2407\n",
      "Epoch 26/150\n",
      "41818/41818 [==============================] - 40s 967us/step - loss: 0.0740 - mae: 0.2397 - val_loss: 0.0742 - val_mae: 0.2400\n",
      "Epoch 27/150\n",
      "41818/41818 [==============================] - 40s 965us/step - loss: 0.0740 - mae: 0.2398 - val_loss: 0.0740 - val_mae: 0.2403\n",
      "Epoch 28/150\n",
      "41818/41818 [==============================] - 41s 992us/step - loss: 0.0740 - mae: 0.2397 - val_loss: 0.0739 - val_mae: 0.2400\n",
      "Epoch 29/150\n",
      "41818/41818 [==============================] - 42s 1ms/step - loss: 0.0740 - mae: 0.2397 - val_loss: 0.0742 - val_mae: 0.2400\n",
      "Epoch 30/150\n",
      "41818/41818 [==============================] - 41s 991us/step - loss: 0.0740 - mae: 0.2397 - val_loss: 0.0739 - val_mae: 0.2400\n",
      "Epoch 31/150\n",
      "41818/41818 [==============================] - 40s 968us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0744 - val_mae: 0.2404\n",
      "Epoch 32/150\n",
      "41818/41818 [==============================] - 40s 960us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0742 - val_mae: 0.2400\n",
      "Epoch 33/150\n",
      "41818/41818 [==============================] - 41s 971us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0743 - val_mae: 0.2404\n",
      "Epoch 34/150\n",
      "41818/41818 [==============================] - 40s 963us/step - loss: 0.0740 - mae: 0.2397 - val_loss: 0.0742 - val_mae: 0.2403\n",
      "Epoch 35/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41818/41818 [==============================] - 40s 958us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0742 - val_mae: 0.2403\n",
      "Epoch 36/150\n",
      "41818/41818 [==============================] - 40s 967us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0744 - val_mae: 0.2403\n",
      "Epoch 37/150\n",
      "41818/41818 [==============================] - 40s 953us/step - loss: 0.0740 - mae: 0.2397 - val_loss: 0.0743 - val_mae: 0.2402\n",
      "Epoch 38/150\n",
      "41818/41818 [==============================] - 40s 946us/step - loss: 0.0740 - mae: 0.2397 - val_loss: 0.0745 - val_mae: 0.2404\n",
      "Epoch 39/150\n",
      "41818/41818 [==============================] - 40s 958us/step - loss: 0.0740 - mae: 0.2398 - val_loss: 0.0741 - val_mae: 0.2400\n",
      "Epoch 40/150\n",
      "41818/41818 [==============================] - 40s 950us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0738 - val_mae: 0.2401\n",
      "Epoch 41/150\n",
      "41818/41818 [==============================] - 40s 951us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0744 - val_mae: 0.2401\n",
      "Epoch 42/150\n",
      "41818/41818 [==============================] - 40s 950us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0740 - val_mae: 0.2400\n",
      "Epoch 43/150\n",
      "41818/41818 [==============================] - 40s 957us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0744 - val_mae: 0.2405\n",
      "Epoch 44/150\n",
      "41818/41818 [==============================] - 40s 959us/step - loss: 0.0740 - mae: 0.2397 - val_loss: 0.0740 - val_mae: 0.2402\n",
      "Epoch 45/150\n",
      "41818/41818 [==============================] - 40s 956us/step - loss: 0.0740 - mae: 0.2397 - val_loss: 0.0742 - val_mae: 0.2400\n",
      "Epoch 46/150\n",
      "41818/41818 [==============================] - 41s 985us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0746 - val_mae: 0.2405\n",
      "Epoch 47/150\n",
      "41818/41818 [==============================] - 41s 974us/step - loss: 0.0740 - mae: 0.2397 - val_loss: 0.0739 - val_mae: 0.2401\n",
      "Epoch 48/150\n",
      "41818/41818 [==============================] - 41s 975us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0741 - val_mae: 0.2401\n",
      "Epoch 49/150\n",
      "41818/41818 [==============================] - 41s 979us/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0742 - val_mae: 0.2402\n",
      "Epoch 50/150\n",
      "41818/41818 [==============================] - 41s 972us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0739 - val_mae: 0.2401\n",
      "Epoch 51/150\n",
      "41818/41818 [==============================] - 41s 971us/step - loss: 0.0740 - mae: 0.2398 - val_loss: 0.0741 - val_mae: 0.2402\n",
      "Epoch 52/150\n",
      "41818/41818 [==============================] - 40s 946us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0740 - val_mae: 0.2401\n",
      "Epoch 53/150\n",
      "41818/41818 [==============================] - 40s 948us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0740 - val_mae: 0.2402\n",
      "Epoch 54/150\n",
      "41818/41818 [==============================] - 40s 959us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0749 - val_mae: 0.2405\n",
      "Epoch 55/150\n",
      "41818/41818 [==============================] - 40s 950us/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0741 - val_mae: 0.2401\n",
      "Epoch 56/150\n",
      "41818/41818 [==============================] - 40s 952us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0741 - val_mae: 0.2401\n",
      "Epoch 57/150\n",
      "41818/41818 [==============================] - 40s 961us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0741 - val_mae: 0.2403\n",
      "Epoch 58/150\n",
      "41818/41818 [==============================] - 40s 961us/step - loss: 0.0740 - mae: 0.2397 - val_loss: 0.0747 - val_mae: 0.2406\n",
      "Epoch 59/150\n",
      "41818/41818 [==============================] - 40s 964us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0743 - val_mae: 0.2401\n",
      "Epoch 60/150\n",
      "41818/41818 [==============================] - 40s 960us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0744 - val_mae: 0.2406\n",
      "Epoch 61/150\n",
      "41818/41818 [==============================] - 40s 966us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0749 - val_mae: 0.2411\n",
      "Epoch 62/150\n",
      "41818/41818 [==============================] - 40s 960us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0742 - val_mae: 0.2401\n",
      "Epoch 63/150\n",
      "41818/41818 [==============================] - 40s 957us/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0750 - val_mae: 0.2405\n",
      "Epoch 64/150\n",
      "41818/41818 [==============================] - 40s 954us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0740 - val_mae: 0.2403\n",
      "Epoch 65/150\n",
      "41818/41818 [==============================] - 40s 955us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0741 - val_mae: 0.2400\n",
      "Epoch 66/150\n",
      "41818/41818 [==============================] - 40s 946us/step - loss: 0.0740 - mae: 0.2396 - val_loss: 0.0740 - val_mae: 0.2402\n",
      "Epoch 67/150\n",
      "41818/41818 [==============================] - 40s 958us/step - loss: 0.0740 - mae: 0.2398 - val_loss: 0.0746 - val_mae: 0.2402\n",
      "Epoch 68/150\n",
      "41818/41818 [==============================] - 40s 954us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0745 - val_mae: 0.2408\n",
      "Epoch 69/150\n",
      "41818/41818 [==============================] - 40s 952us/step - loss: 0.0740 - mae: 0.2398 - val_loss: 0.0748 - val_mae: 0.2409\n",
      "Epoch 70/150\n",
      "41818/41818 [==============================] - 40s 950us/step - loss: 0.0740 - mae: 0.2398 - val_loss: 0.0744 - val_mae: 0.2404\n",
      "Epoch 71/150\n",
      "41818/41818 [==============================] - 40s 952us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0743 - val_mae: 0.2406\n",
      "Epoch 72/150\n",
      "41818/41818 [==============================] - 41s 969us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0745 - val_mae: 0.2404\n",
      "Epoch 73/150\n",
      "41818/41818 [==============================] - 40s 958us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0744 - val_mae: 0.2405\n",
      "Epoch 74/150\n",
      "41818/41818 [==============================] - 40s 954us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0742 - val_mae: 0.2404\n",
      "Epoch 75/150\n",
      "41818/41818 [==============================] - 40s 962us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0742 - val_mae: 0.2403\n",
      "Epoch 76/150\n",
      "41818/41818 [==============================] - 41s 969us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0739 - val_mae: 0.2400\n",
      "Epoch 77/150\n",
      "41818/41818 [==============================] - 41s 986us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0741 - val_mae: 0.2403\n",
      "Epoch 78/150\n",
      "41818/41818 [==============================] - 41s 982us/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0748 - val_mae: 0.2407\n",
      "Epoch 79/150\n",
      "41818/41818 [==============================] - 42s 994us/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0739 - val_mae: 0.2402\n",
      "Epoch 80/150\n",
      "41818/41818 [==============================] - 41s 983us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0743 - val_mae: 0.2402\n",
      "Epoch 81/150\n",
      "41818/41818 [==============================] - 41s 983us/step - loss: 0.0741 - mae: 0.2400 - val_loss: 0.0744 - val_mae: 0.2404\n",
      "Epoch 82/150\n",
      "41818/41818 [==============================] - 41s 991us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0752 - val_mae: 0.2411\n",
      "Epoch 83/150\n",
      "41818/41818 [==============================] - 40s 960us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0745 - val_mae: 0.2403\n",
      "Epoch 84/150\n",
      "41818/41818 [==============================] - 40s 964us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0751 - val_mae: 0.2410\n",
      "Epoch 85/150\n",
      "41818/41818 [==============================] - 40s 963us/step - loss: 0.0741 - mae: 0.2400 - val_loss: 0.0740 - val_mae: 0.2402\n",
      "Epoch 86/150\n",
      "41818/41818 [==============================] - 40s 961us/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0752 - val_mae: 0.2409\n",
      "Epoch 87/150\n",
      "41818/41818 [==============================] - 40s 963us/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0743 - val_mae: 0.2402\n",
      "Epoch 88/150\n",
      "41818/41818 [==============================] - 41s 976us/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0742 - val_mae: 0.2404\n",
      "Epoch 89/150\n",
      "41818/41818 [==============================] - 41s 970us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0748 - val_mae: 0.2406\n",
      "Epoch 90/150\n",
      "41818/41818 [==============================] - 40s 964us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0739 - val_mae: 0.2402\n",
      "Epoch 91/150\n",
      "41818/41818 [==============================] - 40s 959us/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0742 - val_mae: 0.2404\n",
      "Epoch 92/150\n",
      "41818/41818 [==============================] - 40s 967us/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0741 - val_mae: 0.2403\n",
      "Epoch 93/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41818/41818 [==============================] - 40s 957us/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0748 - val_mae: 0.2408\n",
      "Epoch 94/150\n",
      "41818/41818 [==============================] - 40s 952us/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0744 - val_mae: 0.2405\n",
      "Epoch 95/150\n",
      "41818/41818 [==============================] - 40s 959us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0739 - val_mae: 0.2400\n",
      "Epoch 96/150\n",
      "41818/41818 [==============================] - 40s 952us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0747 - val_mae: 0.2404\n",
      "Epoch 97/150\n",
      "41818/41818 [==============================] - 40s 953us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0751 - val_mae: 0.2409\n",
      "Epoch 98/150\n",
      "41818/41818 [==============================] - 40s 947us/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0746 - val_mae: 0.2405\n",
      "Epoch 99/150\n",
      "41818/41818 [==============================] - 41s 981us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0741 - val_mae: 0.2402\n",
      "Epoch 100/150\n",
      "41818/41818 [==============================] - 40s 950us/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0748 - val_mae: 0.2407\n",
      "Epoch 101/150\n",
      "41818/41818 [==============================] - 41s 981us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0746 - val_mae: 0.2405\n",
      "Epoch 102/150\n",
      "41818/41818 [==============================] - 41s 982us/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0753 - val_mae: 0.2409\n",
      "Epoch 103/150\n",
      "41818/41818 [==============================] - 40s 962us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0741 - val_mae: 0.2402\n",
      "Epoch 104/150\n",
      "41818/41818 [==============================] - 40s 953us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0742 - val_mae: 0.2404\n",
      "Epoch 105/150\n",
      "41818/41818 [==============================] - 40s 949us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0740 - val_mae: 0.2402\n",
      "Epoch 106/150\n",
      "41818/41818 [==============================] - 40s 961us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0743 - val_mae: 0.2404\n",
      "Epoch 107/150\n",
      "41818/41818 [==============================] - 40s 960us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0750 - val_mae: 0.2405\n",
      "Epoch 108/150\n",
      "41818/41818 [==============================] - 40s 949us/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0745 - val_mae: 0.2405\n",
      "Epoch 109/150\n",
      "41818/41818 [==============================] - 40s 954us/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0744 - val_mae: 0.2404\n",
      "Epoch 110/150\n",
      "41818/41818 [==============================] - 40s 963us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0746 - val_mae: 0.2406\n",
      "Epoch 111/150\n",
      "41818/41818 [==============================] - 41s 988us/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0745 - val_mae: 0.2406\n",
      "Epoch 112/150\n",
      "41818/41818 [==============================] - 40s 954us/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0743 - val_mae: 0.2405\n",
      "Epoch 113/150\n",
      "41818/41818 [==============================] - 40s 964us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0740 - val_mae: 0.2402\n",
      "Epoch 114/150\n",
      "41818/41818 [==============================] - 40s 959us/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0739 - val_mae: 0.2401\n",
      "Epoch 115/150\n",
      "41818/41818 [==============================] - 40s 961us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0739 - val_mae: 0.2400\n",
      "Epoch 116/150\n",
      "41818/41818 [==============================] - 40s 951us/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0742 - val_mae: 0.2402\n",
      "Epoch 117/150\n",
      "41818/41818 [==============================] - 40s 968us/step - loss: 0.0741 - mae: 0.2397 - val_loss: 0.0743 - val_mae: 0.2405\n",
      "Epoch 118/150\n",
      "41818/41818 [==============================] - 40s 954us/step - loss: 0.0742 - mae: 0.2398 - val_loss: 0.0744 - val_mae: 0.2402\n",
      "Epoch 119/150\n",
      "41818/41818 [==============================] - 40s 959us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0742 - val_mae: 0.2403\n",
      "Epoch 120/150\n",
      "41818/41818 [==============================] - 41s 971us/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0746 - val_mae: 0.2407\n",
      "Epoch 121/150\n",
      "41818/41818 [==============================] - 41s 976us/step - loss: 0.0742 - mae: 0.2399 - val_loss: 0.0745 - val_mae: 0.2406\n",
      "Epoch 122/150\n",
      "41818/41818 [==============================] - 41s 971us/step - loss: 0.0742 - mae: 0.2399 - val_loss: 0.0743 - val_mae: 0.2403\n",
      "Epoch 123/150\n",
      "41818/41818 [==============================] - 41s 983us/step - loss: 0.0742 - mae: 0.2399 - val_loss: 0.0744 - val_mae: 0.2406\n",
      "Epoch 124/150\n",
      "41818/41818 [==============================] - 40s 951us/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0743 - val_mae: 0.2404\n",
      "Epoch 125/150\n",
      "41818/41818 [==============================] - 40s 953us/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0743 - val_mae: 0.2403\n",
      "Epoch 126/150\n",
      "41818/41818 [==============================] - 40s 952us/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0746 - val_mae: 0.2405\n",
      "Epoch 127/150\n",
      "41818/41818 [==============================] - 40s 961us/step - loss: 0.0742 - mae: 0.2399 - val_loss: 0.0741 - val_mae: 0.2402\n",
      "Epoch 128/150\n",
      "41818/41818 [==============================] - 40s 945us/step - loss: 0.0742 - mae: 0.2399 - val_loss: 0.0748 - val_mae: 0.2407\n",
      "Epoch 129/150\n",
      "41818/41818 [==============================] - 40s 956us/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0748 - val_mae: 0.2407\n",
      "Epoch 130/150\n",
      "41818/41818 [==============================] - 40s 962us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0744 - val_mae: 0.2404\n",
      "Epoch 131/150\n",
      "41818/41818 [==============================] - 40s 955us/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0740 - val_mae: 0.2403\n",
      "Epoch 132/150\n",
      "41818/41818 [==============================] - 40s 949us/step - loss: 0.0742 - mae: 0.2398 - val_loss: 0.0748 - val_mae: 0.2409\n",
      "Epoch 133/150\n",
      "41818/41818 [==============================] - 40s 965us/step - loss: 0.0742 - mae: 0.2399 - val_loss: 0.0743 - val_mae: 0.2402\n",
      "Epoch 134/150\n",
      "41818/41818 [==============================] - 41s 977us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0750 - val_mae: 0.2405\n",
      "Epoch 135/150\n",
      "41818/41818 [==============================] - 40s 953us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0742 - val_mae: 0.2402\n",
      "Epoch 136/150\n",
      "41818/41818 [==============================] - 40s 963us/step - loss: 0.0742 - mae: 0.2399 - val_loss: 0.0745 - val_mae: 0.2404\n",
      "Epoch 137/150\n",
      "41818/41818 [==============================] - 40s 955us/step - loss: 0.0741 - mae: 0.2398 - val_loss: 0.0745 - val_mae: 0.2406\n",
      "Epoch 138/150\n",
      "41818/41818 [==============================] - 40s 964us/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0743 - val_mae: 0.2403\n",
      "Epoch 139/150\n",
      "41818/41818 [==============================] - 40s 963us/step - loss: 0.0741 - mae: 0.2399 - val_loss: 0.0743 - val_mae: 0.2404\n",
      "Epoch 140/150\n",
      "41818/41818 [==============================] - 40s 961us/step - loss: 0.0742 - mae: 0.2400 - val_loss: 0.0741 - val_mae: 0.2402\n",
      "Epoch 141/150\n",
      "41818/41818 [==============================] - 40s 967us/step - loss: 0.0742 - mae: 0.2400 - val_loss: 0.0743 - val_mae: 0.2404\n",
      "Epoch 142/150\n",
      "41818/41818 [==============================] - 40s 958us/step - loss: 0.0742 - mae: 0.2399 - val_loss: 0.0742 - val_mae: 0.2406\n",
      "Epoch 143/150\n",
      "41818/41818 [==============================] - 40s 964us/step - loss: 0.0742 - mae: 0.2400 - val_loss: 0.0743 - val_mae: 0.2405\n",
      "Epoch 144/150\n",
      "41818/41818 [==============================] - 41s 975us/step - loss: 0.0742 - mae: 0.2398 - val_loss: 0.0741 - val_mae: 0.2403\n",
      "Epoch 145/150\n",
      "41818/41818 [==============================] - 40s 965us/step - loss: 0.0742 - mae: 0.2399 - val_loss: 0.0742 - val_mae: 0.2405\n",
      "Epoch 146/150\n",
      "41818/41818 [==============================] - 40s 963us/step - loss: 0.0742 - mae: 0.2398 - val_loss: 0.0740 - val_mae: 0.2402\n",
      "Epoch 147/150\n",
      "41818/41818 [==============================] - 40s 968us/step - loss: 0.0742 - mae: 0.2400 - val_loss: 0.0749 - val_mae: 0.2405\n",
      "Epoch 148/150\n",
      "41818/41818 [==============================] - 40s 961us/step - loss: 0.0742 - mae: 0.2398 - val_loss: 0.0747 - val_mae: 0.2405\n",
      "Epoch 149/150\n",
      "41818/41818 [==============================] - 40s 967us/step - loss: 0.0742 - mae: 0.2399 - val_loss: 0.0741 - val_mae: 0.2404\n",
      "Epoch 150/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41818/41818 [==============================] - 40s 967us/step - loss: 0.0742 - mae: 0.2398 - val_loss: 0.0753 - val_mae: 0.2408\n"
     ]
    }
   ],
   "source": [
    "histories, nmse = k_fold(150, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMSE: \n",
      "0.98276360853308\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"NMSE: \")\n",
    "print(np.mean(nmse))\n",
    "\n",
    "num_epochs = 150\n",
    "val_mae_history = [np.mean([x['val_mae'][i] for x in histories]) for i in range(num_epochs)]\n",
    "mae_history = [np.mean([x['mae'][i] for x in histories]) for i in range(num_epochs)]\n",
    "plt.plot(range(3, len(val_mae_history) + 1), val_mae_history[2:], 'ro')\n",
    "plt.plot(range(3, len(mae_history) + 1), mae_history[2:], 'bo')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend(['test', 'train'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_loss_history = [np.mean([x['val_loss'][i] for x in histories]) for i in range(num_epochs)]\n",
    "loss_history = [np.mean([x['loss'][i] for x in histories]) for i in range(num_epochs)]\n",
    "plt.plot(range(1, len(val_loss_history) + 1), val_loss_history, 'ro')\n",
    "plt.plot(range(1, len(loss_history) + 1), loss_history, 'bo')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('LOSS')\n",
    "plt.legend(['test', 'train'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "65341/65341 [==============================] - 64s 987us/step - loss: 0.0765 - mae: 0.2411\n",
      "Epoch 2/20\n",
      "65341/65341 [==============================] - 64s 979us/step - loss: 0.0753 - mae: 0.2403\n",
      "Epoch 3/20\n",
      "65341/65341 [==============================] - 61s 933us/step - loss: 0.0748 - mae: 0.2402\n",
      "Epoch 4/20\n",
      "65341/65341 [==============================] - 64s 984us/step - loss: 0.0745 - mae: 0.2401\n",
      "Epoch 5/20\n",
      "65341/65341 [==============================] - 92s 1ms/step - loss: 0.0744 - mae: 0.2398\n",
      "Epoch 6/20\n",
      "65341/65341 [==============================] - 91s 1ms/step - loss: 0.0742 - mae: 0.2399: 3s - lo - ETA: 0s - loss: 0.0742 - \n",
      "Epoch 7/20\n",
      "65341/65341 [==============================] - 71s 1ms/step - loss: 0.0741 - mae: 0.2398\n",
      "Epoch 8/20\n",
      "65341/65341 [==============================] - 63s 959us/step - loss: 0.0741 - mae: 0.2397\n",
      "Epoch 9/20\n",
      "65341/65341 [==============================] - 72s 1ms/step - loss: 0.0742 - mae: 0.2397: 0s - lo\n",
      "Epoch 10/20\n",
      "65341/65341 [==============================] - 73s 1ms/step - loss: 0.0742 - mae: 0.2397\n",
      "Epoch 11/20\n",
      "65341/65341 [==============================] - 62s 953us/step - loss: 0.0742 - mae: 0.2397\n",
      "Epoch 12/20\n",
      "65341/65341 [==============================] - 57s 877us/step - loss: 0.0742 - mae: 0.2397\n",
      "Epoch 13/20\n",
      "65341/65341 [==============================] - 68s 1ms/step - loss: 0.0742 - mae: 0.2398\n",
      "Epoch 14/20\n",
      "65341/65341 [==============================] - 64s 985us/step - loss: 0.0741 - mae: 0.2397\n",
      "Epoch 15/20\n",
      "65341/65341 [==============================] - 68s 1ms/step - loss: 0.0742 - mae: 0.2398\n",
      "Epoch 16/20\n",
      "65341/65341 [==============================] - 61s 926us/step - loss: 0.0742 - mae: 0.2398\n",
      "Epoch 17/20\n",
      "65341/65341 [==============================] - 62s 941us/step - loss: 0.0741 - mae: 0.2397\n",
      "Epoch 18/20\n",
      "65341/65341 [==============================] - 61s 933us/step - loss: 0.0741 - mae: 0.2397\n",
      "Epoch 19/20\n",
      "65341/65341 [==============================] - 109s 2ms/step - loss: 0.0742 - mae: 0.2397\n",
      "Epoch 20/20\n",
      "65341/65341 [==============================] - 90s 1ms/step - loss: 0.0741 - mae: 0.2397\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "\n",
    "history = model.fit(inputs, targets, epochs=20, batch_size=1, verbose=1)\n",
    "history.history['mae']\n",
    "model.save(\"nn_sensor_reverse.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
